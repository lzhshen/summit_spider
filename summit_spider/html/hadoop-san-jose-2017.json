[
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Vinod Kumar Vavilapalli"}], "base_fname": "Apache_Hadoop_YARN_Present_and_Future", "title": "Apache Hadoop YARN: Present and Future", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/apache-hadoop-yarn-present-and-future-77278025"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/PRsr1hgidQI"}, "desc": "Apache Hadoop YARN is the modern Distributed Operating System. It enables the Hadoop compute layer to be a common resource-management platform that can host a wide variety of applications. Multiple organizations are able to leverage YARN in building their applications on top of Hadoop without themselves repeatedly worrying about resource management, isolation, multi-tenancy issues etc.\nIn this talk, we\u2019ll first hit the ground with the current status of Apache Hadoop YARN \u2013 how it is faring today in deployments large and small. We will cover different types of YARN deployments, in different environments and scale.\nWe'll then move on to the exciting present & future of YARN \u2013 features that are further strengthening YARN as the first-class resource-management platform for datacenters running enterprise Hadoop. We\u2019ll discuss the current status as well as the future promise of features and initiatives like \u2013 10x scheduler throughput improvements, docker containers support on YARN, support for long running services (alongside applications) natively without any changes, seamless application upgrades, fine-grained isolation for multi-tenancy using CGroups on disk & network resources, powerful scheduling features like application priorities, intra-queue preemption across applications and operational enhancements including insights through Timeline Service V2,  a new web UI and better queue management.\nThis session is a  (Advanced) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Docker / Container and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Comcast", "name": "Barbara Eckman"}], "base_fname": "End_to_end_Data_Governance_with_Apache_Avro_and_Atlas", "title": "End-to-end Data Governance with Apache Avro and Atlas", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/endtoend-data-governance-with-apache-avro-and-atlas"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/b--xwHHukRA"}, "desc": "Aeolus is Comcast\u2019s new internal Big Data system for providing access to an integrated view of a wide variety of high-quality, near-real-time and batch data. Such integration can enable data scientists to uncover otherwise hidden trends, anomalies, and powerful predictors of business successes and failures.  But integrating data across silos in a large enterprise is fraught with peril. There typically are few standards on naming conventions and data representation, and spotty documentation at best. The old rule of thumb often applies: 70% of the analysts\u2019 time goes into data wrangling, while only 30% goes toward the actual analyses and simulations. The goal of the Athene Data Governance Platform within Aeolus is to invert this ratio.  This talk will explain how Comcast is using Apache Avro and Atlas for end-to-end data governance, the challenges faced, and methods used to address these challenges.\n\nAvro provides a lingua franca for data representation, data integration, and schema evolution.  All data published for community consumption must have an associated avro schema in Atlas.  Every step in its journey through Aeolus, in flight or at rest, is captured in Atlas.   Atlas\u2019 extensibility has allowed us to add or update various entity types (e.g., avro schemas, kafka topics, object store pseudo-directories)  and lineage types (e.g., storing streaming data in object storage; embellishing and re-publishing streaming data; performing aggregations and other transformations on data at rest; and evolution of schemas with compatibility flags).  Transformation services notify Atlas of lineage links via custom asynchronous kafka messaging.\n\nAtlas provides self-service data discovery and lineage browsing and querying, via full-text search, DSL query language, or gremlin graph query language.  Example queries: \u201cWhere is data from kafka topic X stored?\u201d   \u201cDisplay the journey of data currently stored in pseudo-directory X since it entered the Aeolus system\u201d.  \u201cShow me all earlier versions of schema S, and whether they are forward/backward compatible with each other.\u201d  \nThis session is a  (Advanced) talk in our Governance and Security track. It focuses on Apache Atlas, Apache Avro, Apache Kafka, Apache ORC and is geared towards CXO, Architect, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Verizon Wireless", "name": "Sreenath Akinepalli"}, {"bio": "", "corp": "Verizon Wireless", "name": "sandeep katuku"}], "base_fname": "Verizon_Finance_Data_Lake_implementation_as_a_Self_Service_Discovery_BIg_Data_Platform", "title": "Verizon:  Finance Data Lake implementation as a Self Service Discovery BIg Data Platform", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/verizon-finance-data-lake-implementation-as-a-self-service-discovery-big-data-platform"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/WZaocprS7Uc"}, "desc": "Finance Data Lake objective is to create a centralized enterprise data repository for all Finance and Supply Chain data. It serves as the single source of truth. It enables a self-service discovery Analytics platform for business users to answer adhoc business questions and derive critical insights. The data lake is based on open source Hadoop big data platform and a very cost effective solution in breaking the ERP data silos and simplifying the data architecture in the enterprise.\n\nPOCs were conducted on in-house Hortonworks Hadoop data platform to validate the cluster performance for Production volumes. Based on business priorities, an initial roadmap was defined using 3 data sources including 2 SAP ERPs and Peoplesoft (OLTP systems).  Development environment was established in AWS Cloud for agile delivery. The near real time data ingestion architecture for the data lake was defined using replication tools and custom SQOOP based micro-batching framework and data persisted in Apache Hive DB in ORC format. Data and user security is implemented using Apache Ranger and sensitive data stored at rest in encryption zones. Business data sets were developed in Hive scripts and scheduled using Oozie. Multiple reporting tools connectivity including SQL tools, Excel and Tableau were enabled for Self-service Analytics. Upon successful implementation of the initial phase, a full roadmap is established to extend the Finance data lake to over 25 data sources and enhance data ingestion to scale as well as enable OLAP tools on Hadoop.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hive, Apache Oozie, Apache Ranger, Apache Sqoop, OLAP and is geared towards CXO, Architect, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Progressive", "name": "Krishna Potluri"}, {"bio": "", "corp": "Syncsort", "name": "Tend\u00fc Yogurt\u00e7u, Ph.D."}], "base_fname": "Journey_to_the_Data_Lake_How_Progressive_Paved_a_Faster_Smoother_Path_to_Increase_Enterprise_Adoption", "title": "Journey to the Data Lake: How Progressive Paved a Faster, Smoother Path to Increase Enterprise Adoption", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/journey-to-the-data-lake-how-progressive-paved-a-faster-smoother-path-to-increase-enterprise-adoption"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/9a3GWQfCI_A"}, "desc": "Progressive Insurance is well known for its innovative use of data to better serve its customers, and the important role that Hortonworks Data Platform has played in that transformation.  However, as with most things worth doing, the path to the Data Lake was not without its challenges.  In this session, I\u2019ll share our top use cases for Hadoop \u2013 including telematics and display ads, how a skills shortage turned supporting these applications into a nightmare, and how \u2013 and why \u2013 we now use Syncsort DMX-h to accelerate enterprise adoption by making it quick and easy (or faster and easier) to populate the data lake \u2013 and keep it up to date \u2013 with data from across the enterprise.   I\u2019ll discuss the different approaches we tried, the benefits of using a tool vs. open source, and how we created our Hadoop Ingestor app using Syncsort DMX-h.\nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Hive, Apache Kafka, Apache Spark, Other and is geared towards Architect audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Kenneth Smith"}, {"bio": "", "corp": "Io-Tahoe LLC (A Centrica subsidary)", "name": "Mark Miller"}], "base_fname": "Open_Source_in_the_Energy_Industry_Creating_a_New_Operational_Model_for_Data_Management_Data_Science", "title": "Open Source in the Energy Industry - Creating a New Operational Model for Data Management & Data Science", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/open-source-in-the-energy-industry-creating-a-new-operational-model-for-data-management-data-science"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/ZHYco6GTuA0"}, "desc": "The energy industry is well known to be laggard adopters of new technology.  However, industry challenges such as aging assets & workforce, increased regulatory scrutiny, renewable energy sources, depressed commodity prices, changing customer expectations, and growing data volumes are pushing companies to explore new technologies to help solve these problems.  Learn how Io-Tahoe\u2019s platform built on open source technologies from Hortonworks, is helping organizations in the energy vertical transform into data driven enterprises.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Hive, Apache HBase, Apache Nifi, Apache Spark and is geared towards CXO audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Simon Elliston Ball"}], "base_fname": "Solving_cyber_at_scale", "title": "Solving cyber at scale", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/solving-cyber-at-scale-77280237"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/dQ20kyuDS3o"}, "desc": "Information security is a big problem today. With more attacks happening all the time, and increasingly sophisticated attacks beyond the script-kiddies of yesterday, patrolling the borders of our networks, and controlling threats both from outside and within is becoming harder. We cannot rely on endpoint protection for a few thousand PCs and servers anymore, but as connected cars, internet of things, and mobile devices become more common, so the attack surface broadens. To face these problems, we need technologies that go beyond the traditional SIEM, with human operators writing rules. We need to use the power of the Hadoop ecosystem to find new patterns, machine learning to uncover subtle signals and big data tools to help humans analysts work better and faster to meet these new threats. Apache Metron is a platform on top of Hadoop that meets these needs. Here we will look at the platform in action, and how to use it to trace a real world complex threat, and how it compares to traditional approaches. Come and see how to make your SOC more effective with automated evidence gathering, Hadoop-powered integration, and real-time detection.\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Metron, Apache Nifi and is geared towards CXO, Architect, Data Scientist, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "TELUS", "name": "Cavan Loughran"}, {"bio": "", "corp": "T4G Limited", "name": "Oliver Meyn"}], "base_fname": "Bringing_Real_Time_to_the_Enterprise_with_Hortonworks_DataFlow", "title": "Bringing Real-Time to the Enterprise with Hortonworks DataFlow", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/bringing-realtime-to-the-enterprise-with-hortonworks-dataflow"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/3UrH4ZU5aTI"}, "desc": "TELUS is Canada\u2019s fastest-growing national telecommunications company, with $12.9B of annual revenue and 12.7M customer connections. TELUS provides TELUS TV to more than 1.1 million customers in western Canada, with the goal of maximizing clients\u2019 service experience. Every television set-top box (STB) in the TELUS network regularly logs its diagnostic state and those logs serve as an important signal of the state of the customers equipment. TELUS\u2019 goal was to move from analysis of these STB logs from a daily, batch process towards streaming analytics that would allow the company to respond within minutes to changes in a device or service health. TELUS increased logging frequency by a factor of 50 and adapted the overall architecture to keep pace with that transformation. Members of the TELUS team will walk through the implementation and workflow challenges they overcame, working with Hortonworks to connect Apache Hadoop, Apache NiFi and Apache Spark, within a secure enterprise environment. They will also discuss the impact to their business and their customers\u2019 satisfaction.\nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Nifi, Apache Spark and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Dongjoon Hyun"}, {"bio": "", "corp": "Hortonworks", "name": "Jason Dere"}], "base_fname": "Security_Updates_More_Seamless_Access_Controls_with_Apache_Spark_and_Apache_Ranger", "title": "Security Updates: More Seamless Access Controls with Apache Spark and Apache Ranger", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/security-updates-more-seamless-access-controls-with-apache-spark-and-apache-ranger"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/yGB4DZ29bv8"}, "desc": "Security has always been a fundamental requirement for enterprise adoption. For example, in a company, billing, data science, and regional marketing teams may all have the required access privileges to view customer data, while sensitive data like credit card numbers should be accessible only to the finance team. Previously, Apache Hive\u2122 with Apache Ranger\u2122 policies was used to manage such scenarios. In this talk, we shows that Apache Spark\u2122 SQL is aware of the existing Apache Ranger policies defined for Apache Hive. In other words, for SQL users, access to databases, tables, rows and columns are controlled in a fine-grained manner, irrespective of whether the data is analyzed using Apache Spark SQL or Hive. If a policy is updated, both Apache Spark and Apache Hive users get their result consistently. In addition, all fine-grained access via Apache Spark SQL can be monitored and searched through a centralized interface via Apache Ranger.\nThis session is a  (Beginner) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Hive, Apache Ranger, Apache Spark, Kerberos and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Discover Financial Services", "name": "Santosh Bardwaj"}], "base_fname": "Continuous_Data_Ingestion_pipeline_for_the_Enterprise", "title": "Continuous Data Ingestion pipeline for the Enterprise", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/continuous-data-ingestion-pipeline-for-the-enterprise"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/65MMzCmq4tg"}, "desc": "Continuous Data ingestion platform built on NIFI and Spark that integrates variety of data sources including real-time events,  data from external sources , structured and unstructured data with in-flight governance providing a real-time pipeline moving data from source to consumption in minutes.  The next-gen data pipeline has helped eliminate  the legacy batch latency and improve data quality and governance by designing custom NIFI processors and embedded Spark code. To meet the stringent regulatory requirements the data pipeline is being augmented with features to do in-flight ETL , DQ checks  that enables a continuous workflow enhancing the Raw / unclassified data to Enriched / classified data available for consumption by users and production processes. \nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Hadoop, Apache Nifi, Apache ORC, Apache Spark, Kerberos and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "IBM", "name": "Jeffrey Rodriguez Via\u00f1a"}, {"bio": "", "corp": "IBM", "name": "Tanping Wang"}], "base_fname": "Apache_Knox_Gateway_Single_Sign_On_expands_the_reach_of_the_Enterprise_users", "title": "Apache Knox Gateway \"Single Sign On\" expands the reach of the Enterprise users", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/apache-knox-gateway-single-sign-on-expands-the-reach-of-the-enterprise-users"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/xxYKoDCaX2k"}, "desc": "Apache Knox Gateway is a proxy for interacting with Apache Hadoop clusters in a secure way providing authentication, service level authorization, and many other extensions to secure any HTTP interactions in your cluster.  One main feature of Apache Knox Gateway is the ability to extend the reach of your REST APIs to the internet while still securing your cluster and working with Kerberos. Recent contributions to the Apache Knox community have added support for Single Sign On (SSO) based on Pac4j 1.8.9 which is a very powerful security engine which provides SSO support through SAML2, OAuth, OpenID, and CAS.  In addition, through recent community contributions Apache Ambari, and Apache Ranger can now also provide SSO authentication through Knox. This paper will discuss the architecture of Knox SSO, it will explain how enterprise user could benefit by this feature and will present enterprise use cases for Knox SSO, and integration with open source Shibboleth, ADFS Windows server Idp support, and Okta cloud Idp.\nThis session is a  talk in our Governance and Security track. It focuses on Apache Ambari, Apache Knox, Apache Ranger and is geared towards Architect, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Netflix", "name": "Michelle Ufford"}], "base_fname": "Whoops_The_Numbers_Are_Wrong_Scaling_Data_Quality_Netflix", "title": "Whoops, The Numbers Are Wrong! Scaling Data Quality @ Netflix", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/whoops-the-numbers-are-wrong-scaling-data-quality-netflix"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/fXHdeBnpXrg"}, "desc": "Netflix is a famously data-driven company. Data is used to make informed decisions on everything from content acquisition to content delivery, and everything in-between. As with any data-driven company, it\u2019s critical that data used by the business is accurate. Or, at worst, that the business has visibility into potential quality issues as soon as they arise. But even in the most mature data warehouses, data quality can be hard. How can we ensure high quality in a cloud-based, internet-scale, modern big data warehouse employing a variety of data engineering technologies?\nIn this talk, Michelle Ufford will share how the Data Engineering & Analytics team at Netflix is doing exactly that. We\u2019ll kick things off with a quick overview of Netflix\u2019s analytics environment, then dig into details of our data quality solution. We\u2019ll cover what worked, what didn\u2019t work so well, and what we plan to work on next. We\u2019ll conclude with some tips and lessons learned for ensuring data quality on big data. \nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache Hive, Apache Pig, Apache Spark and is geared towards Architect, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Andy Feng"}, {"bio": "", "corp": "Yahoo!", "name": "Lee Yang"}], "base_fname": "TensorFlowOnSpark_Scalable_TensorFlow_Learning_on_Spark_Clusters", "title": "TensorFlowOnSpark: Scalable TensorFlow Learning on Spark Clusters", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/tensorflowonspark-scalable-tensorflow-learning-on-spark-clusters"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/b3lTvTKBatE"}, "desc": "In recent releases, TensorFlow has been enhanced for distributed learning and HDFS access. Outside of the Google cloud, however, users still needed a dedicated cluster for TensorFlow applications. There are several community projects wiring TensorFlow onto Apache Spark clusters. Unfortunately, they are limited to support synchronous distributed learning only, and don\u2019t allow TensorFlow servers to communicate with each other directly. \nIn this talk, we will introduce a new framework, TensorFlowOnSpark,  for scalable TensorFlow learning, which will be open sourced in Q1 2017. This new framework enables easy experimentation for algorithm designs, and supports scalable training & inferencing on Spark clusters. It supports all TensorFlow functionalities including synchronous & asynchronous learning, model & data parallelism, and TensorBoard. It provides architectural flexibility for data ingestion to TensorFlow and network protocols for server-to-server communication. With a few lines of code changes, an existing TensorFlow algorithm can be transformed into a scalable application. \nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Spark and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "PayPal Inc.", "name": "Venkatesh Ramanathan"}], "base_fname": "Large_Scale_Graph_Processing_Machine_Learning_Algorithms_for_Payment_Fraud_Prevention", "title": "Large Scale Graph Processing & Machine Learning Algorithms for Payment Fraud Prevention", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/large-scale-graph-processing-machine-learning-algorithms-for-payment-fraud-prevention"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/OzT1cQHcZ-4"}, "desc": "PayPal is at the forefront of applying large scale graph processing and machine learning algorithms to keep fraudsters at bay. In this talk, I\u2019ll present how advanced graph processing and machine learning algorithms such as Deep Learning and Gradient Boosting are applied at PayPal for fraud prevention. I\u2019ll elaborate on specific challenges in applying large scale graph processing & machine technique to payment fraud prevention. I\u2019ll explain how we employ sophisticated machine learning tools \u2013 open source and in-house developed. I will also present results from experiments conducted on a very large graph data set containing millions of edges and vertices.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Spark and is geared towards Architect, Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "Forrester Research", "name": "Mike Gualtieri"}], "base_fname": "What_s_Possible_with_AI_and_Data_in_2017", "title": "What\u2019s Possible with AI and Data in 2017?", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/8Ux9vJ72_do"}, "desc": "Artificial intelligence is not new. It emerged as a computer science discipline in the \u201950s and has been a persistent theme in science fiction. What is new now is that billions of dollars are flowing into AI startups and software development efforts by both the internet heavies and enterprise software vendors alike. It promises to be the technology Valhalla for application developer pros who wish to use it to transform or invent new business models by infusing applications with intelligence. This session will unpack the ways in which organizations can use data today to add AI to applications.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Spark, Other and is geared towards CXO, Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Bloomberg", "name": "Joy Chakraborty"}], "base_fname": "Dockerize_and_Kerberize_Notebook_for_Yarn_and_HDFS", "title": "Dockerize and Kerberize Notebook for Yarn and HDFS", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/dockerize-and-kerberize-notebook-for-yarn-and-hdfs"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/7m9VK0kXdcM"}, "desc": "This presentation will provide technical design and development insights in order to set up a Kerberize (secured) JupyterHub notebook for HDFS and Yarn (running Hive, Spark, etc.). Joy will show how Bloomberg set up the Kerberos-based notebook for Data Science community using Docker by integrating JupyterHub, Sparkmagic, and Levy. Sparkmagic provides the Spark kernel for R, Scala and Python. Livy is one of the most promising open source software to allow to submit Spark jobs over http-based REST interfaces. This presentation will highlight the capabilities of Jupyterhub, Sparkmagic and Livy, along with the gap and development required in order to make the notebook to work with Kerberized HDFS/Yarn cluster running Hive, Spark and other services. Docker minimizes the complex integration challenges involving networking and isolation which is essential for such project that will be covered in this presentation. No prior knowledge of any of these technologies is required in order to understand this presentation.\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, Docker / Container, Kerberos and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Junping Du"}, {"bio": "", "corp": "Cloudera", "name": "Andrew Wang"}], "base_fname": "Apache_Hadoop_3_0_Community_Update", "title": "Apache Hadoop 3.0 Community Update", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/apache-hadoop-30-community-update"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": ""}, "desc": "Apache Hadoop 3 is coming! As the next major milestone for hadoop and big data, it attracts everyone's attention as showcase several bleeding-edge technologies and significant features across all components of Apache Hadoop: Erasure Coding in HDFS, Docker container support, Apache Slider integration and Native service support, Application Timeline Service version 2, Hadoop library updates and client-side class path isolation, etc. In this talk, first we will update the status of Hadoop 3.0 releasing work in apache community and the feasible path through alpha, beta towards GA. Then we will go deep diving on each new feature, include: development progress and maturity status in Hadoop 3. Last but not the least, as a new major release, Hadoop 3.0 will contain some incompatible API or CLI changes which could be challengeable for downstream projects and existing Hadoop users for upgrade - we will go through these major changes and explore its impact to other projects and users.\nThis session is a  (Advanced) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Docker / Container and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Uber", "name": "Zhenxiao Luo"}], "base_fname": "Even_Faster_When_Presto_meets_Parquet_Uber", "title": "Even Faster: When Presto meets Parquet @ Uber", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/even-faster-when-presto-meets-parquet-uber"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/0qPhpN1d2AU"}, "desc": "As Uber continues to grow, our big data systems need to grow in scalability, reliability, and performance, to help Uber make business decisions, give user recommendations, and analyze experiments across all data sources. Since 2016, we put Presto in production. Now Presto is serving ~100K queries per day @ Uber, and it becomes a key component for interactive SQL queries on big data. In this presentation, we would like to talk about our experiences and engineering efforts, we start with general introduction about Hadoop Infrastructure & Analytics @ Uber, then comes a brief introduction to Presto, the Interactive SQL engine for big data. We will focus on how we build the New Parquet Reader for Presto, and the detail techniques, Columnar Reads, Lazy Reads, Nested Column Pruning. We will show performance improvements and  Uber's Use Cases. Finally, we would like to share our ongoing plan and future work for Big Data Analytics @ Uber.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache Hive, Apache ORC, Apache Parquet, Cloud and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Evans Ye"}], "base_fname": "Leveraging_Docker_for_Hadoop_build_automation_and_Big_Data_stack_provisioning", "title": "Leveraging Docker for Hadoop build automation and Big Data stack provisioning", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/leveraging-docker-for-hadoop-build-automation-and-big-data-stack-provisioning-77218110"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/-0ZEkyUrWj0"}, "desc": "Apache Bigtop as an open source Hadoop distribution, focuses on developing packaging, testing and deployment solutions that help infrastructure engineers to build up their own customized big data platform as easy as possible. However, packages deployed in production require a solid CI testing framework to ensure its quality. Numbers of Hadoop component must be ensured to work perfectly together as well. In this presentation, we'll talk about how Bigtop deliver its containerized CI framework which can be directly replicated by Bigtop users. The core revolution here are the newly developed Docker Provisioner that leveraged Docker for Hadoop deployment and Docker Sandbox for developer to quickly start a big data stack. The content of this talk includes the containerized CI framework,  technical detail of Docker Provisioner and Docker Sandbox, a hierarchy of docker images we designed, and several components we developed such as Bigtop Toolchain to achieve build automation.\nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Hadoop, Docker / Container, Other and is geared towards Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Pinsight Media", "name": "Matthew Habiger"}, {"bio": "", "corp": "Pinsight Media", "name": "Prashanth Dannamaneni"}], "base_fname": "Project_Destiny_A_Marriage_of_Out_of_Home_Advertising_and_Telco_Data", "title": "Project Destiny: A Marriage of Out-of-Home Advertising and Telco Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/project-destiny-a-marriage-of-outofhome-advertising-and-telco-data"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/3UxPLNko3YI"}, "desc": "Have you ever wondered how advertisers decide whether they should advertise on a billboard? How do they know if they are reaching the right audience? The current state of the art leaves a lot to be desired, relying on small panel studies and census data. Realizing there was an opportunity to leverage telco sourced location data, Pinsight Media's data science and product team set out to help transform the industry. Leveraging Spark, OSRM and a fistful of algorithms, Pinsight turns 20 billion location events into over 1 billion miles of routes traveled every day. This data can then be linked back to demographic and behavioral attributes to provide advertisers with a rich view of the audience they are likely reaching.  In this session Pinsight Media will share the Project Destiny story, from product inception and development to data science prototyping, execution and delivery to the customer. Particular attention will be given to the data science issues faced in creating this product and the deliberate intention to make the output useable across multiple products.\nThis session is a  talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Hive, Apache Parquet, Apache Spark, Apache Tez and is geared towards Data Scientist, Data Analyst, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Atrato.io", "name": "Thomas Weise"}], "base_fname": "From_Batch_to_Streaming_ET_L_with_Apache_Apex", "title": "From Batch to Streaming ET(L) with Apache Apex", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/from-batch-to-streaming-etl-with-apache-apex"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/2v_InS_7xTY"}, "desc": "Stream data processing is increasingly required to support business needs for faster actionable insight with growing volume of information from more sources. Apache Apex is a true stream processing framework for low-latency, high-throughput and reliable processing of complex analytics pipelines on clusters. Apex is designed for quick time-to-production, and is used in production by large companies for real-time and batch processing at scale. \nThis session will use an Apex production use case to walk through the incremental transition from a batch pipeline with hours of latency to an end-to-end streaming architecture with billions of events per day which are processed to deliver real-time analytical reports. The example is representative for many similar extract-transform-load (ETL) use cases with other data sets that can use a common library of building blocks. The transform (or analytics) piece of such pipelines varies in complexity and often involves business logic specific, custom components.\nTopics include:\n* Pipeline functionality from event source through queryable state for real-time insights.\n\n* API for application development and development process.\n\n* Library of building blocks including connectors for sources and sinks such as Kafka, JMS, Cassandra, HBase, JDBC and how they enable end-to-end exactly-once results.\n\n* Stateful processing with event time windowing.\n\n* Fault tolerance with exactly-once result semantics, checkpointing, incremental recovery\n\n* Scalability and low-latency, high-throughput processing with advanced engine features for auto-scaling, dynamic changes, compute locality.\n\n* Who is using Apex in production, and roadmap.\nFollowing the session attendees will have a high level understanding of Apex and how it can be applied to use cases at their own organizations.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Other and is geared towards Architect, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Privacera", "name": "Don Bosco Durai"}, {"bio": "", "corp": "Privacera", "name": "Balaji Ganesan"}], "base_fname": "Beyond_Kerberos_and_Ranger_Tips_to_discover_track_and_manage_risks_in_hybrid_data_environments", "title": "Beyond Kerberos and Ranger - Tips to discover, track and manage risks in hybrid data environments", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/beyond-kerberos-and-ranger-tips-to-discover-track-and-manage-risks-in-hybrid-data-environments"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/LfPI9G1GEi0"}, "desc": "Even after deploying traditional security measures like authentication and authorization to secure sensitive data, data owners and security teams are still struggling to manage and get visibility on risks with data. The same challenge multiplies when data is moving and shared across different data silos such as on-premise Hadoop, public cloud infrastructures such as AWS, Azure and Google Cloud. To control the risks that come with data, enterprises need a comprehensive data-centric approach to easily identify risks, manage security and compliance policies and implement behavior analytics to differentiate between good and bad behavior. This talk will explain a 3 step process of implementing data-centric controls for your hybrid environment including discovering where sensitive data is stored, tracking where data is moving and can easily identifying and controlling potential misuse of the data in near real time. \nThis session is a  (Beginner) talk in our Governance and Security track. It focuses on Apache Ambari, Apache Atlas, Apache Ranger, Apache Spark, Apache Solr and is geared towards CXO, Architect, Data Scientist, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "ChatWork", "name": "Shingo Omura"}, {"bio": "", "corp": "NTT DATA", "name": "Masaru Dobashi"}], "base_fname": "Worldwide_Scalable_and_Resilient_Messaging_Services_by_CQRS_and_Event_Sourcing_using_Akka_Kafka_Streams_and_HBase", "title": "Worldwide Scalable and Resilient Messaging Services by CQRS and Event Sourcing using Akka, Kafka Streams and HBase", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/worldwide-scalable-and-resilient-messaging-services-by-cqrs-and-event-sourcing-using-akka-kafka-streams-and-hbase"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/eZVHuvbOkbY"}, "desc": "ChatWork is one of major business communication platforms in Japan. We keep growing up for 5+ years since our service inception. Now, we hold 110k+ of customer organizations which includes large organizations like telecom companies and the service is widely used across 200+ countries and regions.\nNowadays we have faced drastic increase of message traffic. But, unfortunately, our conventional backend was based on traditional LAMP architecture. Transforming traditional backend into highly available, scalable and resilient backend was imperative.\nTo achieve this, we have applied \u201cCommand Query Responsibility Segregation (CQRS) and Event Sourcing\u201d as a heart of its architecture. The simple idea of segregation brings us independent command-side and query-side system components and it can subsequently achieve highly available, scalable and resilient systems. It is desirable property for messaging services because, for example, even if command-side was down, user can keep reading messages unless query-side was down. Event Sourcing is another key technique to enable us to build optimized systems to handle heterogeneous write/read load. This means that we can choose optimized storage platform for each side. Moreover, the event data can be the rich source for real-time analysis of user\u2019s communication behavior. We have chosen Kafka as a command-side event storage, HBase as a query-side storage, Kafka Streams as a core library to give eventual consistency between the two sides. In application layer, Akka has been chosen as a core framework. Akka can be a good choice as an abstraction layer to build highly concurrent, distributed, resilient and message-driven application effectively. Backpressure introduced by Akka Stream can be important technology to prevent from overflow of data flows in our backend, which contributes system stability very well.\nIn this session, we talk about how above architecture works, how we concluded above architectural decisions on many trade-offs, what was achieved by this architecture, what was the pain points (e.g. how to guarantee eventual consistency, how to migrate systems in the real project, etc.) and several TIPS we learned for realizing our highly distributed and resilient messaging systems.\nChatWork is a business communication platform for global teams. Our four main features are enterprise-grade group chat, file sharing, task management and video chat. NTT DATA is one of biggest solution provider in Japan and providing technical support about Open Source Software and distributed computing. The project has been conducted with cooperation of ChatWork and NTT DATA.\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Hadoop, Apache HBase, Apache Kafka, Apache ZooKeeper and is geared towards Architect, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Rohini Palaniswamy"}], "base_fname": "Yahoo_Moving_beyond_running_100_of_Apache_Pig_jobs_on_Apache_Tez", "title": "Yahoo - Moving beyond running 100% of Apache Pig jobs on Apache Tez", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/yahoo-moving-beyond-running-100-of-apache-pig-jobs-on-apache-tez"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/UKDV_aig9Ws"}, "desc": "Last year at Yahoo, we spent great effort in scaling, stabilizing and making Pig on Tez production ready and by the end of the year retired running Pig jobs on Mapreduce. This talk will detail the performance and resource utilization improvements Yahoo achieved after migrating all Pig jobs to run on Tez.\nAfter successful migration and the improved performance we shifted our focus to addressing some of the bottlenecks we identified and new optimization ideas that we came up with to make it go even faster. We will go over the new features and work done in Tez to make that happen like custom YARN ShuffleHandler, reworking DAG scheduling order, serialization changes, etc.\nWe will also cover exciting new features that were added to Pig for performance such as bloom join and byte code generation. A distributed bloom join that can create multiple bloom filters in parallel was straightforward to implement with the flexibility of Tez DAGs. It vastly improved performance and reduced disk and network utilization for our large joins. Byte code generation for projection and filtering of records is another big feature that we are targeting for Pig 0.17 which will speed up processing by reducing the virtual function calls.\nThis session is a  (Advanced) talk in our Applications track. It focuses on Apache Pig and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "DataTorrent", "name": "Sasha Parfenov"}], "base_fname": "Visualizing_Big_Data_in_Realtime", "title": "Visualizing Big Data in Realtime", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/visualizing-big-data-in-realtime"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/YiECjVhyeh8"}, "desc": "Apache Apex brings you the power to quickly build and run big data batch and stream processing applications.  But what about visualizing your data in real time as it flows through the Apache Apex applications?  Together, we will review Apache Apex, and how it integrates with Apache Hadoop and Apache Kafka to process your big data with streaming computation.  Then we will explore the options available to visualize Apex applications metrics and data, including open-source options like REST and PubSub mechanisms in StrAM, as well as features available in the RTS Console like real-time Dashboards and Widgets.  We will also look into ways of packaging dashboards inside your Apache Apex applications.\nThis session is a  (Beginner) talk in our Applications track. It focuses on Apache Hadoop, Apache Kafka, Other and is geared towards Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "ZEPL", "name": "moonsoo Lee"}], "base_fname": "Helium_makes_Zeppelin_fly", "title": "Helium makes Zeppelin fly!", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/helium-makes-zeppelin-fly"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/1YDiym-OigI"}, "desc": "Apache Zeppelin is interactive data analytics environment for large scale data processing systems. It deeply integrates to Apache spark and many other frameworks, provides beautiful interactive web-based interface, data visualization, collaborative work environment and many other nice features to make your data science lifecycle more fun and enjoyable. Helium is a framework that manages pluggable components like Visualization, Spell inside of Zeppelin. Pluggable component extends Zeppelin's capability and particularly useful when Zeppelin is being used as a collaborative data science environment. Moon will demonstrate create custom visualization, publish to Helium online registry and use them in the notebook. Also talk about how Helium framework and Helium online registry works behind the scene and future roadmap as well. You'll see not only how easy creating and publishing Helium package is but also what possibility these pluggable modules gives to Zeppelin as a data science tool and business intelligence tool.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Zeppelin and is geared towards Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "PipelineIO", "name": "Chris Fregly"}], "base_fname": "Optimizing_profiling_and_deploying_high_performance_Spark_ML_and_TensorFlow_AI_models_in_production_with_GPUs", "title": "Optimizing, profiling and deploying high performance Spark ML and TensorFlow AI models in production with GPUs", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/optimizing-profiling-and-deploying-high-performance-spark-ml-and-tensorflow-ai-models-in-production-with-gpus"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/TF-fSZa8M6c"}, "desc": "Using the latest advancements from TensorFlow including the Accelerated Linear Algebra (XLA) Framework, JIT/AOT Compiler, and Graph Transform Tool , I\u2019ll demonstrate how to optimize, profile, and deploy TensorFlow Models in GPU-based production environment.\nThis talk is contains many Spark ML and TensorFlow AI demos using PipelineIO's 100% Open Source Community Edition. All code and Docker images are available to reproduce on your own CPU or GPU-based cluster.\n* Bio *\n\nChris Fregly is Founder and Research Engineer at PipelineIO, a Streaming Machine Learning and Artificial Intelligence Startup based in San Francisco. He is also an Apache Spark Contributor, a Netflix Open Source Committer, founder of the Global Advanced Spark and TensorFlow Meetup, author of the O\u2019Reilly Video Series High Performance TensorFlow in Production.\nPreviously, Chris was a Distributed Systems Engineer at Netflix, a Data Solutions Engineer at Databricks, and a Founding Member of the IBM Spark Technology Center in San Francisco.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Kafka, Apache Parquet, Apache Spark, Cloud, Docker / Container and is geared towards Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Geisinger Health", "name": "Mark Mossel"}, {"bio": "", "corp": "Geisinger Health", "name": "Dhruv Mathrawala"}], "base_fname": "Big_Data_at_Geisinger_Health_System_Big_Wins_in_a_Short_Time", "title": "Big Data at Geisinger Health System: Big Wins in a Short Time", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/big-data-at-geisinger-health-system-big-wins-in-a-short-time"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/UzgsczrdWbg"}, "desc": "Geisinger Health System is well known in the healthcare community as a pioneer in data and analytics. We have had an Electronic Health Record (EHR) since 1996, and an Electronic Data Warehouse (EDW) since 2008. Much of daily and weekly operational reporting, as well as an abundance of ad hoc analytics, come from the EDW.\nApproximately 18 months ago, the Data Management team implemented Hadoop in the Hortonworks Data Platform (HDP), and successes in implementation and development have proven to the organization that we should abandon the traditional EDW in favor of the Big Data (HDP) platform.\nIn less than 18 months, we stood up the platform, created a data ingestion pipeline, duplicated all source feeds from the EDW into HDP, and had several analytics developed with HDP and Tableau. Furthermore, we have exploited the new capabilities of the platform, where we use Natural Language Processing (NLP) to interrogate valuable (but previously hidden) clinical notes. The new platform has data that is modeled and governed, setting the stage to push Geisinger Health System from a pioneer to a leader in Big Data and Analytics.\nThis session will focus on Hortonworks Data Platform, covering data architecture, security, data process flow, and development. It is geared toward Data Architects, Data Scientists, and Operations/I.T. audiences.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Other and is geared towards CXO audiences."},
{"speakers": [{"bio": "", "corp": "Combust", "name": "Hollin Wilkins"}, {"bio": "", "corp": "Combust Inc.", "name": "Mikhail Semeniuk"}], "base_fname": "MLeap_Deploy_Spark_ML_Pipelines_to_Production_API_Servers", "title": "MLeap: Deploy Spark ML Pipelines to Production API Servers", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/mleap-deploy-spark-ml-pipelines-to-production-api-servers"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/8gOzAK-6jt4"}, "desc": "MLeap is an open-source technology that allows Data Scientists and Engineers to deploy Spark-trained ML Pipelines and Models to a scoring engine instantly. During our presentation, we will show you how to deploy any Spark ML Pipeline, as well as custom transformers, that are trained using Spark streaming to both a cloud-based API server as well as an IoT device.\nWhy MLeap? Data Scientists use a myriad tools to analyze datasets, clean them and build offline models and validate their performance. The resulting scripts are thrown across the wall to Data Engineers and Architects whose job is to bring these pipelines to production. The Engineers are left with the unenviable job of not only reproducing the Data Scientists\u2019 conclusions, but to scale the resulting pipeline both of which require a deep understanding of Data Science itself. As a result, most if not all Data Science deployments in the wild end up either too simplistic or take too long to productionize.\nMLeap solves this problem for Spark users by providing serialization of ML Pipelines\u2019 transformers to an MLeap Bundle, which is a graph-based serialization framework built on top of Protobuf 3 and JSON. In addition, MLeap also provides a highly optimized execution engine that doesn\u2019t rely on the Spark-context, making inference blazing fast and is capable of executing one model or thousands of models in parallel.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Avro, Apache Spark, Apache Zeppelin, Docker / Container, Other and is geared towards Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "NetApp", "name": "Ron Long"}], "base_fname": "Addressing_Enterprise_Customer_Pain_Points_with_a_Data_Driven_Architecture", "title": "Addressing Enterprise Customer Pain Points with a Data Driven Architecture", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/addressing-enterprise-customer-pain-points-with-a-data-driven-architecture"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/-ZfzQeB_30I"}, "desc": "Customers that are implementing Big Data Analytics projects in enterprise environments driven by  line of business applications are faced with the three critical issues of Managing Complexity, Data Movement and Replication, and Cloud Integration.  In this session you will learn about the characteristics of these pain points and how designing and implementing a data driven approach  enables enterprises to implement quickly and efficiently with a future proof architecture of hybrid cloud.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Kafka, Apache Spark, Cloud and is geared towards CXO, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "IBM", "name": "Trevor Grant"}], "base_fname": "Introduction_to_Online_Machine_Learning_Algorithms", "title": "Introduction to Online Machine Learning Algorithms", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/introductions-to-online-machine-learning-algorithms"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/O3gd6elZOlA"}, "desc": "Online algorithms are an increasingly popular yet often misunderstood branch of machine learning, where model parameter estimates are updated for each new piece of information received. While mini-batch methods have often been mislabeled as 'streaming-machine learning', true online methods have different implementations and goals. This talk will explain key differences between online and offline machine learning, an introduction to many common online algorithms, and how online algorithms can be analyzed. An example using Apache Flink to detect trends on Twitter will be presented. Attendees will come away from this talk with a better understanding of the challenges and opportunities from working with online algorithms and how they can begin implementing their own algorithms in Apache Flink.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Flink, Other and is geared towards Architect, Data Scientist audiences."},
{"speakers": [{"bio": "", "corp": "Red Hat", "name": "Suneel Marthi"}], "base_fname": "Large_Scale_Processing_of_Unstructured_Text", "title": "Large Scale Processing of Unstructured Text", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/large-scale-processing-of-unstructured-text"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/rag1Rv_22jQ"}, "desc": "Natural Language Processing (NLP) practitioners often have to deal with analyzing large corpora of unstructured documents and this is often a tedious process. Python tools like NLTK do not scale to large production data sets and cannot be plugged into a distributed scalable framework like Apache Spark or Apache Flink. \nThe Apache OpenNLP library is a popular machine learning based toolkit for processing unstructured text. Combining a permissive licence, a easy-to-use API and set of components which are highly customize and trainable to achieve a very high accuracy on a particular dataset. Built-in evaluation allows to measure and tune OpenNLP\u2019s performance for the documents that need to be processed. \nFrom sentence detection and tokenization to parsing and named entity finder, Apache OpenNLP has the tools to address all tasks in a natural language processing workflow. It applies Machine Learning algorithms such as Perceptron and Maxent, combined with tools such as word2vec to achieve state of the art results. In this talk, we\u2019ll be seeing a demo of large scale Name Entity extraction and Text classification using the various Apache OpenNLP components wrapped into Apache Flink stream processing pipeline and as an Apache NiFI processor.\n\nNLP practitioners will come away from this talk with a better understanding of how the various Apache OpenNLP components can help in processing large reams of unstructured data using a highly scalable and distributed framework like Apache Spark/Apache Flink/Apache NiFi. \nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Flink, Apache Nifi, Apache Spark, Other and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Akshai Sarma"}, {"bio": "", "corp": "Yahoo!", "name": "Michael Natkovich"}], "base_fname": "Bullet_A_Real_Time_Data_Query_Engine", "title": "Bullet:  A Real Time Data Query Engine", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/bullet-a-real-time-data-query-engine"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/c9CHEs5MEG0"}, "desc": "Bullet is an open sourced, lightweight, pluggable querying system for streaming data without a persistence layer implemented on top of Storm. It allows you to filter, project, and aggregate on data in transit. It includes a UI and WS. Instead of running queries on a finite set of data that arrived and was persisted or running a static query defined at the startup of the stream, our queries can be executed against an arbitrary set of data arriving after the query is submitted. In other words, it is a look-forward system. \nBullet is a multi-tenant system that scales independently of the data consumed and the number of simultaneous queries. Bullet is pluggable into any streaming data source. It can be configured to read from systems such as Storm, Kafka, Spark, Flume, etc. Bullet leverages Sketches to perform its aggregate operations such as distinct, count distinct, sum, count, min, max, and average.\nAn instance of Bullet is currently running at Yahoo against its user engagement data pipeline. We\u2019ll highlight how it is powering internal use-cases such as web page and native app instrumentation validation. Finally, we\u2019ll show a demo of Bullet and go over query performance numbers.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Avro, Apache Kafka, Apache Storm, Apache ZooKeeper and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Dremio", "name": "Julien Le Dem"}], "base_fname": "The_columnar_roadmap_Apache_Parquet_and_Apache_Arrow", "title": "The columnar roadmap: Apache Parquet and Apache Arrow", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/t_LRA_58qPI"}, "desc": "The Hadoop ecosystem has standardized on columnar formats, Apache Parquet for on disk storage and Apache Arrow for in-memory. With this trend deep integration with columnar formats is a key differentiator for Big Data technologies. Vertical integration from storage to execution greatly improves the latency of accessing data by pushing projections and filters to the storage layer, reducing time spent in IO reading from disk as well as CPU time spent decompressing and decoding. Standards like Arrow and Parquet make this integration even more valuable as data can now cross system boundaries without incurring costly translation. Cross-system programming using languages such as Spark, Python, or SQL can becomes as fast as native internal performance.\nIn this talk we\u2019ll explain how Parquet is improving at the storage level, with metadata and statistics that will facilitate more optimizations in query engines in the future. We\u2019ll detail how the new vectorized reader from Parquet to Arrow enables much faster reads by removing abstractions as well as several future improvements. We will also discuss how standard Arrow based APIs pave the way to breaking the silos of BigData. One example is Arrow based universal function libraries that can be written in any language (Java, Scala, C++, Python, R, ...) and will be usable in any big data system (Spark, Impala, Presto, Drill, \u2026). Another is a Standard data access API with projection and predicate push downs which will greatly simplify data access optimizations across the board.\nFinally we\u2019ll explore reusable execution components that can be built as part of Arrow. \nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Arrow, Apache Parquet, Apache Spark and is geared towards Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Eugene Koifman"}], "base_fname": "Transactional_SQL_in_Apache_Hive", "title": "Transactional SQL in Apache Hive", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/transactional-sql-in-apache-hive"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/Rk8irGDjpuI"}, "desc": "Apache Hive is an Enterprise Data Warehouse build on top of Hadoop.  Hive supports Insert/Update/Delete SQL statements with transactional semantics and read operations that run at Snapshot Isolation. This talk will describe the intended use cases, architecture of the implementation, new features such as SQL Merge statement and recent improvements. The talk will also cover Streaming Ingest API, which allows writing batches of events into a Hive table without using SQL.  This API is used by Apache NiFi, Storm and Flume to stream data directly into Hive tables and make it visible to readers in near real time.\nThis session is a  (Advanced) talk in our Data Processing and Warehousing track. It focuses on Apache Hive and is geared towards Architect, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Chris Drome"}, {"bio": "", "corp": "Yahoo!", "name": "Jin Sun"}], "base_fname": "OracleStore_A_Highly_Performant_RawStore_Implementation_for_Hive_Metastore", "title": "OracleStore: A Highly Performant RawStore Implementation for Hive Metastore", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/oraclestore-a-highly-performant-rawstore-implementation-for-hive-metastore"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/PB8s07CSKu8"}, "desc": "    Today, Yahoo! uses Hive in many different spaces, from ETL pipelines to adhoc user queries. Increasingly, we are investigating the practicality of applying Hive to real-time queries, such as those generated by interactive BI reporting systems. In order for Hive to succeed in this space, it must be performant in all aspects of query execution, from query compilation to job execution. One such component is the interaction with the underlying database at the core of the Metastore.\n\n    As an alternative to ObjectStore, we created OracleStore as a proof-of-concept. Freed of the restrictions imposed by DataNucleus, we were able to design a more performant database schema that better met our needs. Then, we implemented OracleStore with specific goals built-in from the start, such as ensuring the deduplication of data.\n\n    In this talk we will discuss the details behind OracleStore and the gains that were realized with this alternative implementation. These include a reduction of 97%+ in the storage footprint of multiple tables, as well as query performance that is 13x faster than ObjectStore with DirectSQL and 46x faster than ObjectStore without DirectSQL.\nThis session is a  (Advanced) talk in our Data Processing and Warehousing track. It focuses on Apache Hive and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Verizon Wireless", "name": "Arvind Rajagopalan"}, {"bio": "", "corp": "Attunity", "name": "Jordan Martz"}], "base_fname": "Verizon_Centralizes_Data_into_a_Data_Lake_in_Real_Time_for_Analytics", "title": "Verizon Centralizes Data into a Data Lake in Real Time for Analytics", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/verizon-centralizes-data-into-a-data-lake-in-real-time-for-analytics-77194397"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/NHkHT8crEl4"}, "desc": "Verizon \u2013 Global Technology Services (GTS) was challenged by a multi-tier, labor-intensive process when trying to migrate data from disparate sources into a data lake to create financial reports and business insights. Join this session to learn more about how Verizon:\n\u2022         Easily accessed data from multiple sources including SAP data\n\n\u2022         Ingested data into major targets including Hadoop\n\n\u2022         Achieved real-time insights from data leveraging change data capture (CDC) technology\n\n\u2022         Reduced costs and labor\n\nCospeaker: jordan.martz@attunity.com\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Impetus Technologies", "name": "Vineet Tyagi"}], "base_fname": "From_Insights_to_Value_Building_a_Modern_Logical_Data_Lake_To_Drive_User_Adoption_and_Business_Value", "title": "From Insights to Value\u2014Building a Modern Logical Data Lake To Drive User Adoption and Business Value", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/from-insights-to-value-building-a-modern-logical-data-lake-to-drive-user-adoption-and-business-value"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/549MkIrYiTc"}, "desc": "Businesses often have to interact with different data sources to get a unified view of the business or to resolve discrepancies. These EDW data repositories are often large and complex, are business critical, and cannot afford downtime. This session will share best practices and lessons learned for building a Data Fabric on Spark / Hadoop / HIVE/ NoSQL that provides a unified view, enables a simplified access to the data repositories, resolves technical challenges and adds business value. Businesses often have to interact with different data sources to get a unified view of the business or to resolve discrepancies. These EDW data repositories are often large and complex, are business critical, and cannot afford downtime. This session will share best practices and lessons learned for building a Data Fabric on Spark / Hadoop / HIVE/ NoSQL that provides a unified view, enables a simplified access to the data repositories, resolves technical challenges and adds business value.\nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, NoSQL, OLAP and is geared towards CXO, Architect, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Twitter", "name": "Gera Shegalov"}, {"bio": "", "corp": "Twitter", "name": "Piyush Narang"}], "base_fname": "Slim_Scalding_less_memory_is_more_capacity", "title": "Slim Scalding - less memory is more capacity", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/slim-scalding-less-memory-is-more-capacity"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/L9PAPYMAGM4"}, "desc": "Scalding is the main analytics framework at Twitter. The amount and the complexity of Scalding jobs run at Twitter increases every day. In particular, the experimentation pipeline has to meet the growing demand of the product team testing more and more innovations on a daily basis. We have made significant progress in scaling our Hadoop cluster for both compute and storage to meet the increasing demand. However, the infrastructure of such magnitude is anything but free. We exercise great discipline in keeping the capacity demands in check by enforcing filesystem (HDFS) quotas and compute \u201cquotas\u201d in terms of YARN scheduler queues. Although YARN resource manager supports multiple resource dimensions including CPU, we use primarily memory as the most battle-tested and intuitive option to manage cluster capacity. In many ways YARN is similar to the operating system managing memory of a single device be it a smartphone or a laptop via memory allocations. If requested memory request can be satisfied the app proceeds, otherwise it\u2019s queued. The difference is that YARN manages memory of thousands of machines and an app such as a Scalding MapReduce flow consists of distributed processes instead of local threads. Delays in the pipeline execution often have the cost of lost revenue in addition to the operational cost. \nThe challenge is that the users have to specify the memory demands of their MapReduce flows upfront leading to vast overestimation and wasted resources in order to avoid being paged for production job failures that would have been caused by underestimation. Moreover the granularity of memory settings can exacerbate this issue. For instance, Scalding currently accepts memory settings solely at the flow level. Thus, Scalding users face a tradeoff between the complexity of ensuring that the flow steps are grouped in subflows with equivalent memory requirements for each step, and much more manageable simpler flows with higher memory oversubscription. A similar issue exists within a single MapReduce job as well. MapReduce accepts a single setting per task type, i.e., \u201cmap\u201d and \u201creduce\u201d. However, most real life jobs are susceptible to a skew and users end up accommodating the most memory-intensive task. We cannot do anything about the latter inherent limitation of MapReduce but we devised a solution for the former Scalding issue.  \nIn this talk we'll first describe the methodology of finding the perfect fit container memory using monotonically growing Java heap, and the committed heap memory counter. We'll further explain the challenges of applying it at scale even of a single flow, let alone a complex pipeline comprised of 100s of DAG nodes. And finally we'll show how this can be solved at Twitter scale in a fully automated fashion using job counters from prior executions stored in a special service based on hRaven or YARN Timeline Service v.2. Our v1 implementation provides a memory estimator class in Scalding that is able to auto-tune memory of individual MapReduce steps in contrast to the standard Scalding configuration mechanism at the flow level. This is possible because hRaven can identify individual nodes in job DAG flows, and the config change can be triggered while building the Scalding flow. Note that this solution is not limited to Scalding and can equally be generalized to other DAG processors such as Hive and Pig. Neither it\u2019s limited to MapReduce as the underlying engine, but can also be applied with some limitations (e.g. no JVM reuse) to Tez. Spark\u2019s executors are probably less amenable to this solution but we haven\u2019t fully explored it yet. The estimated effect of applying this technique is a 50% reduction in memory requested by Scalding pipelines at Twitter. \nIn some cases the problem is the opposite one. Pipeline engineers might have done a very good job tuning memory initially, but with the time the increasing record size or something else triggered higher utilization of Java heap. Eventually this organic growth leads to high GC CPU percentage and even OutOfMemory (OOM)  errors. The latter is the better outcome because it will force the responsible team to address the problem. In the worst case, the GC issue will slow down the job significantly without throwing OOM \u201cGC overhead limit exceeded error\u201d. SlimScalding can resolve this by looking at the ratio GC CPU over Total CPU, and increase the requested memory if the ratio is over some reasonable threshold, e.g., 10%.\nWe anticipate that in the future scalding slimming will also be needed in the case where CPU is a scheduling dimension as well. Users will tend to overallocating vcores, and SlimScalding can determine a more accurate vcore estimate by checking the CPU counters.\nThis session is a  (Advanced) talk in our Cloud and Operations track. It focuses on Apache Hadoop, Other and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Comcast", "name": "Ray Harrison"}, {"bio": "", "corp": "Comcast", "name": "Dushyanth Vaddi"}], "base_fname": "Implementing_Security_on_a_Large_Multi_Tenant_Cluster_the_Right_Way", "title": "Implementing Security on a Large Multi-Tenant Cluster the Right Way", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/implementing-security-on-a-large-multitenant-cluster-the-right-way"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/MUfIS8h9498"}, "desc": "Raise your hands if you are deploying Kerberos and other Hadoop security components after deploying Hadoop to the enterprise. We will present the best practices and challenges of implementing security on a large multi-tenant Hadoop cluster spanning multiple data centers. Additionally, we will outline our authentication & authorization security architecture, how we reduced complexity through planning, and how we worked with multiple teams and organizations to implement security the right way the first time. We will share lessons learned and takeaways for implementing security at your company.\nWe will walk through the implementation and its impacts to the user, development, support and security communities and will highlight the pitfalls that we navigated to achieve success. Protecting your customers and  information assets is critical to success. If you are planning to introduce Hadoop security to your ecosystem, don\u2019t miss this in depth discussion on a very important and necessary component to enterprise big data. \nThis session is a  talk in our Governance and Security track. It focuses on Apache Knox, Apache Ranger, Kerberos and is geared towards CXO, Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Oracle", "name": "Diby Malakar"}], "base_fname": "Insights_into_Real_world_Data_Management_Challenges", "title": "Insights into Real-world Data Management Challenges", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/insights-into-realworld-data-management-challenges"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/W0_vcgVhy_8"}, "desc": "Oracle began with the belief that the foundation of IT was managing information. The Oracle Cloud Platform for Big Data is a natural extension of our belief in the power of data. Oracle\u2019s Integrated Cloud is one cloud for the entire business, meeting everyone\u2019s needs. It\u2019s about Connecting people to information through tools which help you combine and aggregate data from any source.\n\nThis session will explore how organizations can transition to the cloud by delivering fully managed and elastic Hadoop and Real-time Streaming cloud services to built robust offerings that provide measurable value to the business. We will explore key data management trends and dive deeper into pain points we are hearing about from our customer base.  \nThis session is a  talk in our Cloud and Operations track. It focuses on Cloud and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "LinkedIn", "name": "Carl Steinbach"}], "base_fname": "The_Past_Present_and_Future_of_Hadoop_at_LinkedIn", "title": "The Past, Present, and Future of Hadoop at LinkedIn", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/S63SfNzFBzw"}, "desc": "This talk is about the past, present, and future of Hadoop at LinkedIn. It is story that begins back in 2008 with a group of renegade engineers cobbling together their first cluster from a collection of mismatched Solaris boxes. I'll describe early big successes that generated confidence, and the growing pains caused by new use cases and new users, and how we responded to explosive increases in capacity requirements. I will also share the nuggets of wisdom we learned along the way: how to support a demanding user population, how to limit and prevent the growth of technical debt, and the factors we take into account when making bets on new technologies like Spark, Presto, and TensorFlow.\nThis session is a  (Beginner) talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Kafka, Apache ORC, Apache Spark, Apache Tez and is geared towards Architect audiences."},
{"speakers": [{"bio": "", "corp": "kpmg", "name": "kevin martelli"}, {"bio": "", "corp": "Freddie Mac", "name": "Lakshmi Purushothaman"}], "base_fname": "A_Freddie_Mac_and_KPMG_Case_Study_PySpark_for_Advanced_Analytics_and_Insights_Over_Semi_Structured_Data", "title": "A Freddie Mac and KPMG Case Study: PySpark for Advanced Analytics and Insights Over Semi-Structured Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/a-freddie-mac-and-kpmg-case-study-pyspark-for-advanced-analytics-and-insights-over-semistructured-data"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/ct6gydYAQr4"}, "desc": "Freddie Mac and KPMG have developed a common, generic data engineering framework to increase purchase certainty, monitor risk and offer new capabilities. The execution of repeatable analytics against multi-dimensional, semi-structured XML data sets leverages Jupyter Notebook as well as core Apache components of Hortonworks Data Platform (such as Spark, Hive, Oozie and Zeppelin). By leveraging PySpark and other tools, the solution provides faster and easier data processing. The resulting analytics allow Freddie to extract knowledge and insights to roll out new product capabilities. The application runs the processes in a highly distributed and memory-intensive framework to reduce processing time.  This high-level overview of the Freddie Mac Big Data Solution will share best practices to generically process semi-structured data while retaining the complex structures needed by data scientists and teams focused on advanced analytics.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Ambari, Apache Hadoop, Apache Hive, Apache Oozie, Apache Spark and is geared towards Architect, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Enterprise Holdings", "name": "Kit Menke"}, {"bio": "", "corp": "Hortonworks", "name": "Scott Shaw"}], "base_fname": "Innovation_in_the_Enterprise_Rent_A_Car_Data_Warehouse", "title": "Innovation in the Enterprise Rent-A-Car Data Warehouse", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/innovation-in-the-enterprise-rentacar-data-warehouse"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/WMHVV34wPqM"}, "desc": "Big Data adoption is a journey. Depending on the business the process can take weeks, months, or even years. With any transformative technology the challenges have less to do with the technology and more to do with how a company adapts itself to a new way of thinking about data. Building a Center of Excellence is one way for IT to help drive success. \nThis talk will explore Enterprise Holdings Inc. (which operates the Enterprise Rent-A-Car, National Car Rental and Alamo Rent A Car) and their experience with Big Data. EHI\u2019s journey started in 2013 with Hadoop as a POC and today are working to create the next generation data warehouse in Microsoft\u2019s Azure cloud utilizing a lambda architecture.\nWe\u2019ll discuss the Center of Excellence, the roles in the new world, share the things which worked well, and rant about those which didn\u2019t.\nNo deep Hadoop knowledge is necessary, architect or executive level.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Cloud and is geared towards CXO, Architect audiences."},
{"speakers": [{"bio": "", "corp": "IBM", "name": "Jun Yang"}, {"bio": "", "corp": "IBM", "name": "Jing Chen (Jerry) He"}], "base_fname": "Enterprise_large_scale_graph_analytics_and_computing_base_on_distribute_graph_database_Titan_DB_Hbase_Solr_and_distribute_graph_computing_in_memory_TinkerPop_Hadoop_Gremlin_sparkgraphcomputer_and_Hadoop2", "title": "Enterprise large scale graph analytics and computing base on distribute graph database(Titan DB Hbase/Solr) and distribute graph computing in memory(TinkerPop Hadoop Gremlin sparkgraphcomputer) and Hadoop2", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/enterprise-large-scale-graph-analytics-and-computing-base-on-distribute-graph-database-titan-db-hbasesolr-and-distribute-graph-computing-in-memory-tinkerpop-hadoop-gremlin-sparkgraphcomputer-and-hadoop2"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/m27jCLbBgZQ"}, "desc": "Graph approaches to structuring, analyzing data have been a significant area of interest, Graphs are well-suited to expressing complex interconnections and clusters of highly related entities.\n\nLarge-scale graph analytics research is growing fast in recent years, to leverage Hadoop2 ecosystem for graph is a good approach, enterprise graph computer requires to store large graph and do fast computing against graph. One for the OLTP database systems which allow the user to query the graph in real-time, Hbase as the distributed NOSql database can be the backend storage to persistent large graph, the property graph stored its vertices and edges in key-value pairs in Hbase, it also provide highly reliable, scalable and fault tolerant to the data, Solr as the distributed indexing will make the query more efficient. Titan itself will handle cache, transaction; And another for the OLAP analytics systems, use TinkerPop hadoop gremlin SparkGraphComputer to processed a large graph, every vertex and edge is analyzed, a cluster-computing platform will help for the processing of large distributed in memory graph datasets.\n\nGraph DB base on Hbase/Solr and graph computing analysis base on spark is powerful for discovering valuable information about relationships in complex and large data, representing significant business opportunity in enterprise. It will help graph data analytics in a wide range of domains such as social networking, recommendation engines, advertisement optimization, knowledge representation, health care, education, and security.\nThis session is a  (Beginner) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache HBase, Apache Spark, Apache Solr and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Rajeshbabu Chintaguntla"}, {"bio": "", "corp": "Hortonworks", "name": "Enis Soztutar"}], "base_fname": "Meet_HBase_2_0_and_Phoenix_5_0", "title": "Meet HBase 2.0 and Phoenix 5.0", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/meet-hbase-20-and-phoenix-50"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/6K7W2Osn-9E"}, "desc": "This talk with give and overview of exciting two releases for Apache HBase and Phoenix. HBase 2.0 is the next stable major release for Apache HBase scheduled for early 2017. It is the next evolution from the Apache HBase community after 1.0. HBase-2.0 contains a large number of features that is long time in the development, some of which include rewritten region assignment, perf improvements (RPC, rewritten write pipeline, etc), async clients, C++ client, offheaping memstore and other buffers, Spark integration, shading of dependencies as well as a lot of other fixes and stability improvements. We will go into technical details on some of the most important improvements in the release, as well as what are the implications for the users in terms of API and upgrade paths. Phoenix 5.0 is the next biggest and most exciting milestone release because of Phoenix integration with Apache Calcite which ads lot of performance benefits with new query optimizer and helps to integrate with other data sources, especially those also based on calcite. It has lot of cool features such as Encoded columns, Kafka, Hive integration, improvements in secondary index rebuilding and many performance improvements.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache HBase, Apache Phoenix and is geared towards Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "TMW Systems", "name": "Timothy Leonard"}], "base_fname": "How_Market_Intelligence_From_Hadoop_on_Azure_Shows_Trucking_Companies_a_Clear_Road_to_Profitability", "title": "How Market Intelligence From Hadoop on Azure Shows Trucking Companies a Clear Road to Profitability", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/how-market-intelligence-from-hadoop-on-azure-shows-trucking-companies-a-clear-road-to-profitability"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/QwpzKISrfKI"}, "desc": "TMW Systems (a Trimble Company) has been in the business of long-haul trucking, logistics operations and fleet management for more than thirty years, but we wanted more data, so we turned to our customer community. Now, we turn that data into market intelligence, which we then provide back to our customers. To do this, we invested heavily in Hortonworks Data Platform running on Microsoft Azure in the cloud. In our talk, we\u2019ll share our strategy for capturing operational, maintenance, financial and mobile communications information and how we provide that back to our customer base. Our approach enables advanced analytics by leveraging Big Data technologies to find new relationships in data that may have been previously overlooked. Survey responses capture business performance metrics, strategy and emerging trends from 150 businesses, representing more than 31 billion dollars in freight movement. Learn how we combine that survey data with other sources like machine and sensor data to help guide our customers to profitability.\nThis session is a  talk in our Apache Spark and Data Science track. It focuses on Apache Ambari, Apache Hive, Apache Ranger, Apache Tez and is geared towards CXO, Architect, Data Scientist audiences."},
{"speakers": [{"bio": "", "corp": "Spark Technology Center", "name": "Jeremy Nixon"}], "base_fname": "Convolutional_Neural_Networks_at_scale_in_Spark_MLlib", "title": "Convolutional Neural Networks at scale in Spark MLlib", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/convolutional-neural-networks-at-scale-in-spark-mllib"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": ""}, "desc": "Jeremy Nixon will focus on the engineering and applications of a new algorithm built on top of MLlib. The presentation will focus on the methods the algorithm uses to automatically generate features to capture nonlinear structure in data, as well as the process by which it\u2019s trained. Major aspects of that are the compositional transformations over the data, convolution, and distributed backpropagation via SGD with adaptive gradients and an adaptive learning rate. Applications will look into how to use convolutional neural networks to model data in computer vision, natural language and signal processing. Details around optimal preprocessing, the type of structure that can be learned, and managing its ability to generalize will inform developers looking to apply nonlinear modeling tools to problems that they face.\nThis session is a  talk in our Apache Spark and Data Science track. It focuses on Apache Spark and is geared towards Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Shane Kumpf"}, {"bio": "", "corp": "Hortonworks", "name": "Jian He"}], "base_fname": "Running_a_container_cloud_on_YARN", "title": "Running a container cloud on YARN", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/running-a-container-cloud-on-yarn"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/kpXN-rZUmDU"}, "desc": "Apache Hadoop YARN is the resource and application manager for Apache Hadoop. In the past, YARN only supported launching containers as processes. However, as containerization has become extremely popular, more and more users wanted support for launching Docker containers. With recent changes to YARN, it now supports running Docker containers alongside process containers. Couple this with the newly added support for running services on YARN and it allows a host of new possibilities. In this talk, we'll present how to run a potential container cloud on YARN.  Leveraging the support in YARN for Docker and services, we can allow users to spin up a bunch of Docker containers for their applications. These containers can be self contained or wired up to form more complex applications(using the Assemblies support in YARN). We will go over some of the lessons we learned as part of our experiences handling issues such as resource management, debugging application failures,  running Docker, etc. \nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Docker / Container and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "IBM", "name": "Alex Yang"}, {"bio": "", "corp": "IBM", "name": "Jun Wang"}], "base_fname": "Alerting_Real_time_Irregular_Traffic_Patterns_in_Spark_with_Automatic_Data_profiling_and_Machine_Learning", "title": "Alerting Real-time Irregular Traffic Patterns in Spark with Automatic  Data-profiling and Machine Learning", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/alerting-realtime-irregular-traffic-patterns-in-spark-with-automatic-dataprofiling-and-machine-learning"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/GAUckuBjIRY"}, "desc": "With the growing use of real time data, it is highly desired to alert irregular traffic patterns to both traffic management  and the public of the modern cities.\nIn this session, we present a Spark application that alerts in real-time irregular events with a predicted event type using a pre-built prediction model for the transportation network in a large city in China. Real-time traffic data (such as GPS, RFID, and surveillance video) are processed via Spark Streaming and Kafka, and automatically profiled into discrete traffic indicators and patterns per road section, the result of which are categorized against historic patterns to determine the irregularity of the incoming pattern based upon the data profile of the road section. An event type is therefore be predicted for a found irregular event using a pre-built spatial-temporal prediction algorithm.  \nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Hadoop, Apache Kafka, Apache Spark and is geared towards Architect, Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "Vizient", "name": "Chuck DeVries"}], "base_fname": "Powering_self_service_discovery_with_Hadoop_and_Data_Virtualization", "title": "Powering self-service discovery with Hadoop and Data Virtualization", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/powering-selfservice-discovery-with-hadoop-and-data-virtualization"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/Ca3zSyDhVcc"}, "desc": "Vizient delivers smart data-driven resources and insights from benchmarking and predictive analytics to cost-savings for their members. The firm employs a modern data architecture utilizing Hadoop with Data Virtualization to power their data discovery and analytics initiatives.\nDiscover Vizient\u2019s success in:\n\u00b7         Helping members apply data and insights in new ways to  achieve sustainable results\n\n\u00b7         Integrate Member Spend and Supplier Sales data from all Vizient organizations to identify opportunities for increasing contract utilization\n\n\u00b7         Enable Single source-of-truth and consistent view of data from distributed data assets\nDon\u2019t miss the opportunity to learn from this healthcare innovator!\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Hive, Cloud, Other and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Facebook", "name": "Martin Traverso"}, {"bio": "", "corp": "Teradata", "name": "Matthew Fuller"}], "base_fname": "Presto_SQL_on_anything", "title": "Presto: SQL-on-anything", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/presto-sqlonanything"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/QcLJvSxa_OA"}, "desc": "Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook. One key feature in Presto is the ability to query data where it lives via a uniform ANSI SQL interface. Presto\u2019s connector architecture creates an abstraction layer for anything that can be expressed in a row-like format, such as HDFS, Amazon S3, Azure Storage, NoSQL stores, relational databases, Kafka streams and even proprietary data stores. Furthermore, a single Presto query can combine data from multiple sources, allowing for analytics across your entire organization.\nThis talk will be co-presented by Facebook and Teradata, the two largest contributors to Presto. The talk will focus on Presto\u2019s ability to query virtually any data source via it\u2019s connector interface. Facebook and Teradata will present some of their use cases of Presto querying various data sources, discuss the existing connectors in Presto, and describe the anatomy of a connector.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Cassandra, Apache Hadoop, Apache Hive, Apache Kafka, Apache ORC and is geared towards Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Casey Stella"}], "base_fname": "MaaS_Model_as_a_Service_Modern_Streaming_Data_Science_with_Apache_Metron_Incubating", "title": "MaaS (Model as a Service): Modern Streaming Data Science with Apache Metron (Incubating)", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/maas-model-as-a-service-modern-streaming-data-science-with-apache-metron-incubating"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/LkrOKvyAc0s"}, "desc": "Apache Metron (Incubating) is a streaming cybersecurity application\n\nbuilt on Apache Storm and Hadoop. One of its core missions is to enable\n\nadvanced analytics through machine learning and data science to the\n\nusers.  Because of the relative immaturity of data science platform\n\ninfrastructure integrated into Hadoop that is oriented to streaming\n\nanalytics applications, we have been forced to create the requisite\n\nplatform components out of necessity, utilizing many of the pieces of\n\nthe Hadoop ecosystem.\nIn this talk, we will speak about the Metron analytics architecture and\n\nhow it utilizes a custom data science model deployment and autodiscovery\n\nservice that is tightly integrated with Hadoop via Yarn and Zookeeper.\n\nWe will discuss how we interact with the models deployed there via a\n\ncustom domain specific language that can query models as data streams\n\npast.  We will generally discuss the full-stack data science tooling that\n\nhas been created to enable data science at scale on an advanced analytics\n\nstreaming application.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache HBase, Apache Metron, Apache ZooKeeper and is geared towards Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "Western Digital", "name": "Thomas Demoor"}, {"bio": "", "corp": "Microsoft", "name": "Virajith Jalaparti"}], "base_fname": "HDFS_Tiered_Storage_Mounting_Object_Stores_in_HDFS", "title": "HDFS Tiered Storage: Mounting Object Stores in HDFS", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/hdfs-tiered-storage-mounting-object-stores-in-hdfs-77189018"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/kpNDZNp-Nlw"}, "desc": "Most users know HDFS as the reliable store of record for big data analytics. HDFS is also used to store transient and operational data when working with cloud object stores, such as Azure HDInsight and Amazon EMR. In these settings- but also in more traditional, on premise deployments- applications often manage data stored in multiple storage systems or clusters, requiring a complex workflow for synchronizing data between filesystems to achieve goals for durability, performance, and coordination.\nBuilding on existing heterogeneous storage support, we add a storage tier to HDFS to work with external stores, allowing remote namespaces to be \"mounted\" in HDFS. This capability not only supports transparent caching of remote data as HDFS blocks, it also supports synchronous writes to remote clusters for business continuity planning (BCP) and supports hybrid cloud architectures.\nThis idea was presented at last year\u2019s Summit in San Jose. Lots of progress has been made since then and the feature is in active development at the Apache Software Foundation on branch HDFS-9806, driven by Microsoft and Western Digital. We will discuss the refined design & implementation and present how end-users and admins will be able to use this powerful functionality.\nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Cloud and is geared towards CXO, Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Health Care Service", "name": "Andy Ashta"}], "base_fname": "It_Takes_a_Village_Organizational_Alignment_to_Deliver_Big_Data_Value_in_Health_Insurance", "title": "It Takes a Village: Organizational Alignment to Deliver Big Data Value in Health Insurance", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/it-takes-a-village-organizational-alignment-to-deliver-big-data-value-in-health-insurance"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/uyDO92WdkJI"}, "desc": "The business and technology teams within a health insurer must align the company\u2019s central data platform with its data strategy. That requires substantial organizational alignment.  Hear the firsthand perspective from Health Care Service Corporation (HCSC), the largest customer-owned health insurance company in the United States. The speaker will cover how they integrated membership information, regulatory compliance, and the general ledger, to improve overall healthcare management.  At HCSC, the strong alignment between executive leadership, business portfolio direction, architectural strategy, technology delivery, and program management have helped create leading-edge capabilities which help the company respond nimbly to a quickly evolving healthcare industry.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Atlas, Apache Hadoop, Apache Nifi and is geared towards CXO, Architect audiences."},
{"speakers": [{"bio": "", "corp": "Databricks", "name": "Tathagata Das Das"}], "base_fname": "Easy_Scalable_Fault_tolerant_stream_processing_with_Structured_Streaming_in_Apache_Spark", "title": "Easy, Scalable, Fault-tolerant stream processing with Structured Streaming in Apache Spark", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/easy-scalable-faulttolerant-stream-processing-with-structured-streaming-in-apache-spark"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/ARH-xa-2PMY"}, "desc": "Last year, in Apache Spark 2.0, we introduced Structured Steaming, a new stream processing engine built on Spark SQL, which revolutionized how developers could write stream processing application. Structured Streaming enables users to express their computations the same way they would express a batch query on static data. Developers can express queries using powerful high-level APIs including DataFrames, Dataset and SQL. Then, the Spark SQL engine is capable of converting these batch-like transformations into an incremental execution plan that can process streaming data, while automatically handling late, out-of-order data, and ensuring end-to-end exactly-once fault-tolerance guarantees.\nSince Spark 2.0 we've been hard at work building first class integration with Kafka.  With this new connectivity, performing complex, low-latency analytics is now as easy as writing a standard SQL query.  This functionality in addition to the existing connectivity of Spark SQL make it easy to analyze data using one unified framework.  Users can now seamlessly extract insights from data, independent of whether it is coming from messy / unstructured files, a structured / columnar historical data warehouse or arriving in real-time from pubsub systems like Kafka and Kinesis.\nWe'll walk through a concrete example where in less than 10 lines, we read Kafka, parse JSON payload data into separate columns, transform it, enrich it by joining with static data and write it out as a table ready for batch and ad-hoc queries on up-to-the-last-minute data. We'll use techniques including event-time based aggregations, arbitrary stateful operations, and automatic state management using event-time watermarks.\nThis session is a  (Beginner) talk in our Apache Spark and Data Science track. It focuses on Apache Kafka, Apache Parquet, Apache Spark and is geared towards Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Edgar Orendain"}], "base_fname": "Building_a_modern_end_to_end_open_source_Big_Data_reference_application", "title": "Building a modern end-to-end open source Big Data reference application", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/building-a-modern-endtoend-open-source-big-data-reference-application"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/6LJkesKTiKs"}, "desc": "In this talk, Edgar Orendain walks through a modern real-time streaming application serving as a reference framework for developing a big data pipeline, complete with a broad range of use cases and powerful reusable core components.\nModern applications can ingest data and leverage analytics in real-time.  These analytics are based on machine learning models typically built using historical big data.\u00a0\u00a0This reference application provides examples of connecting data-in-motion analytics to your application based on Big Data.\nWe review code, best practices and considerations involved when integrating different components into a complete data platform.  From IoT sensor data collection, to flow management, real-time stream processing and analytics, through to machine learning and prediction, this reference project aims to help developers seed their own open source solutions \u2013 fast.\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Ambari, Apache Kafka, Apache Nifi, Apache Spark, Apache Storm and is geared towards Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "AsiaInfo", "name": "Geng Wang"}, {"bio": "", "corp": "AsiaInfo", "name": "Dong Wang"}], "base_fname": "a_Real_time_Processing_System_based_on_Spark_streaming_in_the_filed_of_Telecommunications", "title": "a Real-time Processing System based on Spark streaming in the filed of Telecommunications", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/a-realtime-processing-system-based-on-spark-streaming-int-he-field-of-telecommunications"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/Ek3PRwiQBW4"}, "desc": "With the development of human social life, the city is undertaking an increasing population. Smart city is aimed at collecting, analyzing, integrating the key indicators of city operation through information technology, so as to make intelligent response to various needs including the people's livelihood, environmental protection, public security, city services, industrial and commercial activities. Recently the development of cloud computing, big data, internet of things technology, smart city is gradually evolving from the concept to a technology which can thoroughly change people's lives.\n\nWe have built a Streaming processing system (OCSP) based on Spark streaming, which has been used in two key fields (Location operation system and real time Marketing) in China Mobile corporation. The system used Spark streaming, Kafka, Flume, Redis technologies etc, and processed 30 million  data records (type of location) per minute and 40 million (type of operation)  data records per minute. The data comes from the real-time  use of  mobile phone from 60 million end users. After processing , the processed data are outputted  to Kafka for other applications to use.  OCSP provides APIs for developer to develop the applications for different use.  Developers don't need to know the detail of RDD, streaming concept or other concepts in Spark. They just  focus the logic implementation of the business.\n\nWe will describe the key technologies of OCSP system, then introduces large quantities of real- time data analysis and processing technology in smart tourism, and real-time processing data modeling methods. The technology has the characteristics of high real-time, high reliability, high accuracy of data processing, and has strong applicability, which be extended to other large data real-time processing scenarios.\nThis session is a  (Advanced) talk in our IoT and Streaming track. It focuses on Apache Flume, Apache Hadoop, Apache Kafka, Apache Nifi, Apache Spark and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Marton Elek"}], "base_fname": "From_Open_Source_to_Enterprise_Readiness_Maintaining_Hadoop_Distribution", "title": "From Open Source to Enterprise Readiness \u2013 Maintaining Hadoop Distribution", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/from-open-source-to-enterprise-readiness-maintaining-hadoop-distribution"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/K8r3LWbgXrE"}, "desc": "Hadoop distributions contain numerous Open Source components which support data processing at large scale (Hadoop, Hive, Spark, etc). These projects are developed and maintained as Top Level Apache Software Projects which provides them the freedom to innovate and improve in a liberal way.\nAs an example, HDP\u2019s key advantage is the rock-solid quality and integration of the shipped software.\n\nIn this talk we will provide insights about the process which enables scheduled maintenance releases and yields enterprise ready open source software distribution.\nTopics:\n\nTriaging: identifying the finest selection of patches from upstream\n\nCertification process: System, regression, upgrade & unit testing\n\nStack validation using long-running workloads\n\nApache License Compliance\nThis session is a  (Beginner) talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Spark, Other and is geared towards Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "GE", "name": "Kishore Reddipalli"}], "base_fname": "Optimizing_industrial_operations_using_the_big_data_ecosystem", "title": "Optimizing industrial operations using the big data ecosystem", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/optimizing-industrial-operations-using-the-big-data-ecosystem"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/pB56eEWnTc4"}, "desc": "GE Digital is undertaking a journey to optimize the reliability, availability, and efficiency of assets in the industrial sector and converge IT and OT. To do so, GE Digital is building cloud-based products that enable customers to analyze the asset data, detect anomalies, and provide recommendations for operating plants efficiently while increasing productivity. In a energy sector such as oil and gas, power, or renewables, a single plant comprises multiple complex assets, such as steam turbines, gas turbines, and compressors, to generate power. Each system contains various sensors to detect the operating conditions of the assets, generating large volumes of variety of data. A highly scalable distributed environment is required to analyze such a large volume of data and provide operating insights in near real time.\n\nIn this session I will share the challenges encountered when analyzing the large volumes of data, in-stream data analysis and how we standardized the industrial data based on  data frames, and  performance tuning.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Hadoop, Apache Hive, Apache Kafka, Apache Spark and is geared towards Architect, Data Scientist, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "eBay", "name": "Gurpreet Singh"}, {"bio": "", "corp": "eBay", "name": "Ying Zhang"}], "base_fname": "Entity_Resolution_Service_Bringing_Petabytes_of_Data_Online_for_Instant_Access", "title": "Entity Resolution Service - Bringing Petabytes of Data Online for Instant Access.", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/entity-resolution-service-bringing-petabytes-of-data-online-for-instant-access"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/DjZNXpNrM-k"}, "desc": "2.5B+ ids, 2ms latency, 15K+ TPS and Petabytes of data.These numbers outline the challenges with eBay\u2019s Entity Resolution Service (ERS). ERS provides a temporal map between anyid-anyid. The technology stack of ERS has Hadoop as the batch layer, Couchbase as cache layer, Spring Batch to load data to Couchbase and Rest API at Service layer. In our presentation we will take you through the journey from conceptual to production release. It\u2019s a great story and we would like to share with you!\nThis session is a  talk in our Applications track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, Cloud, NoSQL and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Intulog", "name": "Gurudev Karanth"}, {"bio": "", "corp": "Target", "name": "Karthik Rajagopalan"}], "base_fname": "Batch_and_Streaming_Analytics_for_Prescriptive_Predictive_Solutions", "title": "Batch and Streaming Analytics for Prescriptive & Predictive Solutions", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/-gpFr8oGs1g"}, "desc": "The explosion in data technologies have led to enormous possibilities and equal number of challenges. Today for any organization with the data at hand, the scope of the problem is ever increasing from infrastructure management to user behavior analysis and beyond. We will talk through typical challenges and considerations while putting together a data stack \u2013 message queue/brokers, stream processing, persistence and visualization. We\u2019ll compare and contrast considerations using batch applications v/s real time applications. We will cover critical technologies needed in your stack and how do you go about selecting the right tools for.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Avro, Apache Cassandra, Apache Hadoop, Apache Kafka, Druid and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Yanbo Liang"}], "base_fname": "SparkR_best_practices_for_R_data_scientist", "title": "SparkR best practices for R data scientist", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/sparkr-best-practices-for-r-data-scientist"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/AFt81N8-Q1w"}, "desc": "Spark plays an important role on data scientists to solve all kinds of problems, especially the release of SparkR which provide very friendly APIs for traditional data scientists. However, processing various data size, data format and models will lead to different application patterns compared with traditional R. In this talk, we will illustrate the practical experience that using SparkR to solve some typical data science problems, such as the performance improvement for SparkR and native R interoperation, how to load data from HBase which is a very common data source efficiently, how to schedule a large scale machine learning job with multiple single R machine learning jobs, how to tuning performance for jobs triggered by many different users, how to use SparkR in the cloud-based environment, etc. At last, we will shortly introduce the community efforts in progress on SparkR in the coming releases.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Hive, Apache HBase, Apache Spark, Cloud and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Srikanth Venkat"}, {"bio": "", "corp": "Hortonworks", "name": "Vinay Shukla"}], "base_fname": "Don_t_Let_the_Spark_Burn_Your_House_Perspectives_on_Securing_Spark", "title": "Don\u2019t Let the Spark Burn Your House: Perspectives on Securing Spark", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/dont-let-the-spark-burn-your-house-perspectives-on-securing-spark"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/LkFy9mz-_20"}, "desc": "Apache Spark is emerging as a key enabler for various enterprise use cases including customer intelligence applications, data warehousing, real-time or streaming, recommendation engines, and log processing. Even the most common use case for Spark around business intelligence (BI) or customer intelligence applications via data science encompasses the complete data worker lifecycle from file processing, workflows, cleansing, enrichment, model building and deployments to dash boarding and reporting. However, many aspects of security and governance with Spark are still emerging and pose challenges to enterprise adoption including areas of authorization, authentication, and comprehensive auditing as well as metadata harvesting and governance. We will demonstrate some examples of the current the state of the art in terms of different open source approaches to Spark security and governance. For example, we will show how Spark technologies can be integrated with enterprise identity providers, and how we can enable fine-grained access control for processes, and how to harvest process metadata while providing detailed audits. We will also provide best practices and common usage patterns to secure your Spark clusters and how best to support enterprise compliance and governance needs when using Spark.\nThis session is a  (Advanced) talk in our Governance and Security track. It focuses on Apache Atlas, Apache Knox, Apache Ranger, Apache Spark, Apache Zeppelin and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "ERIC THORSEN"}], "base_fname": "Big_Data_Maturity_Scorecard", "title": "Big Data Maturity Scorecard", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/big-data-maturity-scorecard"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/ggm8l8KJA3Q"}, "desc": "Transforming your company into a data-driven and data-aware company can be complex. Everything from knowing where to start, to executive buy-in, to grandfathered processes can slow data maturity and business growth. The journey begins with understanding the opportunities unique to your business based on your level of data maturity.\nIn this session, we will share findings and insights from customers, how they used this to secure executive sponsorship to ensure the data technology and business requirements were in tandem, as well as the use cases typically pursued. We will discuss the typical organizational constructs we see applicable based on the different stages of maturity and also discuss some best practices for driving best in class process for data driven transformation.\nThe explosion of data is catalyzing new business models and reshaping industries. No longer can you amble your way forward in the age of Big Data; the challenges are too great to address on an ad-hoc basis and the business potential too vast to simply dismiss.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Other and is geared towards CXO, Architect audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Jing Zhao"}, {"bio": "", "corp": "Hortonworks", "name": "Tsz-Wo Nicholas Sze"}], "base_fname": "Scaling_HDFS_to_Manage_Billions_of_Files_with_Distributed_Storage_Schemes", "title": "Scaling HDFS to Manage Billions of Files with Distributed Storage Schemes", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/scaling-hdfs-to-manage-billions-of-files-with-distributed-storage-schemes-77188691"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/htObyW0pMZE"}, "desc": "Hadoop Distributed File System (HDFS) evolves from a MapReduce-centric storage system to a generic, cost-effective storage infrastructure where HDFS stores all data of inside the organizations. The new use case presents a new sets of challenges to the original HDFS architecture.  One challenge is to scale the storage management of HDFS - the centralized scheme within NameNode becomes a main bottleneck which limits the total number of files stored. Although a typical large HDFS cluster is able to store several hundred petabytes of data, it is inefficient to handle large amounts of small files under the current architecture.\nIn this talk, we introduce our new design and in-progress work that re-architects HDFS to attack this limitation. The storage management is enhanced to a distributed scheme. A new concept of storage container is introduced for storing objects. HDFS blocks are stored and managed as objects in the storage containers instead of being tracked only by NameNode. Storage containers are replicated across DataNodes using a newly-developed high-throughput protocol based on the Raft consensus algorithm. Our current prototype shows that under the new architecture the storage management of HDFS scales 10x better, demonstrating that HDFS is capable of storing billions of files.\nThis session is a  (Advanced) talk in our Apache Hadoop track. It focuses on Apache Hadoop and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Uber", "name": "Ankur Bansal"}, {"bio": "", "corp": "Uber", "name": "Mingmin Chen"}], "base_fname": "How_Uber_scaled_its_Real_Time_Infrastructure_to_Trillion_events_per_day", "title": "How Uber scaled its Real Time Infrastructure to Trillion events per day", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/how-uber-scaled-its-real-time-infrastructure-to-trillion-events-per-day"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/K-fI2BeTLkk"}, "desc": "Building data pipelines is pretty hard! Building a multi-datacenter active-active real time data pipeline for multiple classes of data with different durability, latency and availability guarantees is much harder. \nReal time infrastructure powers critical pieces of Uber (think Surge) and in this talk we will discuss our architecture, technical challenges, learnings and how a blend of open source infrastructure (Apache Kafka and Samza) and in-house technologies have helped Uber scale.\nThis session is a  (Advanced) talk in our IoT and Streaming track. It focuses on Apache Flink, Apache HBase, Apache Kafka, Apache Spark and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Ashutosh Chauhan"}, {"bio": "", "corp": "Hortonworks", "name": "Jes\u00fas Camacho Rodr\u00edguez"}], "base_fname": "An_Overview_on_Optimization_in_Apache_Hive_Past_Present_Future", "title": "An Overview on Optimization in Apache Hive: Past, Present, Future", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/an-overview-on-optimization-in-apache-hive-past-present-future-77188613"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/3Gae7MjkcD8"}, "desc": "Apache Hive has been continuously evolving to support a broad range of use cases, bringing it beyond its batch processing roots to its current support for interactive queries with sub-second response times using LLAP. However, the development of its execution internals is not sufficient to guarantee efficient performance, since poorly optimized queries can create a bottleneck in the system. Hence, each release of Hive has included new features for its optimizer aimed to generate better plans and deliver improvements to query execution. In this talk, we present the development of the optimizer since its initial release. We describe its current state and how Hive leverages the latest Apache Calcite features to generate the most efficient execution plans. We show numbers demonstrating the improvements brought to Hive performance, and we discuss future directions for the next-generation Hive optimizer, which include an enhanced cost model, materialized views support, and complex query decorrelation.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hive, Other and is geared towards Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Pentaho", "name": "Chuck Yarbrough"}], "base_fname": "The_5_Keys_to_a_Killer_Data_Lake", "title": "The 5 Keys to a Killer Data Lake", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/the-5-keys-to-a-killer-data-lake"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/h8Aqtjdb1vQ"}, "desc": "In the age of IoT, most everyone is talking about data lakes. For the most part, we all agree on the value data lakes deliver, but beyond this conceptual agreement, there are still many practical questions that need answers. The key to success comes down to how data lakes are implemented and managed.\nChuck Yarbrough outlines the 5 keys for creating a data lake along with strategies for defining, ingesting, governing, managing, and analyzing the data lake in ways that will enable transformative benefits in Iot and other use cases.  This session will show and share how real-world data lake implementations are changing the world. Chuck focuses on automation of the data lake from ingesting data to managing metadata at scale and applying machine learning to drive significant results. Along the way, Chuck explores tools and procedures that help create a well-organized, -governed, and -managed data lake\u2014without the risk of creating a dreaded data swamp. You\u2019ll leave armed with the five keys to successfully creating and managing a killer data lake.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Spark, NoSQL and is geared towards CXO, Architect, Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "ExxonMobil", "name": "Nick Evans"}, {"bio": "", "corp": "ExxonMobil", "name": "Kevin Brown"}], "base_fname": "The_Evolution_of_Streaming_and_Data_Lake_Shared_Services_at_ExxonMobil_Lessons_from_a_Fortune_10_Adoption", "title": "The Evolution of Streaming and Data Lake Shared Services at ExxonMobil: Lessons from a Fortune 10 Adoption", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/the-evolution-of-streaming-and-data-lake-shared-services-at-exxonmobil-lessons-from-a-fortune-10-adoption"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": ""}, "desc": "Analytics applications grow more powerful as they leverage new types of data from sensors, machines, server logs, clickstreams, and social media.  The Hadoop-based Data Lake enables that analytic potential, but the shared service supporting it must scale efficiently and enable deep insight across a large, broad, diverse data set to a variety of consumers.  Come learn how ExxonMobil created its first Big Data shared service across an enormous enterprise \u2013 from data ingestion at the edge using Hortonworks DataFlow to long-term storage in Hortonworks Data Platform, culminating in data exploration and analysis with business intelligence tools.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Nifi and is geared towards CXO, Architect audiences."},
{"speakers": [{"bio": "", "corp": "Parametric", "name": "Scott Sovine"}, {"bio": "", "corp": "Parametric", "name": "Amir Aliabadi"}], "base_fname": "How_to_Use_Innovative_Data_Handling_and_processing_techniques_to_drive_Alpha_in_the_financial_markets", "title": "How to Use Innovative Data Handling and processing techniques to drive Alpha in the financial markets.", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/how-to-use-innovative-data-handling-and-processing-techniques-to-drive-alpha-in-the-financial-markets"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/-IcCcUY0FEw"}, "desc": "For over 30 years, Parametric has been a leading provider of model-based portfolios to institutional and private investors, with unique implementation and customization expertise. Much like other cutting-edge financial services providers, Parametric operates with highly diverse, fast moving data from which they glean insights. Data sources range from benchmark providers to electronic trading participants to stock exchanges etc. The challenge is to not just onboard the data but also to figure out how to monetize it when the schemas are fast changing. This presents a problem to traditional architectures where large teams are needed to design the new ETL flow.  Organizations that are able to quickly adapt to new schemas and data sources have a distinct competitive advantage.  \nIn this presentation and demo, Architects from Parametric , Chris Gambino & Vamsi Chemitiganti  will present the data architecture designed in response to this business challenge. We discuss the approach (and trade-offs) to pooling, managing, processing the data using the latest techniques in  data ingestion & pre-processing. The overall best practices in creating a central data pool are also discussed. Quantitative analysts to have the most accurate and up to date information for their models to work on.  Attendees will be able to draw on their experiences both from a business and technology standpoint on not just creating a centralized data platform but also being able to distribute it to different units.\nThis session is a  talk in our Applications track. It focuses on Apache Nifi, ODPi, Other and is geared towards CXO, Architect, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Microsoft", "name": "Nishant Thacker"}], "base_fname": "Securing_your_Big_Data_Environments_in_the_Cloud", "title": "Securing your Big Data Environments in the Cloud", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/securing-your-big-data-environments-in-the-cloud"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/2ELFmAt3kbo"}, "desc": "Big Data tools are becoming a critical part of enterprise architectures and as such securing the data, at rest, and in motion is a necessity. More so, when you\u2019re implementing these solutions in the cloud and the data doesn't reside within the confines of your trusted data center. Also, there is a fine balance between implementing enterprise-grade security and negotiating utmost performance given the overheads of encryption and/or identity management.\nThis session is designed to tackle these challenges head on and explain the various options available in the cloud. The focal points are the implementation of tools like Ranger and Knox for cloud deployments, but we also pay attention to the security features offered in the cloud that complement this process and secure the data in unprecedented ways.\nCloud Security + OSS Security tools are a deadly combination, when it comes to securing your Data Lake. \nThis session is a  (Advanced) talk in our Governance and Security track. It focuses on Apache Knox, Apache Ranger, Cloud, Kerberos, LDAP and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "DigitalGlobe", "name": "Kent Miller"}], "base_fname": "GeoWave_Open_Source_Geospatial_Temporal_N_dimensional_Indexing_for_Accumulo_HBase_and_Cassandra", "title": "GeoWave: Open Source Geospatial/Temporal/N-dimensional Indexing for Accumulo, HBase and Cassandra", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/geowave-open-source-geospatialtemporalndimensional-indexing-for-accumulo-hbase-and-cassandra"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/svmZ4Ikoluo"}, "desc": "GeoWave is an open-source library that connects geospatial software with distributed computing frameworks. GeoWave leverages the scalability of a distributed key-value store for effective storage, retrieval, and analysis of massive geospatial datasets. It uses a space filling curve to preserve locality between multi-dimensional objects and the single dimensional sort order imposed by key-value stores. What this means to a user is that distributed spatial and spatial-temporal retrieval and analysis can be effectively accomplished at a massive scale.\nAt its core, GeoWave solves the problem of multi-dimensional indexing, and particularly extends this capability to spatial/temporal use cases. GeoWave supports raster, vector, and point cloud data, and provides common spatial algorithms that can be extended to create deep analytic capabilities. It also performs fast subsampling via distributed rendering that integrates with GeoServer, so that a user can interactively visualize data at map scale regardless of density.\nOur goal in presenting GeoWave to the Hadoop Summit is to introduce it to the big data community. We will present GeoWave at a moderate level of detail, to include a short demonstration, and hopefully answer any questions regarding maturity, suitability and implementation details.\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Accumulo, Apache Cassandra, Apache HBase and is geared towards Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Pivotal", "name": "Shivram Mani"}, {"bio": "", "corp": "Pivotal", "name": "Alex(Oleksandr) Diachenko"}], "base_fname": "HAWQ_Meets_Hive_Querying_Unmanaged_Data", "title": "HAWQ Meets Hive - Querying Unmanaged Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/hawq-meets-hive-querying-unmanaged-data"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/sjlZJvHx1hM"}, "desc": "Hadoop users leverage tools such as MapReduce, Hive, HBase etc. for various data processing requirements. These tools do not share a common notion of storage formats, schemas, data models and data types. Apache HAWQ(Incubating) along with its extension framework (PXF) provides a high-performance massively-parallel SQL processing framework on unmanaged data stores/formats in the hadoop ecosystem. HCatalog provides a glue for the entire Hadoop ecosystem by providing a relational abstraction for HDFS data. This talk introduces the integration of Hcatalog metadata into HAWQ's in memory catalog, which provides a simple and seamless access paradigm to data managed by Hive.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache HAWQ, Apache Hive and is geared towards Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Microsoft", "name": "Subru Krishnan"}, {"bio": "", "corp": "Microsoft", "name": "Carlo Curino"}], "base_fname": "Never_late_again_Job_Level_deadline_SLOs_in_YARN", "title": "Never late again! Job-Level deadline SLOs in YARN", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/never-late-again-joblevel-deadline-slos-in-yarn"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/mF_gij3X7mk"}, "desc": "Modern resource management frameworks for large scale analytics leave unresolved the problematic tension between high cluster utilization and job\u2019s performance predictability\u2014respectively coveted by operators and users. We address this in Morpheus a system that: 1) codifies implicit user expectations as explicit Service Level Objectives (SLOs), inferred from historical data, 2) enforces SLOs using novel scheduling techniques that isolate jobs from sharing-induced performance variability, and 3) mitigates inherent performance variance (e.g., due to failures) by means of dynamic reprovisioning of jobs. We validate these ideas against production traces from a 50k node cluster, and show that Morpheus can lower the number of deadline violations, while retaining cluster-utilization, and lowering cluster footprint. We demonstrate the scalability of our implementation by deploying Morpheus on a 2700-node cluster and running it against production-derived workloads. The extensions to the YARN ReservationSystem (2,3) are being open-sourced as part of Apache Hadoop----jira number: YARN-5326.\nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Cloud and is geared towards Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "BMC", "name": "Joe Goldberg"}], "base_fname": "Ingest_Process_Analyze_Automation_and_Integration_through_the_Big_Data_Journey", "title": "Ingest. Process. Analyze \u2013 Automation and Integration through the Big Data Journey", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/ingest-process-analyze-automation-and-integration-through-the-big-data-journey"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/GCJ9h3rFXA4"}, "desc": "As Hadoop and Big Data technologies take root in the enterprise, they bring new challenges for data orchestration and interaction with traditional environments. The journey of transitioning to a modern data platform involves not only ingesting and offloading data from multiple sources, but also transforming the data, making it instantly available to the business, for on demand analytics. Manual efforts and scripting are no longer practical and projects seem to come to a grinding halt due to failures and delays. Furthermore, Big Data projects involve not only multiple technologies but also different teams in the organization that need to work together to deliver fast results.   Join this session and learn through real-world examples of how BMC can help automating every aspect of the big data journey and deliver Big Data projects faster and better with Control-M\u2019s enterprise grade automation capabilities and job-as-code approach.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Cloud and is geared towards Data Scientist, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Microsoft", "name": "Arun Suresh"}, {"bio": "", "corp": "Microsoft", "name": "Konstantinos Karanasos"}], "base_fname": "Medea_Expressive_Scheduling_of_Long_Running_Applications", "title": "Medea: Expressive Scheduling of Long-Running Applications", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/medea-expressive-scheduling-of-longrunning-applications"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/Auwu_dICw34"}, "desc": "In shared production clusters, long-running containers, as used by streaming, machine learning, and latency-sensitive applications, are increasingly common. Within Microsoft's big data clusters, we observe that more than 10%, and at times more than 30% of machines are dedicated to applications with long-running containers. These applications have stringent and complex scheduling requirements: application owners want placement decisions to respect complex constraints, e.g., co-locating or separating long-running containers across node groups, accounting for performance and resilience; cluster operators must also achieve global objectives, such as avoiding decisions that hinder placement of upcoming applications. All of this must be done without affecting the scheduling latency of short-running containers. To this end, we propose Medea, a new cluster scheduler designed for the placement of long- and short-running containers. Medea follows a two-scheduler design: (i) for long-running containers, it applies an optimization-based approach with expressive placement constraints that captures interactions between containers both within and across applications, while meeting cluster operator objectives too; (ii) for short-running containers, Medea uses a traditional task-based scheduler to achieve low placement latency. We have built Medea as an extension of Apache Hadoop/YARN. Our experiments show the benefits of Medea: on a 275-node shared cluster, Medea reduces tail latency for HBase requests by 3x and increases throughput by 34% compared to YARN; in a streaming scenario, Medea decreases average latency by 7.3x. We plan to contribute Medea to Apache Hadoop.\nThis session is a  (Advanced) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Cloud and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Jayush Luniya"}, {"bio": "", "corp": "Hortonworks", "name": "Jaimin Jetly"}], "base_fname": "The_Future_of_Apache_Ambari", "title": "The Future of Apache Ambari", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/the-future-of-apache-ambari"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/3mi63oPDCow"}, "desc": "Apache Ambari is an extensible framework that simplifies provisioning, managing and monitoring Hadoop clusters. Apache Ambari was built on a standardized stack-based operations model. Stacks wrap services of all shapes and sizes with a consistent definition and lifecycle-control layer; thereby providing a consistent approach for managing and monitoring the services. This also provided a natural extension point for operators and the community to bring in their own add-on services and \u201cplug-in\u201d the new services into the stack. \nHowever, one of the fundamental limitations of the current Apache Ambari architecture has been that there is a strong one-on-one coupling between entities. For instance, a cluster is tied to a single stack and a Hadoop operator can only deploy services defined in that stack, a cluster can have only a single instance of a service and a host can have only a single instance of a component. Taking into consideration various use case scenarios that cannot be enabled due to these limitations there is a growing need to revamp the Ambari architecture. \nIn this talk, we propose a revamped Apache Ambari architecture that will open up the floodgates for a wide range of scenarios that wouldn\u2019t have been possible thus far. We will focus the discussion on a new mpack-based operations model that will replace the stack-based operations model. A management package is a self-contained deployment artifact that includes all the details for deploying, managing and upgrading a set of services bundled in the package. A third-party provider can also build their own management package containing their custom services. This eliminates the need to plug-in their services into a stack and also can define their own upgrade story for these custom services. A Hadoop operator will be able to deploy a Hadoop cluster with a mix of services across multiple packages instead of being limited to a single stack. For example, it would be possible to deploy a cluster with HDFS from HDP and NIFI from HDF. \nFurther, we will also discuss about the architectural changes needed to  enable a multi instance architecture in future Ambari releases to support deploying multiple instances of a service in a cluster, deploying multiple instances of a component on a host as well as future proofing the Ambari architecture to leverage some of the advancements happening in the Hadoop community like YARN services (YARN-4692). We will wrap up the conversation with a brief overview of other improvements planned for future releases of Ambari. \nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Ambari, Apache Hadoop, Cloud, ODPi and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Intel", "name": "Snehal Sakhare"}, {"bio": "", "corp": "Intel", "name": "Sandra Guija"}], "base_fname": "Speed_it_up_and_Spark_it_up_at_Intel", "title": "Speed it up and Spark it up at Intel", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/speed-it-up-and-spark-it-up-at-intel"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/rsUe_ps8ulM"}, "desc": "In the data analytics space none can argue that Spark has become the preferred tool for the Data Scientist, Business Analyst and for the Developer. At Intel, Spark is widely used across the organization to interact with Hive, to process streaming data, to ingest data from diverse sources to be used in machine learning or data analytics. In this presentation, we want to share how reusable ingestion components using Spark-sql has accelerated our application development phase. We will be discussing the challenges we faced at Intel when running Spark-on-yarn applications. Also have you spent time wondering why your Spark-sql query was running very slowly or pondering different methods for ingesting data faster from an RDBMs?  We will review Spark-on-yarn deployment and configuration. We will also describe the challenges posed by handling and processing large data-sets. Finally, we will share recommendations on how to tune spark jobs to optimize job performance by properly allocating resources.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Hive, Apache Spark and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Symantec", "name": "Vivek Madani"}, {"bio": "", "corp": "Symantec", "name": "Srinivas Vippagunta"}], "base_fname": "Real_time_and_Batch_Processing_Platform_for_Security_Telemetry", "title": "Real-time and Batch Processing Platform for Security Telemetry", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/0MZb49Ib0Fc"}, "desc": "At Symantec, we collect inconceivable quantities of security telemetry data from our 175MM endpoints and over 57MM attack sensors that is more than 25 TB every day. We take you through our cloud journey where we talk about how we handle our security telemetry and data ingestion into our data lake for both streaming and batch use cases. Kafka, Storm and Trident form the backbone of our streaming data platform that can scale beyond 2MM events/sec. Our batch data platform follows a tiered storage model for cost efficiency. We store our data in ORC file format and use HDFS for our hot data and S3 for our warm data. We also share our learnings on Hive with tiered storage and our hits/misses with Hive transactions. We secure our stack using combination of Kafka-SSL and ACLs, AWS security groups, Ranger and Knox. Lastly, we will talk about our underlying infrastructure deployment on AWS and how we effectively use Cloudbreak along with our home-grown tools to manage our data lake.\nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Ambari, Apache Hadoop, Apache Hive, Apache Kafka, Apache Storm and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "The Home Depot", "name": "Hiren Patel"}, {"bio": "", "corp": "AtScale", "name": "Josh Klahr"}], "base_fname": "How_The_Home_Depot_Creates_Simplicity_on_the_other_Side_of_Complexity_A_Journey_to_Self_Service_Analytics_on_Big_Data", "title": "How The Home Depot Creates Simplicity on the other Side of Complexity: A Journey to Self-Service Analytics on Big Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/how-the-home-depot-creates-simplicity-on-the-other-side-of-complexity-a-journey-to-selfservice-analytics-on-big-data"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/pkKqy88Oq7k"}, "desc": "How do you possibly deliver \u2018self-service\u2019 and \u2018interactive\u2019 reporting to \u20182500 business leaders\u2019 when you have 200 TBs of big data, generated by activity from 100MM Store SKU combinations that lives in a traditional EDW that grows bigger and more complex every day?  You think outside of the big box.\nThis session will share The Home Depot\u2019s journey of data strategy innovation, from traditional enterprise data warehouse (EDW) and Business Intelligence (BI) to our modern and approach to ad-hoc analysis live on a data lake. What began in 2009 with a traditional Teradata EDW + traditional MicroStrategy BI, to support business leaders\u2019 performance analysis, \u2018matured\u2019 by 2011 such that we in IT could barely keep pace with the flood of report and data requests. Self-service demands prompted us to innovate our approach with an SSAS  \u2018super-cube\u2019; which too became so successful, that business units far and wide requested more and more metrics and data added. When our super-cube grew to 1200 metrics, 12 TB and 2500 ad hoc users, and took 16 hours to realign and up to 5 weeks to reload, it was time to innovate again.  The business loved the multi-dimensionality of the \u2018cube\u2019, and the ease of using BI tools they knew (Tableau, Excel), and my IT team needed cost-effective data storage and processing power, in the face of data explosion. How did we solve it this time? The result is a journey, that we are still on, and involves an interesting combination of technology, processes and people.\nPlease join me, Hiren Patel, Software Engineering Manager, as I share details of The Home Depot\u2019s journey to successfully deliver ad-hoc analysis on our big data.  From our history with traditional EDW, to the pros and cons of our \u2018Super-cube\u2019, to trials, hiccups and lessons learned across aggregates and partnering.  I\u2019ll will share the good, bad and ugly, that led me and my team to building a path forward for self-service analytics at THD on HADOOP, and in doing so, helped THD continue to fulfill our goal of helping the do-it-yourselfer be their own hero in home improvement. \nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, Kerberos, OLAP and is geared towards Architect, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Alan Gates"}], "base_fname": "The_Apache_Way", "title": "The Apache Way", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/the-apache-way-77186888"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/ukEHw5zdAEA"}, "desc": "At the heart of much of the Bigdata revolution is the Apache Software Foundation.  Many of the projects, including the big ones like Hadoop, Spark, Hive, and Kafka, are Apache projects.  This means they follow \"The Apache Way\".  Maybe you have heard phrases like \"community over code\" or \"if it didn't happen on the lists, it didn't happen\" and wondered what they meant.  Maybe you would like to get involved with one or more of these projects but have not been sure how.  Maybe you would just like to learn how Apache works, and how its process differs from the way companies build software. If so, this talk is for you.  This talk will introduce Apache, how it is organized, the roles people play, who can contribute (hint, it is not just coders), Apache's tenants of community, meritocracy, collaboration, and openness, give some practical tips for new contributors and even old hands, as well as touch briefly on licenses and trademarks. \nThis session is a  (Beginner) talk in our Apache Hadoop track. It focuses on Apache Hadoop and is geared towards Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Ali Bajwa"}, {"bio": "", "corp": "Hortonworks", "name": "Srikanth Venkat"}], "base_fname": "Partner_Ecosystem_Showcase_for_Apache_Ranger_and_Apache_Atlas", "title": "Partner Ecosystem Showcase for Apache Ranger and Apache Atlas", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/partner-ecosystem-showcase-for-apache-ranger-and-apache-atlas"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/pfxzHFHROik"}, "desc": "Co-presenters: Laurent Bride (CTO of Talend), Shant Hovsepian (CTO of Arcadia Data), Sunil Sabat (Director at Protegrity). \nThe community for Apache Atlas and Apache Ranger, which are foundational components for Security and Governance across the Hadoop stack, has spawned a robust partner ecosystem of tools and platforms. These partner solutions build upon the extensibility offered in these platforms via open and robust APIs via integration patterns to provide innovative \u201cbetter together\u201d capabilities. In this talk, we will showcase how three of Hortonworks partners Talend, Protegrity, and Arcadia Data have effectively extended Apache Ranger and Apache Atlas frameworks to provide value added security and governance features to complement the Hadoop ecosystem. The talk will showcase partner-led demonstrations that will include how to enhance Apache Atlas lineage and metadata to cover ETL operations, how to build Apache Ranger authorizations of custom objects such as visualizations and how to enhance Apache Ranger\u2019s data protection capabilities for encryption and masking. We will also provide a short overview of Hortonworks Gov Ready and Sec Ready programs and how partners can benefit from the certification process as part of this program.\nThis session is a  (Beginner) talk in our Governance and Security track. It focuses on Apache Atlas, Apache Ranger and is geared towards Architect, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Google", "name": "Kenneth Knowles"}], "base_fname": "Stateful_processing_of_massive_out_of_order_streams_with_Apache_Beam", "title": "Stateful processing of massive out-of-order streams with Apache Beam", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/stateful-processing-of-massive-outoforder-streams-with-apache-beam"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/bNttSYBZuaw"}, "desc": "With Apache Beam, you can process massive out-of-order streams (or standard batch use cases too) by defining high-level transformation pipelines that you can then run on a variety of backends, including Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow.\nThis talk introduces a new feature of the Beam programming model: stateful processing with processing-time and event-time timers. This enhancement unlocks new use cases and efficiencies, such as:\n - Micro-service like workflows (\"register this user, remind them after a day, and expire their sign up after a week\")\n\n - Customized output control (\"only output when the signal has changed by more than 0.3\")\n\n - Carefully batched RPCs (\"write as many items as possible at the same time, but no more than 500\")\n\n - Stream joins with custom output triggering (\"join these two streams on an arbitrary join predicate with correct exactly once results\")\nIn this talk, you will learn how to use Beam to develop complex, stateful pipelines to easily implement scenarios like the above, which you can finely tailor to your precise use case.\nThis session is a  (Advanced) talk in our IoT and Streaming track. It focuses on Apache Flink, Apache Spark, Cloud, Other and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Yale School of Medicine", "name": "Wade Schulz"}, {"bio": "", "corp": "China", "name": "Hao Dai, National Center for Cardiovascular Diseases"}], "base_fname": "International_Precision_Medicine_A_Five_Million_Person_Case_Study_in_Hadoop", "title": "International Precision Medicine \u2013 A Five Million Person Case Study in Hadoop", "slide": {"dl_link": "", "src_link": ""}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/FoRP1PJx4NI"}, "desc": "Precision medicine and digital health research have the potential to improve the efficiency and effectiveness of healthcare delivery. With the continued interest in population genomics and its effect on disease, it is now essential to provide robust technical platforms for data management and analysis in order to realize the full potential of these large studies. In this session, we will present the use case for an ongoing international research collaboration between Yale University, the National Center for Cardiovascular Disease, and colleagues with a shared goal of enrolling five million individuals. We will discuss the architecture and design of the Hadoop-based infrastructure used for data integration that enables researchers around the world to securely analyze the data acquired for this study. To illustrate, we will present a specific example that uses Kafka, Storm, Spark, Python, and GPU-enabled Hadoop nodes that will enable our researchers to complete machine learning projects that link biomedical imaging data, genomics, electrocardiograms, and patient outcomes. Similar architectural approaches can be used within nearly any industry, and we will demonstrate how others can use similar approaches to gain valuable insights into their data. The topic will be co-presented by collaborators from Yale University and the National Center for Cardiovascular Disease, China.\nThis session is a  talk in our Applications track. It focuses on Apache Hadoop, Apache Ranger, Apache Spark, Apache Sqoop, Apache Zeppelin and is geared towards Architect, Data Scientist, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Carolinas HealthCare System", "name": "Victoria Morris"}, {"bio": "", "corp": "Carolinas HealthCare System", "name": "Lance Richey"}], "base_fname": "Microsoft_HDInsight_as_a_Big_Data_and_Interoperability_Platform_to_Drive_Point_of_Care_Decisions_in_Oncology_at_Levine_Cancer_Institute_LCI", "title": "Microsoft HDInsight as a Big Data and Interoperability Platform to Drive Point of Care Decisions in Oncology at Levine Cancer Institute (LCI)", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/microsoft-hdinsight-as-a-big-data-and-interoperability-platform-to-drive-point-of-care-decisions-in-oncology-at-levine-cancer-institute"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": ""}, "desc": "Learn how a small team of 3-4 technology and subject matter experts developed an Azure HDInsight solution.  The solution captures genomics data for solid tumors, summary data from a third party and various internal sources, and does genomic Clinical Trial matching. This was done strictly using the Azure cloud and interactions with cloud-based Office 365 SharePoint web applications utilizing only batch scripting, Hive, and Sqoop. HD Insights is the data munging layer and SharePoint is the user access layer. \nThe process was stood up in a 6-8 week period, while doing our day jobs. The business benefit is to enable providers, at the point of care, to suggest clinical trials for oncology patients based on genomic matches (Molecular Tumor Board).  This has increased participation rates in clinical trials with the goal to improve the survival rates and quality of life for patients. The success of this project has spread to capturing local home grown registries in data silos to share with other like-minded providers within Levine Cancer Institute.\nThis session is a  (Beginner) talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache Hive, Apache Sqoop and is geared towards Architect, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Bloomberg", "name": "Clay Baenziger"}], "base_fname": "Multitenancy_At_Bloomberg_HBase_and_Oozie", "title": "Multitenancy At Bloomberg - HBase and Oozie", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/multitenancy-at-bloomberg-hbase-and-oozie"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/iPCA1ZTitQk"}, "desc": "HBase provides many features for multi-tenancy and isolation. However, the operation of these features require integration into the broader operations of a cluster. This talk will cover some methods we use at Bloomberg for multi-tenancy and discuss some HBase-Oozie integration. Particularly of interest is our work on an Oozie action for secure snapshot export -- this extends the HBase security model via Oozie allowing self-service (non-hbase user) snapshot export on secure clusters.\nKey topics:\n\n* Bloomberg's Oozie HBase export snapshot action\n\n* Oozie coordinated time based major compactions\n\n* How we use LDAP with HBase (and why to take care with HADOOP-12291)\n\n* Some of our multi-tenancy setups around monitoring for SLAs\n\n* Suggesting HBase stays the course of being \"just\" a datastore -- and all projects following the Unix philosophy (this has made things like our Oozie integration much easier!)\nThis session is a  (Advanced) talk in our Cloud and Operations track. It focuses on Apache Hadoop, Apache HBase, Apache Oozie, Kerberos, LDAP and is geared towards Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Land O'Lakes", "name": "Chakra Sankaraiah"}, {"bio": "", "corp": "Land O'Lakes", "name": "Dwayne Beberg"}], "base_fname": "Marketing_Digital_Command_Center", "title": "Marketing Digital Command Center", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/marketing-digital-command-center"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/1IdN6-rDTf8"}, "desc": "Real time analytics is a beautiful thing, especially if you can build it in quick, scalable & robust way. We built a digital command center for our marketing team, which provided real time analytics on social media, clickstream and google search term in a span of couple of months. This solution was entirely build on open source technologies, using a combination of Apache Nifi, Elastic search & Hadoop. Simple but very effective. In this presentation i would like to share the architecture, learning and business benefits of this solution. \nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Hadoop, Apache Hive, Apache Nifi and is geared towards Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "IBM", "name": "Hebert Pereyra"}, {"bio": "", "corp": "IBM", "name": "Paul Yip"}], "base_fname": "Big_SQL_Powerful_SQL_Optimization_Re_Imagined_for_open_source", "title": "Big SQL: Powerful SQL Optimization - Re-Imagined for open source", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/big-sql-powerful-sql-optimization-reimagined-for-open-source"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/edbH-9xMMY8"}, "desc": "Let's be honest - there are some pretty amazing capabilities locked in proprietary SQL engines which have had decades of R&D baked into them. At this session, learn how IBM, working with the Apache community, has unlocked the value of their SQL optimizer for Hive, HBase, ObjectStore, and Spark - helping customers avoid lock-in while providing best performance, concurrency and scalability for complex, analytical SQL workloads. You'll also learn how the SQL engine was extended and integrated with Ambari, Ranger, YARN/Slider and HBase. We share the results of this project which has enabled running all 99 TPC-DS queries at world record breaking 100TB scale factor.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Ambari, Apache Hadoop, Apache Slider, ODPi and is geared towards Architect, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Owen O'Malley"}], "base_fname": "ORC_File_Optimizing_Your_Big_Data", "title": "ORC File - Optimizing Your Big Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/orc-file-optimizing-your-big-data"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/5vt2cVyE1GQ"}, "desc": "ORC files were originally introduced in Hive, but have now migrated to an independent Apache project. This has sped up the development of ORC and simplified integrating ORC into other projects, such as Hadoop, Spark, Presto, and Nifi. There are also many new tools that are built on top of ORC, such as Hive\u2019s ACID transactions and LLAP, which provides incredibly fast reads for your hot data. LLAP also provides strong security guarantees that allow each user to only see the rows and columns that they have permission for.\nThis talk will discuss the details of the ORC and Parquet formats and what the relevant tradeoffs are. In particular, it will discuss how to format your data and the options to use to maximize your read performance. In particular, we\u2019ll discuss when and how to use ORC\u2019s schema evolution, bloom filters, and predicate push down. It will also show you how to use the tools to translate ORC files into human-readable formats, such as JSON, and display the rich metadata from the file including the type in the file and min, max, and count for each column.\nThis session is a  (Advanced) talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache Hive, Apache Nifi, Apache ORC, Apache Spark and is geared towards Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Jon Eagles"}, {"bio": "", "corp": "Yahoo!", "name": "Kuhu Shukla"}], "base_fname": "Tez_Shuffle_Handler_Shuffling_at_Scale_with_Apache_Hadoop", "title": "Tez Shuffle Handler: Shuffling at Scale with Apache Hadoop", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/tez-shuffle-handler-shuffling-at-scale-with-apache-hadoop"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/0SazMDtIUgM"}, "desc": "In this talk we introduce a new Shuffle Handler for Tez, a YARN Auxiliary Service, that addresses the shortcomings and performance bottlenecks of the legacy MapReduce Shuffle Handler, the default shuffle service in Apache Tez. Based on our experiences of running Apache Pig and *Hive at scale on Apache Tez at Yahoo!, advanced features like auto-parallelism and session mode expose specific limitations in the shuffle service which was not designed with these features in mind. \nA highly auto-reduced job suffers from longer fetch times as the number of fetches per downstream task increases by the auto-reduction factor. The Apache Tez Shuffle Handler adds composite fetch which has support for multi-partition fetch to mitigate this performance slow down.\nAlso, since Apache Tez DAGs are run completely within a single application unlike their equivalent MapReduce jobs, intermediate shuffle data in Tez can linger beyond its usefulness. The Apache Tez Shuffle Handler provides deletion APIs to reduce disk usage for such long running Tez sessions.\nAs an emerging technology we will outline future roadmap for the Apache Tez Shuffle Handler and provide performance evaluation results from real world jobs at scale.\nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Apache Tez and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Cloudera", "name": "Todd Lipcon"}], "base_fname": "Creating_real_time_data_centric_applications_with_Impala_and_Kudu", "title": "Creating real-time, data-centric applications with Impala and Kudu", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/FIKfI6jxF0s"}, "desc": "Running real-time data-intensive applications on Apache Hadoop requires complex architectures to store and query data, typically involving multiple independent systems that are tied together through custom-engineered pipelines. A common pattern is to use a NoSQL engine like Apache HBase for caching and later transformations, the results of which are periodically written to HDFS in one of the popular open columnar file formats as a prerequisite for querying by a SQL engine.\nApache Kudu, a new scalable distributed storage engine designed for the Hadoop environment, gives the user low-latency single-row access as well as high-throughput bulk data scans. Integrated with Apache Impala (incubating), these capabilities are made available to the user via standard SQL language elements for updates and querying, combining the flexible update functionality of an RDBMS with the performance of a parallel analytic database system.\nTodd Lipcon explains how to simplify Hadoop-based data-centric applications with the CRUD (create, read, update, and delete) and interactive analytic functionality of Apache Impala (incubating) and Apache Kudu, offering an introduction to using Impala + Kudu to power your real-time data-centric applications for use cases like time series analysis (fraud detection, stream market data), machine data analytics, and online reporting.\nThis session is a  (Beginner) talk in our Data Processing and Warehousing track. It focuses on Apache Kudu, Other and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Thiruvel Thirumoolan"}, {"bio": "", "corp": "Yahoo!", "name": "Francis Liu"}], "base_fname": "Achieving_HBase_Multi_Tenancy_with_RegionServer_Groups_and_Favored_Nodes", "title": "Achieving HBase Multi-Tenancy with RegionServer Groups and Favored Nodes", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/achieving-hbase-multitenancy-with-regionserver-groups-and-favored-nodes-77157377"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/Hum7E6gb6Ss"}, "desc": "At Yahoo! HBase has been running as a hosted multi-tenant service since 2013. In a single HBase cluster we have around 30 tenants running various types of workloads (ie batch, near real-time, ad-hoc, etc). Typically such a deployment would cause tenant workloads to negatively affect each other because of resource contention (disk, cpu, network, cache thrashing, etc). Using RegionServer Groups we are able to designate a dedicated subset of RegionServers in a cluster to host only tables of a given tenant (HBASE-6721). \nMost HBase deployments use HDFS as their distributed filesystem, which in turn does not guarantee that a region\u2019s data is locally available to the hosting regionserver. This poses a problem when providing isolation since the hdfs data blocks may have to be read remotely from a different tenant\u2019s host thus contending for disk or network resources. Favored nodes addresses this problem by providing hints to HDFS on which datanodes data should be stored and only assigns regions to these favored regionservers (HBASE-15531).\nWe will walk through these features explaining our motivation, how they work as well as our experiences running these multi-tenant clusters. These features will be available in Apache HBase 2.0. \nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache HBase and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Nick Huang"}, {"bio": "", "corp": "Yahoo!", "name": "svdixit"}], "base_fname": "Data_Driving_Yahoo_Mail_Growth_and_Evolution_with_a_50_PB_Hadoop_Warehouse", "title": "Data Driving Yahoo Mail Growth and Evolution with a 50 PB Hadoop Warehouse", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/data-driving-yahoo-mail-growth-and-evolution-with-a-50-pb-hadoop-warehouse"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/Aqi0DqUykq4"}, "desc": "Yahoo Mail has 200+ million users a month and generates hundreds of terabytes of data per day, which continues to grow steadily. The nature of email messages has also evolved: for example, today the majority of them are generated by machines, consisting of newsletters, social media notifications, purchase invoices, travel bookings, and the like, which drove innovations in product development to help users organize their inboxes.  \nSince 2014, the Yahoo Mail Data Engineering team took on the task of revamping the Mail data warehouse and analytics infrastructure in order to drive the continued growth and evolution of Yahoo Mail. Along the way we have built a 50 PB Hadoop warehouse, and surrounding analytics and machine learning programs that have transformed the way data plays in Yahoo Mail.\nIn this session we will share our experience from this 3 year journey, from the system architecture, analytics systems built, to the learnings from development and drive for adoption.\nThis session is a  (Beginner) talk in our Applications track. It focuses on Apache Hadoop, Apache Hive, Apache Oozie, Apache ORC, Apache Pig and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Julian Hyde"}], "base_fname": "Data_profiling_in_Apache_Calcite", "title": "Data profiling in Apache Calcite", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/data-profiling-in-apache-calcite-77279140"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/cBaOPjoSUQw"}, "desc": "Query optimizers and people have one thing in common: the better they understand their data, the better they can do their jobs. Optimizing queries is hard if you don't have good estimates for the sizes of the intermediate join and aggregate results. Data profiling is a technique that scans data, looking for patterns within the data such as keys, functional dependencies, and correlated columns. These richer statistics can be used in Apache Calcite's query optimizer, and the projects that use it, such as Apache Hive, Phoenix and Drill. We describe how we built a data profiler as a table function in Apache Calcite, review the recent research and algorithms that made it possible, and show how you can use the profiler to improve the quality of your data.\nThis session is a  (Advanced) talk in our Data Processing and Warehousing track. It focuses on Apache Hive, Apache Phoenix, Other and is geared towards Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Vinay Shukla"}], "base_fname": "Running_Zeppelin_in_Enterprise", "title": "Running Zeppelin in Enterprise", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/running-zeppelin-in-enterprise"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/B1LCQ407fE0"}, "desc": "Zeppelin has become a popular way to unlock the value of data lake due to its user interface and appeal to business users. These business users ask their IT department for access to Zeppelin. Enterprise IT department want to help their business users but they have several enterprise concerns such as enterprise security, integration with their corporate LDAP/AD, scalability and multi-user environment, integration with Ranger and Kerberos.  This session will walk through enterprise concerns and how these concerns can be handled with Zeppelin.\nThis session is a  (Beginner) talk in our Apache Spark and Data Science track. It focuses on Apache Spark, Apache Zeppelin and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Microsoft", "name": "Pranav Rastogi"}], "base_fname": "Build_Big_Data_Enterprise_solutions_faster_on_Azure_HDInsight", "title": "Build Big Data Enterprise solutions faster on Azure HDInsight", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/build-big-data-enterprise-solutions-faster-on-azure-hdinsight-77279162"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/ol00fixCeDw"}, "desc": "Hadoop and Spark are big data frameworks used to extract useful span a variety of scenarios from ingestion, data prep, data management, processing, analyzing and visualizing data. Each step requires specialized toolsets to be productive. In this talk I will share solution examples in the Big Data ecosystem such as Cask, StreamSets, Datameer, AtScale, Dataiku on Microsoft\u2019s Azure HDInsight that simplify your Big Data solutions.  Azure HDInsight is a cloud Spark and Hadoop service for the enterprise and take advantage of all the benefits of HDInsight giving you the best of both worlds.  Join this session for practical information that will enable faster time to insights for you and your business.\nThis session is a  talk in our Cloud and Operations track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, Cloud and is geared towards Architect, Data Scientist, Data Analyst, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Travelers Insurance", "name": "Denise Rogers"}, {"bio": "", "corp": "Liberty Mutual Insurance", "name": "Naresh Mudunuru"}], "base_fname": "Real_time_Data_is_Changing_the_Face_of_the_Insurance_Industry", "title": "Real-time Data is Changing the Face of the Insurance Industry", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/realtime-data-is-changing-the-face-of-the-insurance-industry"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/0_K4DC6KyWc"}, "desc": "The insurance industry was founded on data and yet, new data sources and the \u201cspeed\u201d of data are entirely changing how the industry conducts its business. Real-time data used to be a foreign term for insurers but in the digital and connected world it has a significant impact on how the industry engages with customers, manages relationships, conducts core operations of risk assessments and manages claims.\nPredictive analytics is the minimum table stakes to remain competitive. Preventive analytics and machine learning are on the rise to the extent they are called out and considered critical success factors in an insurance company\u2019s business strategy.   The question is, how do you prepare the organization and adjust the mindset of a business to use real-time data to better serve customers whether individuals or companies?\nDuring this interactive session insurance industry leaders will discuss a variety of topics, including:\n\n\u00b7         how business data strategies are changing\n\n\u00b7         filling the skills gap\n\n\u00b7         value of open data sources and incorporating machine learning\nIn an age where the insurer must be founded on machine learning and advanced analytics, you\u2019ll hear from the leaders who have a grasp on the opportunities, as well as how to avoid and/or prepare for the bumps along the way\nSpeakers for this Session:\n\n1. Cindy Maike\n\n2. Denise Rogers\n\n\u200b3. \u200bNaresh\u200b \u200bMudunuru\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Spark and is geared towards CXO, Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "Comcast", "name": "Imran Amjad"}, {"bio": "", "corp": "Comcast", "name": "Dave Torok"}], "base_fname": "Data_Ingest_Self_Service_and_Management_using_Nifi_and_Kafka", "title": "Data Ingest Self Service and Management using Nifi and Kafka", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/data-ingest-self-service-and-management-using-nifi-and-kafka"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/YGo7Ggvaguc"}, "desc": "We\u2019re feeling the growing pains of maintaining a large data platform. Last year we went from 50 to 150 unique data feeds by adding them all by hand. In this talk we will share the best practices developed to handle our 300% increase in feeds through self service. Having self-service capabilities will increase your teams velocity and decrease your time to value and insight.\n* Self service data feed design and ingest\n\n* configuration management\n\n* automatic debugging\n\n* light weight data governance\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Kafka, Apache Nifi and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Casey Stella"}], "base_fname": "Bringing_it_All_Together_Apache_Metron_Incubating_as_a_Case_Study_of_a_Modern_Streaming_Architecture_on_Hadoop", "title": "Bringing it All Together: Apache Metron (Incubating) as a Case Study of a Modern Streaming Architecture on Hadoop", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/bringing-it-all-together-apache-metron-incubating-as-a-case-study-of-a-modern-streaming-architecture-on-hadoop"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://www.youtube.com/watch?v=0LrrAQXhqGY&feature=youtu.be"}, "desc": "There have been many voices discussing how to architect streaming\n\napplications on Hadoop.  Before now, there have been very few worked\n\nexamples existing within the open source.  Apache Metron (Incubating) is a\n\nstreaming advanced analytics cybersecurity application which utilizes\n\nthe components within the Hadoop stack as its platform. \nWe will attempt to go beyond theoretical discussions of Kappa vs Lambda\n\narchitectures and describe the nuts and bolts of a streaming\n\narchitecture that enables advanced analytics in Hadoop.  We will discuss\n\nthe componentry that we had to build and what we could utilize.  We will\n\ndiscuss why we made the architectural decisions that we made and how\n\nthey fit together to knit together a coherent application on top of many\n\ndifferent Hadoop ecosystem projects.\nWe will also discuss the domain specific language that we created out of\n\nnecessity to enable a pluggable layer to enable user defined enrichments.\n\nWe will discuss how this helped make Metron less rigid and easier to\n\nuse.  We will also candidly discuss mistakes that we made early on.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache HBase, Apache Metron, Apache ZooKeeper and is geared towards Architect, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Srikanth Venkat"}, {"bio": "", "corp": "Hortonworks", "name": "Jeff Sposetti"}], "base_fname": "Bridle_your_Flying_Islands_and_Castles_in_the_Sky_Built_in_Governance_and_Security_for_the_Cloud", "title": "Bridle your Flying Islands and Castles in the Sky:  Built-in Governance and Security for the Cloud", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/bridle-your-flying-islands-and-castles-in-the-sky-builtin-governance-and-security-for-the-cloud-77157153"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/nYrZ9_UjuGU"}, "desc": "Today enterprises desire to move more and more of their data lakes to the cloud to help them execute faster, increase productivity, drive innovation while leveraging the scale and flexibility of the cloud. However, such gains come with risks and challenges in the areas of data security, privacy, and governance. In this talk we cover how enterprises can overcome governance and security obstacles to leverage these new advances that the cloud can provide to ease the management of their data lakes in the cloud. We will also show how the enterprise can have consistent governance and security controls in the cloud for their ephemeral analytic workloads in a multi-cluster cloud environment without sacrificing any of the data security and privacy/compliance needs that their business context demands. Additionally, we will outline some use cases and patterns as well as best practices to rationally manage such a multi-cluster data lake infrastructure in the cloud.\nThis session is a  (Intermediate) talk in our Governance and Security track. It focuses on Apache Atlas, Apache Ranger, Apache Spark, Apache Zeppelin, Cloud and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Mridul Jain"}, {"bio": "", "corp": "Yahoo!", "name": "Jun Shi"}], "base_fname": "CaffeOnSpark_Update_Recent_Enhancements_and_Use_Cases", "title": "CaffeOnSpark Update: Recent Enhancements and Use Cases", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/caffeonspark-update-recent-enhancements-and-use-cases"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/DaHESwMLPJw"}, "desc": "By combining salient features from deep learning framework Caffe and big-data frameworks Apache Spark and Apache Hadoop, CaffeOnSpark enables distributed deep learning on a cluster of GPU and CPU servers. We released CaffeOnSpark as an open source project in early 2016, and shared its architecture design and basic usage at Hadoop Summit 2016. \nIn this talk, we will update audiences about the recenet development of CaffeOnSpark. We will highlight new features and capabilities: unified data layer which multi-label datasets, distributed LSTM training,  interleave testing with training, monitoring/profiling framework, and docker deployment. \nWe plan to share some interesting use cases from Yahoo, including image classification, NSFW image detection, and automatic identification of eSports game highlights. We will offer an interactive demo of image auto captioning using CaffeOnSpark in a Hadoop based notebook. \nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Spark and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Arizona State University", "name": "Jay Etchings"}], "base_fname": "An_Interdisciplinary_Approach_to_Research_within_the_Educational_Institution_Data_Driven_Innovation_to_Transform_Society", "title": "An Interdisciplinary Approach to Research within the Educational Institution -- Data-Driven Innovation to Transform Society", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/an-interdisciplinary-approach-to-research-within-the-educational-institution-datadriven-innovation-to-transform-society"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/7JaAScls0bU"}, "desc": "Universities are critical to our nation\u2019s ability to innovate and remain competitive in the global economy. High-Performance Computing (HPC) and Data Intensive Computing (Big Data) have merged into offerings now known as Research Computing. The primary objective of Research Computing at Arizona State University (ASU) is to ensure that the University can adequately support science and engineering communities as well as underserved domains in the social sciences, arts, and digital humanities. Today's grand challenges and most complex problems are interdisciplinary and they demand a heterogeneous tool set. Arizona State University was recently named the \"Most Innovative University\" in the US.  Although Big Data, data-intensive computing, and high-throughput analytics are now the commonplace at most institutions, ASU has deployed a multi-tenant model, that provides secure, ubiquitous Hadoop access to a large community of researchers. In this session,  Jay Etchings will demonstrate how the University deployed and maintains democratized access to a large catalog of resources, workspaces, tools and data. Jay will cover cyberinfrastructure and precision medicine,  non-obvious relationship analysis of ceramics from the Bronze Age,  dynamic provisioning for ubiquitous access and social good, and protein analysis in Apache Spark.\nThis session is a  talk in our Cloud and Operations track. It focuses on Apache Ranger, Apache Spark, Cloud, Docker / Container and is geared towards Architect, Data Scientist, Data Analyst, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hewlett Packard Enterprise", "name": "Jisheng Wang"}, {"bio": "", "corp": "Hewlett Packard Enterprise", "name": "Shirley Wu"}], "base_fname": "Deep_Learning_in_Security_Examples_Infrastructure_Challenges_and_Suggestions", "title": "Deep Learning in Security \u2013 Examples, Infrastructure,  Challenges, and Suggestions", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/deep-learning-in-security-examples-infrastructure-challenges-and-suggestions"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/p7Aq4hn4Pw0"}, "desc": "Recently, deep learning has delivered ground-breaking advances in many industries by delivering human-like understanding for difficult cognition problems.  We will share our empirical experiences of applying deep learning to some real-world security challenges, together with leant lessons and suggestions.\n1. Examples\n\nWe are going to explain our innovative User & Entity Behavior Analytics (UEBA) solution which includes 2 deep learning examples: 1. user and entity behavior anomaly detection using Convolutional Neural Network (CNN), 2. stateful user risk scoring using Long Short Term Memory (LSTM), in order to detect slow-gestating and multi-stage targeted attacks. We are also going to share several real-life use cases of successfully detecting compromised users and malicious insiders in big enterprises. \n2. Infrastructure\n\nThe production data processing and analytics workflow is developed using Spark, Spark Streaming and TensorFlow. We will share the experience of managing and tuning distributed TensorFlow and Spark on a middle/small size cluster in both SAS and on-premises deployments.  This includes how to manage and split resources between Spark and TensorFlow, how to split and tune workloads between parameter servers and worker servers in TensorFlow, etc.\n3. Challenges and Guidance\n\nAt the end, we are going to discuss the special challenges of applying deep learning (or general ML) into security than most other consumer industries, e.g., lack of large volume of high-quality labeled data, interpretation of models, fast detection, high cost of inaccurate detections.\nHuman intelligence \u2013 including knowledge of both enterprise business context and security heuristics \u2013 is a very precious resource to help cover these gaps. Thus any effective security ML solution has to have well integrated human and machine intelligence.\nTo achieve this partnership, there are several suggestions based on our current experiences, e.g., mix of complex and simple models, reinforcement learning based on human feed, pairing probabilistic ML results with deterministic forensic data.\nThis session is a  (Advanced) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache HBase, Apache Parquet, Apache Spark, NoSQL and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Uber", "name": "Mayank Bansal"}], "base_fname": "Hadoop_Infrastructure_Uber_Past_Present_and_Future", "title": "Hadoop Infrastructure @Uber  Past , Present and Future", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/hadoop-infrastructure-uber-past-present-and-future"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/37u1fBkp9vE"}, "desc": "Uber\u2019s mission is to provide transportation as reliable as running water and for fulfilling that mission data plays a critical role. In Uber, Hadoop plays a critical role in Data Infrastructure. We want to talk about the journey of Hadoop @Uber and our future plans in terms of scaling for billions of trips. We will talk about most unique use case Uber have and how Hadoop and eco system which we built, helped us in this journey. We want to talk about how we scaled from 10 -> 2000 and In future to scale up to 10\u2019s X1000 of Nodes. We will talk about our mistakes, learning and wins and how we process billions of events per day. We will talk about the unique challenges and real world use-cases and how we will co-locate the Uber\u2019s service architecture with batch (e.g data pipelines, machine learning and analytical workloads). Uber have done lot of improvements to current Hadoop eco system and uniquely solved some of the problems in a way which is never been solved in the past. This presentation will help audience to use this as an example and even encourage them to enhance the eco system. This will help to increase the community of these project and overall help the whole big data space. Audience is anybody who is working on Big Data and want to understand how to scale Hadoop and eco system for 10s of thousands of node. This talk will help them understand the Hadoop ecosystem and how to efficiently use that. It will also introduce them to some of the awesome technologies which Uber team is building in big data space.\nThis session is a  (Intermediate) talk in our Applications track. It focuses on Apache Hadoop, Apache Hive, Apache Parquet, Apache Spark and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Microsoft", "name": "Hitesh Sharma"}, {"bio": "", "corp": "Microsoft", "name": "Roni Burd"}], "base_fname": "Lessons_learned_from_scaling_YARN_to_40k_machines_in_a_multi_tenancy_environment", "title": "Lessons learned from scaling YARN to 40k machines in a multi tenancy environment", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/lessons-learned-from-scaling-yarn-to-40k-machines-in-a-multi-tenancy-environment"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/xUuiCxDw6Jg"}, "desc": "At Microsoft we have 500,000 jobs per day running a special query engine over exabytes of data with above 70% CPU utilization and we have made a big bet on YARN as our Resource Manager. We leverage Federation (YARN-2915) and Mercury (YARN-2877) to scale out to more than 40,000 nodes (spread across clusters) at 3000 allocate/second while achieving <5s response time @95%tile. To get there, we had to overcome several challenges: how do you measure and ensure there are no performance regressions? How do you deal with vast heterogeneous container sizes (from seconds to minutes)? What lessons did we find to get high CPU utilization with Mercury? What issues from HA, JVM, routing policy, throttling and DoS we found running and scaling? Join this session and learn about the challenges and learning from running YARN at humongous scale.\n\nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Hadoop, Cloud, Docker / Container and is geared towards Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hewlett Packard Enterprise", "name": "Bob Patterson"}], "base_fname": "Powering_the_Intelligent_Edge_HPE_s_Strategy_and_Direction_for_IoT_Big_Data", "title": "Powering the Intelligent Edge: HPE\u2019s Strategy and Direction for IoT & Big Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/powering-the-intelligent-edge-hpes-strategy-and-direction-for-iot-big-data"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/t7a4x3ev13Q"}, "desc": "Powering the Intelligent Edge is one of the three pillars of Hewlett Packard Enterprise's corporate strategy.    The session will cover HPE\u2019s strategic direction and approach in the areas of IoT and data analytics. Join the discussion and learn how HPE\u2019s solutions can help businesses prepare for the big data era.  \nThis session is a  talk in our IoT and Streaming track. It focuses on Apache Hadoop, Other and is geared towards CXO, Architect, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "CenterPoint Energy", "name": "Daniel Sumners"}, {"bio": "", "corp": "Hortonworks", "name": "Kenneth Smith"}], "base_fname": "Pouring_the_Foundation_Data_Management_in_the_Energy_Industry", "title": "Pouring the Foundation: Data Management in the Energy Industry", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/pouring-the-foundation-data-management-in-the-energy-industry"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/oj_uMi6UqIM"}, "desc": "At CenterPoint Energy, both structured and unstructured data are continuing to grow at a rapid pace. This growth presents many opportunities to deliver business value and many challenges to control costs. To maximize the value of this data while controlling costs, CenterPoint Energy created a data lake using SAP HANA and Hadoop. During this presentation, CenterPoint will discuss their journey of moving smart meter data to Hadoop, how Hadoop is allowing CenterPoint to derive value from big data and their future use case road map. \nThis session is a  talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache Hive, Apache ORC, Apache Spark and is geared towards Architect, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Google", "name": "Davor Bonaci"}], "base_fname": "Realizing_the_promise_of_portable_data_processing_with_Apache_Beam", "title": "Realizing the promise of portable data processing with Apache Beam", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/realizing-the-promise-of-portable-data-processing-with-apache-beam"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/_HRhCNSWkxc"}, "desc": "The world of big data involves an ever changing field of players. Much as SQL stands as a lingua franca for declarative data analysis, Apache Beam aims to provide a portable standard for expressing robust, out-of-order data processing pipelines in a variety of languages across a variety of platforms. In a way, Apache Beam is a glue that can connect the Big Data ecosystem together; it enables users to \"run-anything-anywhere\".\nThis talk will briefly cover the capabilities of the Beam model for data processing, as well as the current state of the Beam ecosystem. We'll discuss Beam architecture and dive into the portability layer. We'll offer a technical analysis of the Beam's powerful primitive operations that enable true and reliable portability across diverse environments. Finally, we'll demonstrate a complex pipeline running on multiple runners in multiple deployment scenarios (e.g. Apache Spark on Amazon Web Services, Apache Flink on Google Cloud, Apache Apex on-premise), and give a glimpse at some of the challenges Beam aims to address in the future.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Flink, Apache Kafka, Apache Spark, Cloud, Other and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Uber", "name": "Karthik Ramasamy"}, {"bio": "", "corp": "Uber", "name": "Gaurav Agarwal"}], "base_fname": "Semi_Supervised_Learning_In_An_Adversarial_Environment", "title": "Semi-Supervised Learning In An Adversarial Environment", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/semisupervised-learning-in-an-adversarial-environment"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/SdEPqFCC13U"}, "desc": "Unlike a typical e-commerce marketplace, Uber\u2019s marketplace is real-time, therefore stopping fraud has to happen in real-time too. In this talk, we will dive into Account Takeover (ATO) attacks and how we built a near real-time semi-supervised learning system to keep your Uber accounts safe. ATO attacks evolve very fast and may last only for a short time period. Thus traditional way of training a model once a week/day doesn\u2019t really help us to defend against new attack vectors. This lead us to develop a semi-supervised learning system that is built on top of Apache Spark and uses clustering techniques and feedback signals from our ATO challenges to detect and stop new attack vectors. We will discuss our results from online clustering using streaming k-means and also from batch clustering using more complex clustering algorithm, DBSCAN. We will also share some lessons on feature selection and hyperparameter tuning for clustering algorithms which plays a crucial part in performance.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Spark and is geared towards Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Comcast", "name": "Michael Fagan"}, {"bio": "", "corp": "Comcast", "name": "Dushyanth Vaddi"}], "base_fname": "Hadoop_Query_Performance_Smackdown", "title": "Hadoop Query Performance Smackdown", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/hadoop-query-performance-smackdown"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/dS1Ke-_hJV0"}, "desc": "Are you using the fastest query tool for Hadoop? Provide and discuss the latest performance results of the industry standard TPC_H benchmarks executed across an assortment of open source query tools such as Hive (using MR, TEZ, LLAP, SPARK), SparkSQL, Presto, and Drill.  Additionally, the performance tests will utilize a variety of data sizes and popular storage formats such as ORC, Parquet and Text and compression codecs.\nThis session is a  talk in our Applications track. It focuses on Apache Hive, Apache ORC, Apache Parquet, Apache Spark, Apache Tez and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Sriharsha Chintalapani"}, {"bio": "", "corp": "Hortonworks", "name": "Satish Duggana"}], "base_fname": "Schema_Registry_Set_your_data_free", "title": "Schema Registry - Set your data free", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/schema-registry-set-your-data-free"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/PYNCuXZHrSM"}, "desc": "Many Organizations are currently processing various types of data and in different formats. Most often this data will be in free form, As the consumers of this data growing it\u2019s imperative that this free-flowing data needs to adhere to a schema. It will help data consumers to have an expectation of about the type of data they are getting and also they will be able to avoid immediate impact if the upstream source changes its format. Having a uniform schema representation also gives the Data Pipeline a really easy way to integrate and support various systems that use different data formats.\n\n       SchemaRegistry is a central repository for storing, evolving schemas. It provides an API & tooling to help developers and users to register a schema and consume that schema without having any impact if the schema changed. Users can tag different schemas and versions, register for notifications of schema changes with versions etc.\n\n      In this talk, we will go through the need for a schema registry and schema evolution and showcase the integration with Apache Nifi, Apache Kafka, Apache Storm.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Flink, Apache Kafka, Apache Spark, Apache Storm and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Wangda Tan"}, {"bio": "", "corp": "Hortonworks", "name": "Yanbo Liang"}], "base_fname": "Hadoop_ecosystem_boosts_Tensorflow_and_machine_learning_technologies", "title": "Hadoop ecosystem boosts Tensorflow and machine learning technologies", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/hadoop-ecosystem-boosts-tensorflow-and-machine-learning-technologies"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/v60dZpw6dVY"}, "desc": "TensorFlow\u2122 is a popular open source software library for machine intelligence. While TF gives people abilities to describe the latest algorithm for machine learning and deep learning, it is also very important to make TF can be best fitted into the Hadoop ecosystem. In this session, we will talk about how Hadoop ecosystem components boosts TF and other machine learning technologies, including:\n\n1) Using Hadoop YARN to manage large scale TF services running on a GPU-equipped cluster, and share the same cluster with other tenants and applications.\n\n2) Using Spark/Hive for large scale data preprocessing.\n\n3) Using Zeppelin as an interactive interface to orchestrate and visualize the learning workflow.\n\nAt last, we will use a classic machine learning challenge - online ads Click Through Rate (CTR) prediction as an example to show how TF works with YARN, Spark and Zeppelin to train a better model in an efficient way.\nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, Apache Zeppelin and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "CSL Behring", "name": "Mark Baker"}], "base_fname": "Integrating_and_Analyzing_Data_from_Multiple_Manufacturing_Sites_using_Apache_NIFI_and_Apache_Zeppelin_to_a_central_Hadoop_data_lake_at_CSL_Behring", "title": "Integrating and Analyzing Data from Multiple Manufacturing Sites using Apache NIFI and Apache Zeppelin to a central Hadoop data lake at CSL Behring", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/integrating-and-analyzing-data-from-multiple-manufacturing-sites-using-apache-nifi-and-apche-zeppelin-to-a-central-hadoop-data-lake-at-csl-behring"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/GTlMMy5XU0k"}, "desc": "In this talk Mark Baker (CSL) will show how CSL Behring is Integrating and Analyzing Data from Multiple Manufacturing Sites using Apache NIFI to a central Hadoop data lake at CSL Behring\nThe challenge of merging data from disparate systems has been a leading driver behind investments in data warehousing systems, as well as, in Hadoop. While data warehousing solutions are ready-built for RDBMS integration, Hadoop adds the benefits of infinite and economical scale \u2013 not to mention the variety of structured and non-structured formats that it can handle. Whether using a data warehouse or Hadoop or both, physical data movement and consolidation is the primary method of integration.\n\nThere may also be challenges with synchronizing rapidly changing data from a system of record to a consolidated Hadoop platform  .\n\nThis introduces the need for \u201cdata federation\u201d , where data is integrated without copying data between systems.\n\nFor  historical/batch data use cases there is a replication of data across remote data hubs into a central data lake using Apache NIFI.\n\nWe will demo using Apache Zeppelin for analyzing data using Apache Spark and Apache HIVE.\nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Hive, Apache Nifi, Apache Spark, Apache Zeppelin and is geared towards Architect, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Sergey Shelukin"}], "base_fname": "LLAP_Building_Cloud_First_BI", "title": "LLAP: Building Cloud-First BI", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/llap-building-cloudfirst-bi"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/oBdBzG9nQAc"}, "desc": "LLAP (Live Long and Process) is the newest query acceleration engine for Hive 2.0, which entered GA in 2017. LLAP brings into light a new set of trade-offs and optimizations that allows for efficient and secure multi-user BI systems on the cloud. In this talk, we discuss the specifics of building a modern BI engine within those boundaries, designed to be fast and cost-effective on the public cloud. The focus of the LLAP cache is to speed up common BI query patterns on the cloud, while avoiding most of the operational administration overheads of maintaining a caching layer, with an automatically coherent cache with intelligent eviction and support for custom file formats from text to ORC, and explore the possibilities of combining the cache with a transactional storage layer which supports online UPDATE and DELETES without full data reloads. LLAP by itself, as a relational data layer, extends the same caching and security advantages to any other data processing framework. We overview the structure of such a hybrid system, where both Hive and Spark use LLAP to provide SQL query acceleration on the cloud with new, improved concurrent query support and production-ready tools and UI.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hive, Apache Spark, Cloud and is geared towards Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Bitfusion", "name": "Pierce Spitler"}], "base_fname": "Deep_Learning_with_Spark_and_GPUs", "title": "Deep Learning with Spark and GPUs", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/deep-learning-with-spark-and-gpus"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/nvH0OoAzC7o"}, "desc": "Spark is a powerful, scalable real-time data analytics engine that is fast becoming the de facto hub for data science and big data. However, in parallel, GPU clusters is fast becoming the default way to quickly develop and train deep learning models. As data science teams and data savvy companies mature, they will need to invest in both platforms if they intend to leverage both big data and artificial intelligence for competitive advantage.\nThis talk will discuss and show in action:\n\n* Leveraging Spark and Tensorflow for hyperparameter tuning\n\n* Leveraging Spark and Tensorflow for deploying trained models\n\n* An examination of DeepLearning4J, CaffeOnSpark, IBM's SystemML, and Intel's BigDL\n\n* Sidecar GPU cluster architecture and Spark-GPU data reading patterns\n\n* Pros, cons, and performance characteristics of various approaches\nAttendees will leave this session informed on:\n\n* The available architectures for Spark and Deep Learning and Spark with and without GPUs for Deep Learning\n\n* Several deep learning software frameworks, their pros and cons in the Spark context and for various use cases, and their performance characteristics\n\n* A practical, applied methodology and technical examples for tackling big data deep learning\nThis session is a  (Advanced) talk in our Apache Spark and Data Science track. It focuses on Apache Hadoop, Apache Spark, Cloud, Docker / Container and is geared towards Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Alibaba Group", "name": "Xiaowei Jiang"}, {"bio": "", "corp": "Alibaba Group", "name": "shaoxuan wang"}], "base_fname": "Flink_SQL_TableAPI_in_Large_Scale_Production_at_Alibaba", "title": "Flink SQL & TableAPI in Large Scale Production at Alibaba", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/flink-sql-tableapi-in-large-scale-production-at-alibaba"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/Eu1sC35gosg"}, "desc": "Search and recommendation system for Alibaba\u2019s e-commerce platform use batch and streaming processing heavily.  Flink SQL and Table API (which is a SQL-like DSL) provide simple, flexible, and powerful language to express the data processing logic. More importantly, it opens the door to unify the semantics of batch and streaming jobs.\nBlink is a project at Alibaba which improves Apache Flink to make it ready for large scale production use. To support our products, we made lots of improvements to Flink SQL & TableAPI in Alibaba's Blink project. We added the support for User-Defined Table function (UDTF), User-Defined Aggregates (UDAGG), Window Aggregate, and retraction, etc. We are actively working with the Flink community to contribute these improvements back. In this talk, we will present the rationale, semantics, design and implementation of these improvements. We will also share the experience of running large scale Flink SQL and TableAPI jobs at Alibaba.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Flink, Apache Hadoop, Apache Hive and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "YoppWorks", "name": "Matt Andruff"}], "base_fname": "Kafka_to_the_Maxka_Kafka_Performance_Tuning", "title": "Kafka to the Maxka - (Kafka Performance Tuning)", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/kafka-to-the-maxka-kafka-performance-tuning"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/MjLwhhnEQak"}, "desc": "Kafka  is becoming an ever more popular choice for users to help enable fast data and Streaming.  Kafka provides a wide landscape of configuration to allow you to tweak its performance profile. Understanding the internals of Kafka is critical for picking your ideal configuration. Depending on your use case and data needs, different settings will perform very differently.  Lets walk through performance essentials of Kafka.  Let's talk about how your Consumer configuration, can speed up or slow down the flow of messages to Brokers.  Lets talk about message keys, their implications and their impact on partition performance.  Lets talk about how to figure out how many partitions and how many Brokers you should have.  Let's discuss consumers and what effects their performance.  How do you combine all of these choices and develop  the best strategy moving forward?   How do you test performance of Kafka?  I will attempt a live demo with the help of Zeppelin to show in real time how to tune for performance. \nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Kafka, Apache Zeppelin, Apache ZooKeeper and is geared towards Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Cloudera", "name": "Todd Lipcon"}], "base_fname": "Apache_Kudu_1_0_and_beyond", "title": "Apache Kudu: 1.0 and beyond", "slide": {"dl_link": "", "src_link": ""}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/6MhQweRHG4k"}, "desc": "Apache Kudu was first announced as a public beta release at Strata NYC 2015 and reached 1.0 last fall. In February, Cloudera introduced commercial support, and Kudu is now in production usage at many different organizations. Todd Lipcon offers a very brief refresher on the goals and feature set of the Kudu storage engine, covering the development that has taken place over the last year, including new features such as improved support for time series workloads, performance improvements, Spark integration, and highly available replicated masters. Along the way, Todd explores real-world production deployments and some of the tools that have been built to help operators manage a Kudu cluster. He ends with a view of the road map of the Kudu project for the upcoming year, including plans for security and other new features.         (I really need to come up with 150 words in order to make this submission form happy? OK, I'll waste some with this complaint that 150 words is too long for a meaningful abstract).\nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Kudu and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "GENIVI Alliance", "name": "Steve Crumb"}, {"bio": "", "corp": "Hortonworks", "name": "Michael Ger"}], "base_fname": "Creating_the_Smart_Transportation_Infrastructure_of_the_Future", "title": "Creating the Smart Transportation Infrastructure of the Future", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/creating-the-smart-transportation-infrastructure-of-the-future"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/U4WGj3aY5dI"}, "desc": "In this session, discover connected vehicle innovations being driven by GENIVI, the nonprofit industry alliance driving open source and open technology standards for connected vehicles. The GENIVI RVI platform provides a foundation for bi-directional vehicle communications \u2013 for example, vehicle data (i.e. low fuel level) can be communicated to the GENIVI RVI server, analytics performed (determine closest gas station) and a message delivered back to the vehicle (directions to gas station). In combination with the Hortonworks Connected Data Platform (Apache Hadoop and NiFi) vehicle data can be seamlessly ingested, analyzed and stored leveraging a 100% open source solution. This session will culminate by outlining a new initiative being undertaken by the City of Las Vegas and GENIVI to integrate connected vehicles into the city\u2019s emerging Smart City landscape, creating the Smart Transportation Infrastructure of the Future.\nBACKGROUND: On January 13th, 2017, The GENIVI Alliance and Nevada Center for Advanced Mobility announced an In-Vehicle Communication Pilot Project to Increase Awareness for Pedestrian Safety and Assist Traffic Flow in Las Vegas. In-Vehicle Communications Technology will be Deployed to Help Improve Vehicle-to-Pedestrian Awareness on High-Traffic and Multi-Modal Corridors.\nPlease refer to the official Press Release for this initiative:\n\nhttps://www.genivi.org/sites/default/files/press-releases/english/GENIVI-Team%20Nevada%20Press%20Release%20Final%20.pdf\nThis session is a  talk in our IoT and Streaming track. It focuses on Apache Hadoop, Apache Nifi and is geared towards CXO, Architect, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Roshan Naik"}], "base_fname": "Next_Generation_Execution_Engine_for_Apache_Storm", "title": "Next Generation Execution Engine for Apache Storm", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/next-generation-execution-engine-for-apache-storm"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/bZPpt4NnvsA"}, "desc": "With its large install base in production, the Storm 1.x line has proven itself as a stable and reliable workhorse that scales well horizontally. Much has been learnt from evolving the 1.x line that we can now leverage to build the next generation execution engine. Under the STORM-2284 umbrella, we are working hard to bring you this new engine which is being redesigned at a fundamental level for Storm 2.0. The goal is to dramatically improve performance and enhance Storm's abilities without breaking compatibility.\n\nThis improved vertical scaling will help meet the needs of the growing user base by delivering more performance with less hardware.\nIn this talk, we will take an in-depth look at the existing and proposed designs for Storm's threading model and the messaging subsystem. We will also do a quick run-down of the major proposed improvements and share some early results from the work in progress.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Storm and is geared towards Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Mayo Clinic", "name": "Dequan Chen"}], "base_fname": "Securing_Enterprise_Healthcare_Big_Data_by_the_Combination_of_Knox_F5_Ranger_TFA_and_Kerberos_Coupled_With_Enterprise_Active_Directory_and_LDAP", "title": "Securing Enterprise Healthcare Big Data by the Combination of Knox/F5, Ranger, TFA and Kerberos Coupled With Enterprise Active Directory and LDAP", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/securing-enterprise-healthcare-big-data-by-the-combination-of-knoxf5-ranger-tfa-and-kerberos-coupled-with-enterprise-active-directory-and-ldap"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/4b5-Pt0p1uE"}, "desc": "Data security is critical to the success of large enterprises such as Mayo Clinic (MC). There is no exception for healthcare data stored on the enterprise Big Data platforms. At MC, healthcare Big Data ingestion, storage, processing and analytics are all in the enterprise-secured environments including Sandbox, Dev, Int/Test and Prod Hadoop clusters. The primary data security in the enterprise-secured Hadoop clusters has been achieved at MC by the combination of Knox Gateway/F5 Balancer, Ranger authorization/auditing, Two Factor local authentication (TFA) and Kerberos authentication that are coupled to MC Active Directory and LDAP.  In other words, any major HDFS, HBase and Hive healthcare data operations at MC have to go through the dedicated Knox Gateway or F5 balancer (for Knox HA) via Rest API, which interacts with Ranger and other primary security components involved. The data security on the Big Data platforms at MC is going to be strengthened by the on-going network segmentation and SSL enabling on the related Hadoop ecosystem components. The above approaches adopted on MC Big Data platforms have significantly improved the security of data for the success of MC Big Data program although the data need high-skilled clients or applications to use.\nThis session is a  talk in our Governance and Security track. It focuses on Apache Hadoop, Apache HBase, Apache Knox, Apache Ranger, Kerberos and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Barcelona Supercomputing Center (BSC)", "name": "Nicolas Poggi"}, {"bio": "", "corp": "Rackspace", "name": "David Grier"}], "base_fname": "Accelerating_HBase_with_NVMe_and_BucketCache", "title": "Accelerating HBase with NVMe and BucketCache", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/accelerating-hbase-with-nvme-and-bucketcashe"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/OEuphm6aTmc"}, "desc": "NVMe storage is faster than SSDs and denser than RAM. We apply this new storage technology to a PoC Hadoop cluster at Rackspace, to evaluate the maximum performance of the devices, and how and by how much they can accelerate HBase applications.\nTo begin with, an introduction is given on NVMe and the HBase read-through path.  Then, a system-level disk comparison tool (FIO) is used to measure maximum bandwidth and latencies of the NVMe and rotational drives. Next, the HBase I/O model is presented, alongside the different strategies to accelerate it using the BucketCache.  BucketCache is a recent feature to create an external (off-heap) L2-type cache in each node (RegionServer), ideal to use either with NVMe, SSDs, or extra available RAM. Different caching strategies are explored using the popular YCSB benchmark, where a mix of read and write workloads are tested to simulate heavy production usage.\nOverall, we found that applying the NVMe devices in ideal conditions yielded performance improvements of 9x in read-only in HBase. Although this speedup dropped substantially to less than 2x on mixed read and write workloads.   Still far from the maximum values offered by the technology, where HBase needs to incorporate new features, some of which are already planned for the next major release.\nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Hadoop, Apache HBase, NoSQL and is geared towards Architect, Data Scientist, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Yahoo!", "name": "Nilam Sharma"}, {"bio": "", "corp": "Yahoo!", "name": "Huibing Yin"}], "base_fname": "Data_Highway_Rainbow_Petabyte_Scale_Event_Collection_Transport_Delivery_at_Yahoo", "title": "Data Highway Rainbow - Petabyte Scale Event Collection, Transport & Delivery at Yahoo", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/data-highway-rainbow-petabyte-scale-event-collection-transport-delivery-at-yahoo"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/vKCBHQLebO4"}, "desc": "This paper will present the architecture and features of Data Highway Rainbow, Yahoo\u2019s hosted multi-tenant infrastructure which offers event collection, transport and aggregated delivery as a service. Data Highway supports collection from multiple data centers & aggregated delivery in primary Yahoo data centers which provide a big data computing cluster. From a delivery perspective, Data Highway supports endpoints/sinks such as  HDFS, Storm and Kafka; with Storm & Kafka endpoints tailored towards latency latency consumers.\nWe will also look into the evolution of the service in terms of prominent features added, the motivation behind these features starting from it\u2019s initial launch, some of which were customer asks, while others were driven from optimizing the efficiency and footprint of the deployed infrastructure. Some of the features we will touch upon are\n\n* Delivery Completeness Audit WebService\n\n* Publisher Daemon & Client API Robustness\n\n* Aggregated HDFS File Delivery\n\n* Filters for Low Latency Delivery.\n\n* Schema Registry\n\n* Adaptive Rate Limiting\n\n* Various Load Balancing techniques.\n\n* Event Deduplication \nAggregated Daily Metrics\n\n* Events Ingested: 250 Billion\n\n* Bytes Ingested (Uncompressed)  : 700 Tera Bytes\n\n* Bytes Delivered (Batch + Near Real Time) : 1.5 Peta Bytes\n\n* Near Real Time Delivery (Storm & Kafka) Latency : 95th percentile 500ms - 1 second\n\n* Batch Delivery Latency (Aggregated into 1 minute files) : 95th percentile within 3 minutes\n\n* Production H/W Footprint : 651\n\n* Total Active Event Schema Types:  ~200\nUnderlying Technology Stack : ZeroMQ, Apache Avro, libevent, Apache HttpComponents\nThe paper will conclude with the next steps we\u2019re considering as a logical evolution for Data Highway in light of considerable developments in similar open source projects such as Apache Kafka.\nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Avro, Apache Hadoop, Apache Kafka, Apache Storm, Cloud and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Alan Gates"}], "base_fname": "AN_APACHE_HIVE_BASED_DATA_WAREHOUSE", "title": "AN APACHE HIVE BASED DATA WAREHOUSE", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/an-apache-hive-based-data-warehouse-77154164"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/tfvqp0NvhOY"}, "desc": "Using Apache Hadoop and related technologies as a data warehouse has been an area of interest since the early days of Hadoop. In recent years Hive has made great strides towards enabling data warehousing by expanding its SQL coverage, adding transactions, and enabling sub-second queries with LLAP. But data warehousing requires more than a full powered SQL engine. Security, governance, data movement, workload management, monitoring, and user tools are required as well. These functions are being addressed by other Apache projects such as Ranger, Atlas, Falcon, Ambari, and Zeppelin. This talk will examine how these projects can be assembled to build a data warehousing solution. It will also discuss features and performance work going on in Hive and the other projects that will enable more data warehousing use cases. These include use cases like data ingestion using merge, support for OLAP cubing queries via Hive\u2019s integration with Druid, expanded SQL coverage, replication of data between data warehouses, advanced access control options, data discovery, and user tools to manage, monitor, and query the warehouse.\nThis session is a  (Beginner) talk in our Data Processing and Warehousing track. It focuses on Apache Atlas, Apache Hive, Apache Ranger, Cloud and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Microsoft", "name": "Felix Cheung"}], "base_fname": "Scalable_Data_Science_with_SparkR", "title": "Scalable Data Science with SparkR", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/scalable-data-science-with-sparkr"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/NUYZcaJIzEc"}, "desc": "R is a very popular platform for Data Science. Apache Spark is a highly scalable data platform. How could we have the best of both worlds? How could a Data Scientist leverage the rich 10000+ packages on CRAN, and integrate Spark into their existing Data Science toolset?\nSparkR is a new language binding for Apache Spark and it is designed to be familiar to native R users. In this talk we will walkthrough many examples how several new features in Apache Spark 2.x will enable scalable machine learning on Big Data. In addition to talking about the R interface to the ML Pipeline model, we will explore how SparkR support running user code on large scale data in a distributed manner, and give examples on how that could be used to work with your favorite R packages. We will also discuss best practices around using this new feature. We will also look at exciting changes in and coming next in Apache Spark 2.x releases.\nThis session is a  (Intermediate) talk in our Apache Spark and Data Science track. It focuses on Apache Spark and is geared towards Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "GoPro", "name": "David Winters"}, {"bio": "", "corp": "Gopro", "name": "hao zou"}], "base_fname": "Dynamic_DDL_Adding_structure_to_streaming_IoT_data_on_the_fly", "title": "Dynamic DDL: Adding structure to streaming IoT data on the fly", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/dynamic-ddl-adding-structure-to-streaming-iot-data-on-the-fly"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/FKpZrATL2vk"}, "desc": "At the end of day the only thing that data scientists want is one thing.  They want tabular data for their analysis.\n\nThey do not want to spend hours or days preparing data.  How does a data engineer handle the massive amount of data\n\nthat is being streamed at them from IoT devices and apps and at the same time add structure to it so that data scientists\n\ncan focus on finding insights and not preparing data?  By the way, you need to do this within minutes (sometimes seconds).\n\nOh...  and there are a bunch more data sources that you need to ingest and the current providers of data are changing their structure.\n\nAt GoPro, we have massive amounts of heterogeneous data being streamed at us from our consumer devices\n\nand applications, and we have developed a concept of \"dynamic DDL\" to structure our streamed data on the fly using\n\nSpark Streaming, Kafka, HBase, Hive, and S3.  The idea is simple.  Add structure (schema) to the data as soon as possible.\n\nAllow the providers of the data to dictate the structure.  And automatically create event-based and state-based tables (DDL)\n\nfor all data sources to allow data scientists to access the data via their lingua franca, SQL, within minutes.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Hadoop, Apache Hive, Apache HBase, Apache Kafka, Apache Spark and is geared towards Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Devin Pinkston"}, {"bio": "", "corp": "Hortonworks", "name": "Henry Sowell"}], "base_fname": "Enabling_Modern_Application_Architecture_using_Data_gov_open_government_data", "title": "Enabling Modern Application Architecture using Data.gov open government data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/enabling-modern-application-architecture-using-datagov-open-government-data"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/LcJlZdX2avE"}, "desc": "Description:\n\nBig Data and the Internet of Things (IoT) have forced businesses and the Federal Government to reevaluate their existing data strategies and adopt a more modern data architecture. With the advent of the connected data platform, migrating or building data-driven applications that take advantage of data-in-motion and data-at-rest can be a daunting journey to undertake. Scaling, reusability, and achieving operational agility are just some of the common pitfalls associated with existing software architectures. How do we embrace this paradigm shift?\u00a0 Adopting agile methodologies and emerging development practices such as Microservices and DevOps offer greater agility and operational efficiency enabling the government to rapidly build modern data-driven applications.\nDuring this talk and demonstration, we will show how the federal government can unleash the true power of the connected data platform with modern data-driven applications.\nConnected Data Platform:\n\n\u2022\tHortonworks DataFlow\n\no\tUsing Apache NiFi for capturing data at the edge of the data lake & managing the flow of data to the data platform\n\no\tApache Storm for complex event processing and stream processing\n\n\u2022\tHortonworks Data Platform\n\no\tApache Accumulo for scalability and cell-level security\n\no\tApache YARN for resource management\n\n\u2022\tModern Data-Driven Applications\n\no\tMicroservices: a software architecture practice for designing software applications as suites of independently deployable services, promoting componentization, single responsibility & scalability.\u00a0 Adopting a Microservices mindset enables the government to be technology agnostic: using the best tool or programming language for the job.\n\n\u2663\tDemoed REST API\u2019s on-top of Apache Accumulo.\u00a0 (Spark-Java, AngularJS/Typescript)\n\no\tDevOps: A culture and practice that breaks down the silos found between development and operations teams in traditional software practices.\u00a0\n\n\u2663\tCI / CD pipelines, automated build kick-offs using containers (Docker, Jenkins)\u00a0\n\n\u00a0\n\nThis talk will lay out a basic environment for promoting greater agility and operational efficiency for the federal government while taking advantage of a connected data platform. \nThis session is a  (Advanced) talk in our Applications track. It focuses on Apache Accumulo, Apache Kafka, Apache Nifi, Apache Storm, Docker / Container and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Ram Venkatesh"}, {"bio": "", "corp": "Hortonworks", "name": "Mingliang Liu"}], "base_fname": "Cloudy_with_a_chance_of_Hadoop_real_world_considerations", "title": "Cloudy with a chance of Hadoop \u2013 real world considerations", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/cloudy-with-a-chance-of-hadoop-real-world-considerations-77278947"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/j-VCBcGm-5I"}, "desc": "Over the last eighteen months, we have seen significant adoption of Hadoop eco-system centric big data processing in Microsoft Azure and Amazon AWS. In this talk we present some of the lessons learned and architectural considerations for cloud-based deployments including security, fault tolerance and auto-scaling.\nWe look at how Hortonworks Data Cloud and Cloudbreak can automate that scaling of Hadoop clusters, showing how it can react dynamically to workloads, and what that can deliver in cost-effective Hadoop-in-cloud deployments.\nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Ambari, Apache Hadoop, Cloud, Docker / Container and is geared towards Architect, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "LinkedIn", "name": "Anant Nag"}, {"bio": "", "corp": "LinkedIn", "name": "Shankar Manian"}], "base_fname": "Beyond_unit_tests_Deployment_and_testing_for_Hadoop_Spark_workflows", "title": "Beyond unit tests: Deployment and testing for Hadoop/Spark workflows", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/beyond-unit-tests-deployment-and-testing-for-hadoopspark-workflows"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/oL2upHnLt0c"}, "desc": "As a Hadoop developer, do you want to quickly develop your Hadoop workflows? Do you want to test your workflows in a sandboxed environment similar to production? Do you want to write unit tests for your workflows and add assertions on top of it? \nIn just a few years, the number of users writing Hadoop/Spark jobs at LinkedIn have grown from tens to hundreds and the number of jobs running every day has grown from hundreds to thousands. With the ever increasing number of users and jobs, it becomes crucial to reduce the development time for these jobs. It is also important to test these jobs thoroughly before they go to production. \nWe\u2019ve tried to address these issues by creating a testing framework for Hadoop/Spark jobs. The testing framework enables the users to run their jobs in an environment similar to the production environment and on the data which is sampled from the original data. The testing framework consists of a test deployment system, a data generation pipeline to generate the sampled data, a data management system to help users manage and search the sampled data and an assertion engine to validate the test output. \nIn this talk, we will discuss the motivation behind the testing framework before deep diving into its design. We will further discuss how the testing framework is helping the Hadoop users at LinkedIn to be more productive. \nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Apache Hive, Apache Pig, Apache Spark and is geared towards Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Walgreens", "name": "Gupta Narayanam"}, {"bio": "", "corp": "Walgreens", "name": "Abey Koshy"}], "base_fname": "Hadoop_Journey_at_Walgreens", "title": "Hadoop Journey at Walgreens", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/hadoop-journey-at-walgreens"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/sX8rypVYmGU"}, "desc": "Prior to 2014, Walgreens has traditional Enterprise Datawarehouse Systems that have reached the capacity limits. Over the last three years we have evolved, learned lessons, experienced successes and failures. Our initial adoption of Hadoop came from the need to run complex analytics which simply did not scale on MPP RDBMS.  Our business data demands were rapidly increasing and the 8 to 12 weeks concomitant extract, transform, and load turn around cycles was not a acceptable deliverable timeframe in the retail space. A self service model where data lands on a distributed platform, apply schema where necessary, and process at scale was a necessary paradigm for business value enablement.  Our journey started with single use case which has now evolved to enterprise data hub. We will discuss following points: Evolution of our infrastructure profile, streamlining the hardware provisioning cycle, and our hybrid deployment model (on premise & cloud).  Operations, how SmartSense has helped us proactively tune our cluster, and which operational tests we use for benchmarking the cluster. Monitoring, how we monitor and the tools required for enterprise grade monitoring. Security and governance how we progressed from non\u2013compliance to enterprise grade using Ranger, Knox, Kerberos, HP voltage, encryption at rest, and many other services. 3rd Party integration with HDP, what we learned and how we overcame the challenges. Lastly, how we approach our disaster recovery strategy, what is driving the need for a DR and the key capabilities required.\nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Ambari, Apache Hadoop, Apache Hive, Apache HBase, Apache Ranger and is geared towards CXO, Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "IBM", "name": "carlo appugliese"}], "base_fname": "The_Future_of_Data_Science", "title": "The Future of Data Science", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/the-future-of-data-science-77153877"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/beSA_IF2Y-Q"}, "desc": "A changing market landscape and open source innovations are having a dramatic impact on the consumability and ease of use of data science tools. Join this session to learn about the impact these trends and changes will have on the future of data science. If you are a data scientist, or if your organization relies on cutting edge analytics, you won't want to miss this!\nThis session is a  talk in our Apache Spark and Data Science track. It focuses on Apache Spark and is geared towards Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Diego Baez"}], "base_fname": "The_Unbearable_Lightness_of_Ephemeral_Processing", "title": "The Unbearable Lightness of Ephemeral Processing", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/the-unbearable-lightness-of-ephemeral-processing"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/SRqDOhNjrAI"}, "desc": "Ephemeral clusters can be launched quickly (minutes), are pre-configured for a specific processing purpose, and can be brought down quickly as soon as their usefulness has expired.  The ability  to launch Ephemeral clusters for on-demand processing, quickly and efficiently, is transforming how organizations design, deploy and Manage applications. The velocity and elasticity of fast cluster deployment enables seamless peak-demand provisioning, enables cost optimization by leveraging significantly lower cloud spot pricing, and maximizes utilization of existing compute capacity.  Additionally, being able to launch bespoke clusters for specific compute needs in a repeatable fashion and within a shared infrastructure provides flexibility for special purpose processing needs.  Organizations can leverage Ephemeral Clusters  for parallel compute intensive applications which require short bursts of power but are short lived.  In this session we will explore how to design Ephemeral clusters, how to launch, modify and bring them down, as well as application design considerations to maximize Ephemeral clusters usability.\nThis session is a  talk in our Cloud and Operations track. It focuses on Apache Ambari, Apache Nifi, Apache Spark, Cloud, Docker / Container and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Anu Engineer"}, {"bio": "", "corp": "Hortonworks", "name": "Xiaoyu Yao"}], "base_fname": "CBlocks_Posix_compliant_files_systems_for_HDFS", "title": "CBlocks - Posix compliant files systems for HDFS", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/cblocks-posix-compliant-files-systems-for-hdfs"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/JhtC_bh8r5k"}, "desc": "With YARN running Docker containers, it is possible to run applications that are not HDFS aware inside these containers. \u00a0 It\u00a0is hard to customize these applications since most of them assume a Posix file system with rewrite capabilities.\u00a0 In this talk, we will dive into how we created a block storage, how it is being tested internally and the storage containers which makes it all possible.\n\n\u00a0\n\nThe storage container framework was developed as part of Ozone (HDFS-7240). This is talk will also explore the current state of Ozone along with CBlocks. This talk will\u00a0explore\u00a0architecture of storage containers, how replication is handled, scaling to millions of volumes and I/O performance optimizations.\nThis session is a  (Intermediate) talk in our Apache Hadoop track. It focuses on Apache Hadoop, Docker / Container and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Accenture", "name": "Clara Fletcher"}, {"bio": "", "corp": "Accenture", "name": "NAYANJYOTI PAUL"}], "base_fname": "Governance_Bots_Metadata_Driven_Compliance_Through_AI_Atlas_and_NiFi", "title": "Governance Bots - Metadata Driven Compliance Through AI, Atlas and NiFi", "slide": {"dl_link": "", "src_link": ""}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/vnA_l6Wolr8"}, "desc": "Building and maintaining a modern data governance strategy on hybrid architectures is one of the most challenging problems Accenture clients face. Metadata management, lineage and user access must all be done as autonomously as possible to enable the scaling and long term stability that is required in the enterprise. Let's go beyond column headers and develop ways to utilize deep learning and artificial intelligence to drive next generation metadata driven governance.\n\nLet's eliminate the mystery of long lost stored procedures with API driven services and dynamic data ingestion. This talk will focus on the technical components of a next generation Data Governance solution that utilizes Apache Atlas, NiFi and deep learning to tag and manage data end to end in automated frameworks.\nThis session is a  talk in our Governance and Security track. It focuses on Apache Atlas, Apache Nifi and is geared towards CXO, Architect, Data Scientist, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Aldrin Piri"}, {"bio": "", "corp": "Hortonworks", "name": "Joseph Witt"}], "base_fname": "Connecting_the_Drops_with_Apache_NiFi_Apache_MiNiFi", "title": "Connecting the Drops with Apache NiFi & Apache MiNiFi", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/connecting-the-drops-with-apache-nifi-apache-minifi"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/goZPz4fZVQ4"}, "desc": "Demand for increased capture of information to drive analytic insights into an organizations' assets and infrastructure is growing at unprecedented rates. However, as data volume growth soars, the ability to provide seamless ingestion pipelines becomes operationally complex as the magnitude of data sources and types expands.\nThis talk will focus on the efforts of the Apache NiFi community including subproject, MiNiFi; an agent based architecture and its relation to the core Apache NiFi project. MiNiFi is focused on providing a platform that meets and adapts to where data is born while providing the core tenets of NiFi in provenance, security, and command and control.  These capabilities provide versatile avenues for the bi-directional exchange of information across data and control planes while dealing with the constraints of operation at opposite ends of the scale spectrum tackling the first and last miles of dataflow management.\nWe will highlight ongoing and new efforts in the community to provide greater flexibility with deployment and configuration management of flows.  Versioned flows provide greater operational flexibility and serve as a powerful foundation to orchestrate the collection and transmission from the point of data's inception through to its transmission to consumers and processing systems.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Nifi and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Liberty Mutual Insurance", "name": "Michael Klein"}, {"bio": "", "corp": "Liberty Mutual Insurance", "name": "Yiqing Wang"}], "base_fname": "Security_ETL_BI_Analytics_and_Software_Integration", "title": "Security, ETL, BI & Analytics, and Software Integration", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/security-etl-bi-analytics-and-software-integration"}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/JpW0jhcVcBk"}, "desc": "Liberty Mutual Enterprise Data Lake Use Case Study \nBy building a data lake, Liberty Mutual Insurance Group Enterprise Analytics department has created a platform to implement various big data analytic projects. We will share our journey and how we leveraged Hortonworks Hadoop distribution and other open source technologies to meet our project needs. This session will cover data lake architecture, security, and use cases.\nThis session is a  (Intermediate) talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Nifi, Apache Spark and is geared towards CXO, Architect, Data Scientist, Data Analyst, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Alejandro Fernandez"}, {"bio": "", "corp": "Hortonworks", "name": "Aravindan VIjayan"}], "base_fname": "Tuning_Apache_Ambari_performance_for_Big_Data_at_scale_with_3000_agents", "title": "Tuning Apache Ambari performance for Big Data at scale with 3000 agents", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/tuning-apache-ambari-performance-for-big-data-at-scale-with-3000-agents-77153769"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/UthP7LUt9-o"}, "desc": "Apache Ambari manages Hadoop at large-scale and it becomes increasingly difficult for cluster admins to keep the machinery running smoothly as data grows and nodes scale from 30 to 3000 agents. To test at scale, Ambari has a Performance Stack that allows a VM to host as many as 50 Ambari Agents. The simulated stack and 50 Agents per VM can stress-test Ambari Server with the same load as a 3000 node cluster. This talk will cover how to tune the performance of Ambari and MySQL, and share performance benchmarks for features like deploy times, bulk operations, installation of bits, Rolling & Express Upgrade. Moreover, the speaker will show how to use Ambari Metrics System and Grafana to plot performance, detect anomalies, and pinpoint tips on how to improve performance for a more responsive experience. Lastly, the talk will discuss roadmap features in Ambari 3.0 for improving performance and scale.\nThis session is a  (Beginner) talk in our Cloud and Operations track. It focuses on Apache Ambari and is geared towards Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Honeywell Aerospace", "name": "Dan Marshall"}], "base_fname": "Data_Federation_with_Apache_Spark", "title": "Data Federation with Apache Spark", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/data-federation-with-apache-spark"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/YyFof-nj8H0"}, "desc": "Apache Spark continues to grow in popularity - due to advanced analytics/machine learning, high performance processing, real-time streaming and multiple language support.  Big Data technology is adding more data processing options to an already long list of legacy databases and file systems. As a result, enterprises continue to look for effective and approachable ways to federate all these data sources to solve business information needs.  One under-appreciated feature of Spark is its ability to help quickly and powerfully enable federated data access. This presentation will discuss and demonstrate using Spark to query/combine multiple disparate data sources. We will see how to access the various data sources from Spark, normalize to Spark RDDs and combine for processing. The demo will show combining sources such as HDFS, JSON files, HBase, Hive and PostgreSQL and write the result back to a Data Mart for analysis. Also we will show the use of SparkSQL to access federated data in Spark through the Spark Thrift Server using the the Tableau BI tool. \nThis session is a  (Intermediate) talk in our Data Processing and Warehousing track. It focuses on Apache Spark and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Dell EMC", "name": "Thomas Henson"}], "base_fname": "Future_Architecture_of_Streaming_Analytics_Capitalizing_on_the_Analytics_of_Things_AoT", "title": "Future Architecture of Streaming Analytics: Capitalizing on the Analytics of Things (AoT)", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/future-architecture-of-streaming-analytics-capitalizing-on-the-analytics-of-things-aot"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/UMcdm8HKToo"}, "desc": "The proliferation of connected devices and sensors is leading the Digital Transformation. By 2020 there will be over 20 billion connected devices. Data from these devices need to be ingested at extreme speeds in order to be analyzed before the data decays. The life cycle of the data is critical in revealing what insight can be revealed and how quickly they can be acted upon.\nIn this session we will look at the past, present and future architecture trends streaming analytics. We will look at how to turn all the data from devices into actionable insights and dive into recommendations for streaming architecture depending on the data streams and time factor of the data. We will also discuss how to manage all the sensor data, understand the life cycle cost of the data, and how to scale capacity and capability easily with a modern infrastructure strategy.\nThis session is a  talk in our Apache Hadoop track. It focuses on Apache Hadoop and is geared towards Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Ford Motor Company", "name": "Daniel Totten"}, {"bio": "", "corp": "Hortonworks", "name": "Joseph Niemiec"}], "base_fname": "Real_Time_Streaming_Architecture_at_Ford", "title": "Real Time Streaming Architecture at Ford", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/real-time-streaming-architecture-at-ford"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/_idsi5JWJj4"}, "desc": "Ford Motor Company's mission to become both an Automotive and Mobility company has required an evolution in our analytics data flow, from traditional batch processing systems to dynamically routed stream processing based systems. Valuable data is continually being generated across the enterprise, from consumer WiFi in dealerships, robots working on the assembly line, and vehicle diagnostic data, and is now flowing into Ford's Real Time Streaming Architecture (RTSA). Our goal was to develop a provider agnostic, end to end solution to ingest and dynamically route individual streams of data in less than one second from edge node to Ford's on premise data center, or vice versa. The architecture dynamically scales in the cloud to reliably handle thousands of outbound and inbound transactions per second, with data provenance capabilities to audit data flow from end to end.\nThis session is a  (Intermediate) talk in our IoT and Streaming track. It focuses on Apache Hadoop, Apache Kafka, Apache Nifi, Cloud and is geared towards Architect, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Ramesh Mani"}, {"bio": "", "corp": "Pivotal", "name": "Alexander Denissov"}], "base_fname": "Extending_Apache_Ranger_Authorization_Beyond_Hadoop_Review_of_Apache_Ranger_Extensibility_Framework_Case_Study_for_integration_with_Apache_HAWQ", "title": "Extending Apache Ranger Authorization Beyond Hadoop: Review of Apache Ranger Extensibility Framework & Case Study for integration with Apache HAWQ", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/extending-apache-ranger-authorization-beyond-hadoop-review-of-apache-ranger-extensibility-framework-case-study-for-integration-with-apache-hawq"}, "tag": "Governance and Security", "video": {"dl_link": "", "src_link": "https://youtu.be/6SE32zrgIAU"}, "desc": "Ranger\u2019s pluggable architecture allows resource access policy administration and enforcement for standard and custom services from a \u201csingle pane of glass\u201d.  Apache Ranger has a rich Authorization Model, which provides the mechanism to author Policy in a Ranger Admin Server and serves as policy decision and audit point in authorizing user\u2019s resource access within various components of Hadoop ecosystem.\n\nThis session will provide a deep dive into Ranger framework and a cook-book for extending Ranger to do authorization / auditing on resource  access to external applications, including technical details of Rest APIs, Ranger policy engine and enriching authorization requests, with a demo of a sample application.We will then demonstrate a real-world example of how Ranger has simplified security enforcement for Hadoop-native MPP SQL engine like Apache HAWQ (incubating),which previously used its built-in Postgres-like authorization mechanisms. The integration design includes a Ranger Plugin Service that allows transparent authorization API calls between C-based Apache HAWQ and Java-based Apache Ranger.\nThis session is a  (Advanced) talk in our Governance and Security track. It focuses on Apache Ranger and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Nishant Bangarwa"}], "base_fname": "Druid_Sub_Second_OLAP_queries_over_Petabytes_of_Streaming_Data", "title": "Druid : Sub-Second OLAP queries over Petabytes of Streaming Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/druid-subsecond-olap-queries-over-petabytes-of-streaming-data"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/iIaceyMLrQA"}, "desc": "When interacting with analytics dashboards in order to achieve a smooth user experience, two major key requirements are sub-second response time and data freshness. Cluster computing frameworks such as Hadoop or Hive/Hbase work well for storing large volumes of data, although they are not optimized for ingesting streaming data and making it available for queries in realtime. Also, long query latencies make these systems sub-optimal choices for powering interactive dashboards and BI use-cases.\n In this talk we will present Druid as a complementary solution to existing hadoop based technologies. Druid is an open-source analytics data store, designed from scratch, for OLAP and business intelligence queries over massive data streams. It provides low latency realtime data ingestion and fast sub-second adhoc flexible data exploration queries.\nMany large companies are switching to Druid for analytics, and we will cover how druid is able to handle massive data streams and why it is a good fit for BI use cases. \nAgenda -\n\n1) Introduction and Ideal Use cases for Druid\n\n2) Data Architecture\n\n3) Streaming Ingestion with Kafka\n\n4) Demo using Druid, Kafka and Superset.\n\n5) Recent Improvements in Druid moving from lambda architecture to Exactly once Ingestion\n\n6) Future Work \nThis session is a  (Beginner) talk in our Data Processing and Warehousing track. It focuses on Apache Kafka, Druid, OLAP and is geared towards Data Scientist, Data Analyst, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Sriharsha Chintalapani"}, {"bio": "", "corp": "Hortonworks", "name": "Arun Mahadevan"}], "base_fname": "Its_Finally_Here_Building_Complex_Streaming_Analytics_Apps_in_under_10_mins_without_writing_any_code", "title": "Its Finally Here!  Building Complex Streaming Analytics Apps in under 10 mins without writing any code", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/its-finally-here-building-complex-streaming-analytics-apps-in-under-10-min-without-writing-any-code"}, "tag": "IoT and Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/sHmOcyQiLaI"}, "desc": "Imagine if you could build and deploy an end to end complex streaming analytics app on a streaming engine like Storm or Flink that did the following:\n\n    1. Joining Streams\n\n    2. Aggregations over Windows (Time or Count based)\n\n    3. Complex Event Processing\n\n    4. Pattern Matching\n\n    5. Model scoring.\n\nNow imagine implementing and deploying  this without writing a single line of code in under 10 mins.\n\nImagine no more; it is indeed here.  In this talk, we will discuss an exciting open source project led by Hortonworks on building and deploying streaming applications using a drag and drop paradigm.  \nThis session is a  (Advanced) talk in our IoT and Streaming track. It focuses on Apache Ambari, Apache Flink, Apache Kafka, Apache Storm and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "SciSpike", "name": "Vladimir Bacvanski"}], "base_fname": "Introduction_to_Deep_Learning_and_AI_at_Scale_for_Managers", "title": "Introduction to Deep Learning and AI at Scale for Managers", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/introduction-to-deep-learning-and-ai-at-scale-for-managers"}, "tag": "Apache Spark and Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/El38bC0Ix4Y"}, "desc": "Deep Learning and the new wave of AI are inevitably coming to your business area. If you are a manager and if you are trying to make sense of all the buzzwords, this session is four you. We will show you what is Deep Learning in a way that you will understand how it works and how can you apply it. We then expand the scope and apply the deep learning and AI techniques in the Big Data context. You will learn about things that don't work out so well, the risks and challenges in both applying and developing with deep learning and AI technologies. We conclude with practical guidance on how to add the exciting deep learning and AI capabilities to your next project.\n\nOutline:\n\n- The path to Deep Learning\n\n- From machine learning to Deep Learning\n\n- But how does it work?\n\n- Deep Learning architectures\n\n- Deep Learning applications\n\n- Deep Learning at scale\n\n- Running AI at scale\n\n- Deep learning at Scale using Spark\n\n- The trouble with AI\n\n- Application challenges\n\n- Development challenges\n\n- How to start your first Deep Learning project\nThis session is a  talk in our Apache Spark and Data Science track. It focuses on Apache Flink, Apache Hadoop, Apache Spark, Cloud, Other and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Scott Gnau"}], "base_fname": "Analyst_Panel_Unravels_the_Data_Industry", "title": "Analyst Panel Unravels the Data Industry", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise Adoption", "video": {"dl_link": "", "src_link": "https://youtu.be/jCr1aUCCNjY"}, "desc": "The community is innovating at such an extraordinarily rapid pace that sometimes it seems impossible to stay abreast of the latest trends and disruptions across the industry. How can enterprise business and technical decision makers know what their competition is doing, what new tech can transform their business, or how to optimize their architecture? Enter the industry analysts. In this session, Hortonworks\u2019 Chief Technology Officer will moderate a panel of top industry analysts from Forrester and Ovum and 451 Research to sort out cloud adoption, container wars, what\u2019s real with AI/ML, decipher the streaming/IoT landscape, and more.\nScott will be joined by;\n\nTony Baer, Principal Analyst, Ovum\n\nJim Curtis, Senior Analyst, 451 Research\n\nBrian Hopkins, VP and Principal Analyst, Forrester  \nThis session is a  talk in our Enterprise Adoption track. It focuses on Apache Hadoop, Apache Nifi, Apache Ranger, Apache Spark, Cloud and is geared towards CXO, Architect, Data Scientist, Data Analyst audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "P. Taylor Goetz"}], "base_fname": "Large_Scale_Graph_Analytics_with_JanusGraph", "title": "Large Scale Graph Analytics with JanusGraph", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/large-scale-graph-analytics-with-janusgraph-77153443"}, "tag": "Data Processing and Warehousing", "video": {"dl_link": "", "src_link": "https://youtu.be/OzT1cQHcZ-4"}, "desc": "The past few years have seen an enormous growth in the popularity of graph databases, but what exactly is a graph database and how can I use one to gain deeper insights from my data?\nIn this session we will introduce JanusGraph, a highly scalable, transactional graph database with flexible backend storage options such as Apache HBase, Apache Cassandra, and Oracle Berkeley DB. We will begin with a brief introduction to graph databases and data models, common use cases, and the benefits of a relationship centric approach to analytics. We will follow with a more technical dive into the features and deployment options of JanusGraph, including accessing the graph with the Apache Tinkerpop API stack, manipulating it with the Blueprints API, and querying the graph with the Gremlin query language. Finally, we will look at how JanusGraph integrates with other technologies like Apache Spark as part of an overall analytics architecture.\nThis session is a  (Beginner) talk in our Data Processing and Warehousing track. It focuses on Apache Cassandra, Apache Hadoop, Apache HBase, Apache Spark, Apache Solr and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."},
{"speakers": [{"bio": "", "corp": "LinkedIn", "name": "Savitha Ravikrishnan"}, {"bio": "", "corp": "Yahoo!", "name": "Sameer Gawande"}], "base_fname": "Handling_Kernel_Upgrades_at_Scale_The_Dirty_Cow_Story", "title": "Handling Kernel Upgrades at Scale - The Dirty Cow Story", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/handling-kernel-upgrades-at-scale-the-dirty-cow-story"}, "tag": "Apache Hadoop", "video": {"dl_link": "", "src_link": "https://youtu.be/QoXjCIRPePU"}, "desc": "Apache Hadoop at Yahoo is a massive platform with 36 different clusters spread across YARN, Apache HBase, and Apache Storm deployments, totaling 60,000 servers made up of 100s of different hardware configurations accumulated over generations, presenting unique operational challenges and a variety of unforeseen corner cases. In this talk, we will share methods, tips and tricks to deal with large scale kernel upgrade on heterogeneous platforms within tight timeframes with 100% uptime and no service or data loss through the Dirty COW use case (privilege escalation vulnerability found in the Linux Kernel in late 2016). \nWe will dive deep into our three phased approach that led to eventual success of the program - pre work, kernel upgrade itself, and post work / cleanup. We will share the details on automation tools, UIs, and reporting tools developed and used to achieve the stated objectives of 800+ server upgrades per hour, track the upgrade progress, validate and report data blocks, and recover quickly from bad blocks encountered. Throughout the talk, we will highlight the importance of process management, communicating with 100s of custom teams to ensure they are onboard and aware, and successful coordination tactics with SREs and Site Operations. We will also touch upon some of the unique challenges we faced along with way such as BIOS updates necessary on over 20,000 hosts along the way, and explain system rolling upgrade support we added to HBase and Storm for avoiding service disruption to low latency customer during these upgrades.\nThis session is a  (Advanced) talk in our Apache Hadoop track. It focuses on Apache Storm and is geared towards CXO, Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT, IT / Line-Of-Business Manager audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Nadeem Asghar"}], "base_fname": "How_Big_Data_and_Predictive_Analytics_are_revolutionizing_AML_and_Financial_Crime_Detection", "title": "How Big Data and Predictive Analytics are revolutionizing AML and Financial Crime Detection", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/how-big-data-and-predictive-analytics-are-revolutionizing-aml-and-financial-crime-detection"}, "tag": "Applications", "video": {"dl_link": "", "src_link": "https://youtu.be/YpJlbTaJz-E"}, "desc": "Banks, Payment Providers and capital markets firms are under intense regulatory mandate to process huge amounts of transaction-related data from both traditional and non-traditional sources. Compliance teams need to constantly analyze data-in-motion (wires, fund transfers, banking transactions) and data-at-rest (years worth of historical data) for actionable intelligence required for Suspicious Activity Reports\u2014to discover illegal activity and provide detailed reporting to authorities. Annual estimates of global money laundering flows ranging anywhere from $ 1 trillion to 2 trillion \u2013 almost 5% of global GDP.  Almost all of this is laundered via Retail & Merchant Banks,  Payment Networks, Securities & Futures firms, Casino Services & Clubs etc \u2013 which explains why annual AML related fines on Banking organizations run into the billions and are increasing every year. However, the number of SARs (Suspicious Activity Reports) filed by banking institutions are much higher as a category as compared to the numbers filed by these other businesses. In this presentation we will discuss the business imperatives, value drivers and the woeful inadequacy of current technology architectures and approaches in tackling AML. We will then pivot to a deepdive around Big Data and Predictive Analytics in how they can ease and solve these vexing challenges that Banking executives are grappling with globally.\nThis session is a  talk in our Applications track. It focuses on Apache Atlas, Apache Hadoop, Apache HBase, Apache Spark, Apache Zeppelin and is geared towards CXO, Architect, Data Scientist, Developer / Engineer audiences."},
{"speakers": [{"bio": "", "corp": "Hortonworks", "name": "Sanjay Radia"}], "base_fname": "Dancing_elephants_efficiently_working_with_object_stores_from_Apache_Spark_and_Apache_Hive", "title": "Dancing elephants \u2013 efficiently working with object stores from Apache Spark and Apache Hive", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/Hadoop_Summit/dancing-elephants-efficiently-working-with-object-stores-from-apache-spark-and-apache-hive"}, "tag": "Cloud and Operations", "video": {"dl_link": "", "src_link": "https://youtu.be/LxCh0jqG8xU"}, "desc": "As Hadoop applications move into cloud deployments, object stores become more and more the source and destination of data. But object stores are not filesystems: sometimes they are slower; security is different,\nWhat are the secret settings to get maximum performance from queries against data living in cloud object stores? That's at the filesystem client, the file format and the query engine layers? It's even how you lay out the files \u2014the directory structure and the names you give them.\nWe know these things, from our work in all these layers, from the benchmarking we've done \u2014and the support calls we get when people have problems. And now: we'll show you.\nThis talk will start from the ground up \"why isn't an object store a filesystem?\" issue, showing how that breaks fundamental assumptions in code, and so causes performance issues which you don't get when working with HDFS. We'll look at the ways to get Apache Hive and Spark to work better, looking at optimizations which have been done to enable this \u2014and what work is ongoing. Finally, we'll consider what your own code needs to do in order to adapt to cloud execution.\nThis session is a  (Intermediate) talk in our Cloud and Operations track. It focuses on Apache Hadoop, Apache Hive, Apache Spark, Cloud, Docker / Container and is geared towards Architect, Data Scientist, Data Analyst, Developer / Engineer, Operations / IT audiences."}
]