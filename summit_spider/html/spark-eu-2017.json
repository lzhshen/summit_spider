[
{"speakers": [{"corp": "Databricks", "name": "Matei Zaharia"}, {"corp": "Databricks", "name": "Sue Ann Hong"}], "base_fname": "Deep_Learning_and_Streaming_in_Apache_Spark_2_x", "title": "Deep Learning and Streaming in Apache Spark 2.x", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/databricks/deep-learning-and-streaming-in-apache-spark-2x-with-matei-zaharia-81232855"}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/zom9J9sK6wY"}, "desc": "2017 continues to be an exciting year for Apache Spark. I will talk about new updates in two major areas in the Spark community this year: stream processing with Structured Streaming, and deep learning with high-level libraries such as Deep Learning Pipelines and TensorFlowOnSpark. In both areas, the community is making powerful new functionality available in the same high-level APIs used in the rest of the Spark ecosystem (e.g., DataFrames and ML Pipelines), and improving both the scalability and ease of use of stream processing and machine learning."},
{"speakers": [{"corp": "Databricks", "name": "Tathagata Das"}], "base_fname": "Easy_Scalable_Fault_Tolerant_Stream_Processing_with_Structured_Streaming_in_Apache_Spark_continues", "title": "Easy, Scalable, Fault-Tolerant Stream Processing with Structured Streaming in Apache Spark - continues", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/databricks/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-with-tathagata-das"}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/QRRGI4EysSI"}, "desc": "Last year, in Apache Spark 2.0, Databricks introduced Structured Streaming, a new stream processing engine built on Spark SQL, which revolutionized how developers could write stream processing application. Structured Streaming enables users to express their computations the same way they would express a batch query on static data. Developers can express queries using powerful high-level APIs including DataFrames, Dataset and SQL. Then, the Spark SQL engine is capable of converting these batch-like transformations into an incremental execution plan that can process streaming data, while automatically handling late, out-of-order data and ensuring end-to-end exactly-once fault-tolerance guarantees.Since Spark 2.0, Databricks has been hard at work building first-class integration with Kafka. With this new connectivity, performing complex, low-latency analytics is now as easy as writing a standard SQL query. This functionality, in addition to the existing connectivity of Spark SQL, makes it easy to analyze data using one unified framework. Users can now seamlessly extract insights from data, independent of whether it is coming from messy / unstructured files, a structured / columnar historical data warehouse, or arriving in real-time from Kafka/Kinesis.In this session, Das will walk through a concrete example where \u2013 in less than 10 lines \u2013 you read Kafka, parse JSON payload data into separate columns, transform it, enrich it by joining with static data and write it out as a table ready for batch and ad-hoc queries on up-to-the-last-minute data. He\u2019ll use techniques including event-time based aggregations, arbitrary stateful operations, and automatic state management using event-time watermarks."},
{"speakers": [{"corp": "Microsoft", "name": "Ali Zaidi"}], "base_fname": "Deep_Natural_Language_Processing_with_R_and_Apache_Spark", "title": "Deep Natural Language Processing with R and Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/FGNp3s2iuJo"}, "desc": "Neural embeddings (Bengio et al. (2003), Olah (2014)) aim to map words, tokens, and general compositions of text to vector spaces, which makes them amenable for modeling, visualization, and inference. In this talk, we describe how to use neural embeddings of natural and programming languages using R and Spark. In particular, we\u2019ll see how the combination of a distributed computing paradigm in Spark with the interactive programming and visualization capabilities in R can make exploration and inference of natural language processing models easy and efficient. Building upon the tidy data principles formalized and efficiently crafted in Wickham (2014), Silge and Robinson (2016) have provided the foundations for modeling and crafting natural language models with the tidytext package. In this talk, we\u2019ll describe how we can build scalable pipelines within this framework to prototype text mining and neural embedding models in R, and then deploy them on Spark clusters using the sparklyr and the RevoScaleR packages. To describe the utility of this framework we\u2019ll provide an example where we\u2019ll train a sequence to sequence neural attention model for summarizing git commits, pull request and their associated messages (Zaidi (2017)), and then deploy them on Spark clusters where we will then be able to do efficient network analysis on the neural embeddings with a sparklyr extension to GraphFrames. References Bengio, Yoshua, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. \u201cA Neural Probabilistic Language Model.\u201d J. Mach. Learn. Res. 3 (March). JMLR.org: 1137-55. http://dl.acm.org/citation.cfm?id=944919.944966. Olah, Christopher. 2014. \u201cDeep Learning, NLP, and Representations.\u201d https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/. Silge, Julia, and David Robinson. 2016. \u201cTidytext: Text Mining and Analysis Using Tidy Data Principles in R.\u201d JOSS 1 (3). The Open Journal. doi:10.21105/joss.00037. Wickham, Hadley. 2014. \u201cTidy Data.\u201d Journal of Statistical Software 59 (1): 1-23. doi:10.18637/jss.v059.i10. Zaidi, Ali. 2017. \u201cSummarizing Git Commits and Github Pull Requests Using Sequence to Sequence Neural Attention Models.\u201d CS224N: Final Project,Session hashtag: EUds2"},
{"speakers": [{"corp": "Lightbend", "name": "Gerard Maas"}], "base_fname": "Apache_Spark_Streaming_Programming_Techniques_You_Should_Know", "title": "Apache Spark Streaming Programming Techniques You Should Know", "slide": {"dl_link": "", "src_link": ""}, "tag": "Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/iIo1hN-2JEI"}, "desc": "At its heart, Spark Streaming is a scheduling framework, able to efficiently collect and deliver data to Spark for further processing. While the DStream abstraction provides high-level functions to process streams, several operations also grant us access to deeper levels of the API, where we can directly operate on RDDs, transform them to Datasets to make use of that abstraction or store the data for later processing. Between these API layers lie many hooks that we can manipulate to enrich our Spark Streaming jobs. In this presentation we will demonstrate how to tap into the Spark Streaming scheduler to run arbitrary data workloads, we will show practical uses of the forgotten \u2018ConstantInputDStream\u2019 and will explain how to combine Spark Streaming with probabilistic data structures to optimize the use of memory in order to improve the resource usage of long-running streaming jobs. Attendees of this session will come out with a richer toolbox of techniques to widen the use of Spark Streaming and improve the robustness of new or existing jobs.Session hashtag: #EUstr2"},
{"speakers": [{"corp": "CERN", "name": "Luca Canali"}], "base_fname": "Apache_Spark_Performance_Troubleshooting_at_Scale_Challenges_Tools_and_Methodologies", "title": "Apache Spark Performance Troubleshooting at Scale, Challenges, Tools, and Methodologies", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/JoQ8m-kM_ZY"}, "desc": "This talk is about methods and tools for troubleshooting Spark workloads at scale and is aimed at developers, administrators and performance practitioners. You will find examples illustrating the importance of using the right tools and right methodologies for measuring and understanding performance, in particular highlighting the importance of using data and root cause analysis to understand and improve the performance of Spark applications. The talk has a strong focus on practical examples and on tools for collecting data relevant for performance analysis. This includes tools for collecting Spark metrics and tools for collecting OS metrics. Among others, the talk will cover sparkMeasure, a tool developed by the author to collect Spark task metric and SQL metrics data, tools for analysing I/O and network workloads, tools for analysing CPU usage and memory bandwidth, tools for profiling CPU usage and for Flame Graph visualization.Session hashtag: #EUdev2"},
{"speakers": [{"corp": "Facebook", "name": "Brandon Carl"}], "base_fname": "Lessons_Learned_Developing_and_Managing_Massive_300TB_Apache_Spark_Pipelines_in_Production", "title": "Lessons Learned Developing and Managing Massive (300TB+) Apache Spark Pipelines in Production", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/zP-6hjY7SMM"}, "desc": "With more than 700 million monthly active users, Instagram continues to make it easier for people across the globe to join the community, share their experiences, and strengthen connections to their friends and passions. Powering Instagram\u2019s various products requires the use of machine learning, high performance ranking services, and most importantly large amounts of data. At Instagram, we use Apache Spark for several critical production pipelines, including generating labeled training data for our machine learning models. In this session, you\u2019ll learn about how one of Instagram\u2019s largest Spark pipelines has evolved over time in order to process ~300 TB of input and ~90 TB of shuffle data. We\u2019ll discuss the experience of building and managing such a large production pipeline and some tips and tricks we\u2019ve learned along the way to manage Spark at scale. Topics include migrating from RDD to Dataset for better memory efficiency, splitting up long-running pipelines in order to better tune intermediate shuffle data, and dealing with changing data skew over time. Finally, we will also go over some optimizations we have made in order to maintain reliability of this critical data pipeline.Session hashtag: #EUde0"},
{"speakers": [{"corp": "Databricks", "name": "Tathagata Das"}], "base_fname": "Easy_Scalable_Fault_Tolerant_Stream_Processing_with_Structured_Streaming_in_Apache_Spark", "title": "Easy, Scalable, Fault-Tolerant Stream Processing with Structured Streaming in Apache Spark", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/databricks/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-with-tathagata-das"}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/_jPKqJ-gaIY"}, "desc": "Last year, in Apache Spark 2.0, Databricks introduced Structured Streaming, a new stream processing engine built on Spark SQL, which revolutionized how developers could write stream processing application. Structured Streaming enables users to express their computations the same way they would express a batch query on static data. Developers can express queries using powerful high-level APIs including DataFrames, Dataset and SQL. Then, the Spark SQL engine is capable of converting these batch-like transformations into an incremental execution plan that can process streaming data, while automatically handling late, out-of-order data and ensuring end-to-end exactly-once fault-tolerance guarantees.Since Spark 2.0, Databricks has been hard at work building first-class integration with Kafka. With this new connectivity, performing complex, low-latency analytics is now as easy as writing a standard SQL query. This functionality, in addition to the existing connectivity of Spark SQL, makes it easy to analyze data using one unified framework. Users can now seamlessly extract insights from data, independent of whether it is coming from messy / unstructured files, a structured / columnar historical data warehouse, or arriving in real-time from Kafka/Kinesis.In this session, Das will walk through a concrete example where \u2013 in less than 10 lines \u2013 you read Kafka, parse JSON payload data into separate columns, transform it, enrich it by joining with static data and write it out as a table ready for batch and ad-hoc queries on up-to-the-last-minute data. He\u2019ll use techniques including event-time based aggregations, arbitrary stateful operations, and automatic state management using event-time watermarks.Session hashtag: #EUdd1"},
{"speakers": [{"corp": "Netflix", "name": "DB Tsai"}, {"corp": "Netflix", "name": "Roger Menezes"}], "base_fname": "VEGAS_The_Missing_Matplotlib_for_Scala_Apache_Spark", "title": "VEGAS: The Missing Matplotlib for Scala/Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/5GfQVsnHj88"}, "desc": "In this talk, we\u2019ll present techniques for visualizing large scale machine learning systems in Spark. These are techniques that are employed by Netflix to understand and refine the machine learning models behind Netflix\u2019s famous recommender systems that are used to personalize the Netflix experience for their 99 millions members around the world. Essential to these techniques is Vegas, a new OSS Scala library that aims to be the \u201cmissing MatPlotLib\u201d for Spark/Scala. We\u2019ll talk about the design of Vegas and its usage in Scala notebooks to visualize Machine Learning Models.Session hashtag: #EUds0"},
{"speakers": [{"corp": "Intel", "name": "Xiaochang Wu"}], "base_fname": "Apache_Spark_Structured_Streaming_Helps_Smart_Manufacturing", "title": "Apache Spark Structured Streaming Helps Smart Manufacturing", "slide": {"dl_link": "", "src_link": ""}, "tag": "Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/Su2XhnEIAno"}, "desc": "This presentation introduces how we design and implement a real-time processing platform using latest Spark Structured Streaming framework to intelligently transform the production lines in the manufacturing industry. In the traditional production line there are a variety of isolated structured, semi-structured and unstructured data, such as sensor data, machine screen output, log output, database records etc. There are two main data scenarios: 1) Picture and video data with low frequency but a large amount; 2) Continuous data with high frequency. They are not a large amount of data per unit. However the total amount of them is very large, such as vibration data used to detect the quality of the equipment. These data have the characteristics of streaming data: real-time, volatile, burst, disorder and infinity. Making effective real-time decisions to retrieve values from these data is critical to smart manufacturing. The latest Spark Structured Streaming framework greatly lowers the bar for building highly scalable and fault-tolerant streaming applications. Thanks to the Spark we are able to build a low-latency, high-throughput and reliable operation system involving data acquisition, transmission, analysis and storage. The actual user case proved that the system meets the needs of real-time decision-making. The system greatly enhance the production process of predictive fault repair and production line material tracking efficiency, and can reduce about half of the labor force for the production lines.Session hashtag: #EUstr1"},
{"speakers": [{"corp": "Databricks", "name": "Tim Hunter"}], "base_fname": "From_Pipelines_to_Refineries_Building_Complex_Data_Applications_with_Apache_Spark", "title": "From Pipelines to Refineries: Building Complex Data Applications with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/ET7_6i-1Tng"}, "desc": "Big data tools are challenging to combine into a larger application: ironically, big data applications themselves do not tend to scale very well. These issues of integration and data management are only magnified by increasingly large volumes of data. Apache Spark provides strong building blocks for batch processes, streams and ad-hoc interactive analysis. However, users face challenges when putting together a single coherent pipeline that could involve hundreds of transformation steps, especially when confronted by the need of rapid iterations. This talk explores these issues through the lens of functional programming. It presents an experimental framework that provides full-pipeline guarantees by introducing more laziness to Apache Spark. This framework allows transformations to be seamlessly composed and alleviates common issues, thanks to whole program checks, auto-caching, and aggressive computation parallelization and reuse.Session hashtag: #EUdev1"},
{"speakers": [{"corp": "Databricks", "name": "Ali Ghodsi"}, {"corp": "Databricks", "name": "Michael Armbrust"}], "base_fname": "Announcing_Databricks_Delta", "title": "Announcing Databricks Delta", "slide": {"dl_link": "", "src_link": ""}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/-eZkegBnyMU"}, "desc": "Databricks CEO Ali Ghodsi introduces Databricks Delta, a new data management system that combines the scale and cost-efficiency of a data lake, the performance and reliability of a data warehouse, and the low latency of streaming."},
{"speakers": [{"corp": "Hotels.com", "name": "Matt Fryer"}], "base_fname": "Hotels_com_s_Journey_to_Becoming_an_Algorithmic_Business_Exponential_Growth_in_Data_Science_Whilst_Migrating_to_Apache_Spark_Cloud_All_at_the_Same_Time", "title": "Hotels.com's Journey to Becoming an Algorithmic Business...Exponential Growth in Data Science Whilst Migrating to Apache Spark+Cloud All at the Same Time", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/SparkSummit/hotelscoms-journey-to-becoming-an-algorithmic-businessexponential-growth-in-data-science-whilst-migrating-to-apache-sparkcloud-all-at-the-same-time-with-matt-fryer-81397080"}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/knUnPiiqpJc"}, "desc": "In the last year Hotels.com has begun it\u2019s journey to becoming an algorithmic business. Matt will talk about their experiences of exponential growth in Data Science Algorithms whilst at the same time the team have migrated to using Spark as their core underlying architecture from SAS / SQL, migrated to the cloud from on-premise are transforming the capability of the data science function. He will also highlight the key enablers that have made this successful including CEO support, the internal concepts of organic intelligence and how Databricks has helped make this happen. He will also highlight the pitfalls on the journey."},
{"speakers": [{"corp": "Altocloud", "name": "Gevorg Soghomonyan"}, {"corp": "Altocloud", "name": "Maciej Dabrowski"}], "base_fname": "Applying_Multiple_ML_Pipelines_to_Heterogeneous_Data_Streams", "title": "Applying Multiple ML Pipelines to Heterogeneous Data Streams", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/EhRHQPCdldI"}, "desc": "Spark ML Pipelines provide a comprehensive framework for predictive modeling, including feature engineering, batch model training, and real-time predictions based on streams of data. For example, a model predicting likelihood of cart abandonment may be trained periodically using features based on Web activity of customers and applied to a stream of Web events to make real-time predictions for live users. However, in multi-tenant environments where streams contain events from different sources, application of ML Pipelines becomes difficult. Even though the pipeline paradigm can be applied to model training using datasets that contain events separated by source, generating real-time prediction in Spark Streaming poses multiple challenges, since a single micro-batch contains events that require evaluation of different pipelines. In this talk we will show how Altocloud applies Spark Pipelines to train hundreds of predictive models and to enable real-time predictions on high-throughput heterogeneous data streams. In particular we will focus on: 1. Training multiple models for activity streams from different sources. 2. Application of these models in real-time to a heterogeneous stream of events containing behavioural data for millions of users. 3. Automated training, validation, selection, and deployment of multiple predictive models in a multi-tenant environment at scale.Session hashtag: #EUds4"},
{"speakers": [{"corp": "Intel", "name": "Carson Wang"}], "base_fname": "An_Adaptive_Execution_Engine_For_Apache_Spark_SQL", "title": "An Adaptive Execution Engine For Apache Spark SQL", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/FZgojLWdjaw"}, "desc": "Catalyst is an excellent optimizer in SparkSQL, provides open interface for rule-based optimization in planning stage. However, the static (rule-based) optimization will not consider any data distribution at runtime. A technology called Adaptive Execution has been introduced since Spark 2.0 and aims to cover this part, but still pending in early stage. We enhanced the existing Adaptive Execution feature, and focus on the execution plan adjustment at runtime according to different staged intermediate outputs, like set partition numbers for joins and aggregations, avoid unnecessary data shuffling and disk IO, handle data skew cases, and even optimize the join order like CBO etc.. In our benchmark comparison experiments, this feature save huge manual efforts in tuning the parameters like the shuffled partition number, which is error-prone and misleading. In this talk, we will expose the new adaptive execution framework, task scheduling, failover retry mechanism, runtime plan switching etc. At last, we will also share our experience of benchmark 100 -300 TB scale of TPCx-BB in a hundreds of bare metal Spark cluster.Session hashtag: EUdev4"},
{"speakers": [{"corp": "CERN", "name": "Jakub Wozniak"}], "base_fname": "The_Architecture_of_the_Next_CERN_Accelerator_Logging_Service", "title": "The Architecture of the Next CERN Accelerator Logging Service", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/80-csZJK7kc"}, "desc": "The Next Accelerator Logging Service (NXCALS) is a new Big Data project at CERN aiming to replace the existing Oracle-based service.The main purpose of the system is to store and present Controls/Infrastructure related data gathered from thousands of devices in the whole accelerator complex.During this talk, Jakub will speak about NXCALS requirements and design choices that lead to the selected architecture based on Hadoop and Spark. He will present the Ingestion API, the abstractions behind the Meta-data Service and the Spark-based Extraction API where simple changes to the schema handling greatly improved the overall usability of the system. The system itself is not CERN specific and can be of interest to other companies or institutes confronted with similar Big Data problems.Session hashtag: #EUent9"},
{"speakers": [{"corp": "Cloudera", "name": "Tom White"}], "base_fname": "Practical_Genomics_with_Apache_Spark", "title": "Practical Genomics with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/EzdZt-5T3q0"}, "desc": "Discussions about the role of technology in genomics invariably focus on the massive growth in DNA sequencing since the beginning of the century, growth faster than Moore\u2019s law and which has led to the $1000 genome. However, future growth is projected to be even more spectacular, and to be a reality we need more powerful tools for genome analysis. Apache Spark is providing the foundation for these new tools, including two that I will cover in this talk: GATK and Hail, both open source projects from the Broad Institute. GATK and Hail are complementary: GATK provides pipelines for transforming DNA sequence data into the raw material (variant call data) needed by Hail to run genetic analysis across thousands of individuals. GATK started out originally as a single process program, but has now been ported to run on Spark at scale. Hail was written from the outset to run on Spark. In this talk I will look at how these frameworks take advantage of Spark to scale, some of the challenges in getting existing data formats to work with Spark, and some of the plans for the future.Session hashtag: #EUres9"},
{"speakers": [{"corp": "9LivesData", "name": "Marcin Kulka"}, {"corp": "9LivesData", "name": "Micha\u0142 Kaczmarczyk"}], "base_fname": "No_More_Cumbersomeness_Automatic_Predictive_Modeling_on_Apache_Spark", "title": "No More Cumbersomeness: Automatic Predictive Modeling on Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/2nJMNkUR4w8"}, "desc": "Building accurate machine learning models has been an art of data scientists, i.e., algorithm selection, hyper parameter tuning, feature selection and so on. Recently, challenges to breakthrough this \u201cblack-arts\u201d have got started. In cooperation with our partner, NEC Laboratories America, we have developed a Spark-based automatic predictive modeling system. The system automatically searches the best algorithm, parameters and features without any manual work. In this talk, we will share how the automation system is designed to exploit attractive advantages of Spark. The evaluation with real open data demonstrates that our system can explore hundreds of predictive models and discovers the most accurate ones in minutes on a Ultra High Density Server, which employs 272 CPU cores, 2TB memory and 17TB SSD in 3U chassis. We will also share open challenges to learn such a massive amount of models on Spark, particularly from reliability and stability standpoints. This talk will cover the presentation already shown on Spark Summit SF\u201917 (#SFds5) but from more technical perspective.Session hashtag: #EUai9"},
{"speakers": [{"corp": "InfoEdge Ltd.", "name": "Arvind Heda"}, {"corp": "Amazon", "name": "Kapil Malik"}], "base_fname": "Indicium_Interactive_Querying_at_Scale_Using_Apache_Spark_Zeppelin_and_Spark_Job_Server", "title": "Indicium: Interactive Querying at Scale Using Apache Spark, Zeppelin, and Spark Job-Server", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/IuiBNAjlNuM"}, "desc": "Kapil Malik and Arvind Heda will discuss a solution for interactive querying of large scale structured data, stored in a distributed file system (HDFS / S3), in a scalable and reliable manner using a unique combination of Spark SQL, Apache Zeppelin and Spark Job-server (SJS) on Yarn. The solution is production tested and can cater to thousands of queries processing terabytes of data every day. It contains following components \u2013 1. Zeppelin server : A custom interpreter is deployed, which de-couples spark context from the user notebooks. It connects to the remote spark context on Spark Job-server. A rich set of APIs are exposed for the users. The user input is parsed, validated and executed remotely on SJS. 2. Spark job-server : A custom application is deployed, which implements the set of APIs exposed on Zeppelin custom interpreter, as one or more spark jobs. 3. Context router : It routes different user queries from custom interpreter to one of many Spark Job-servers / contexts. The solution has following characteristics \u2013 * Multi-tenancy There are hundreds of users, each having one or more Zeppelin notebooks. All these notebooks connect to same set of Spark contexts for running a job. * Fault tolerance The notebooks do not use Spark interpreter, but a custom interpreter, connecting to a remote context. If one spark context fails, the context router sends user queries to another context. * Load balancing Context router identifies which contexts are under heavy load / responding slowly, and selects the most optimal context for serving a user query. * Efficiency We use Alluxio for caching common datasets. * Elastic resource usage We use spark dynamic allocation for the contexts. This ensures that cluster resources are blocked by this application only when it\u2019s doing some actual work.Session hashtag: #EUeco9"},
{"speakers": [{"corp": "Between VCNC", "name": "Kevin (Sangwoo) Kim"}], "base_fname": "Powering_a_Startup_with_Apache_Spark", "title": "Powering a Startup with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/WIrTQSDbmEU"}, "desc": "In Between (A mobile App for couples, downloaded 20M in Global), from daily batch for extracting metrics, analysis and dashboard. Spark is widely used by engineers and data analysts in Between, thanks to the performance and expendability of Spark, data operating has become extremely efficient. Entire team including Biz Dev, Global Operation, Designers are enjoying data results so Spark is empowering entire company for data driven operation and thinking. Kevin, Co-founder and Data Team leader of Between will be presenting how things are going in Between. Listeners will know how small and agile team is living with data (how we build organization, culture and technical base) after this presentation.Session hashtag: #EUent8"},
{"speakers": [{"corp": "Institute of Communication and Computer Systems ICCS-NTUA", "name": "Christoforos Kachris"}], "base_fname": "Hardware_Acceleration_of_Apache_Spark_on_Energy_Efficient_FPGAs", "title": "Hardware Acceleration of Apache Spark on Energy-Efficient FPGAs", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/CrPR1oYJahs"}, "desc": "In this talk, we will present SPynq framework: A framework for the efficient mapping and acceleration of Spark applications on heterogeneous all-programmable MPSoC-based platforms, such as Zynq. Spark has been mapped to the Pynq platform and the proposed framework allows the seamlessly utilization of the programmable logic for the hardware acceleration of computational intensive Spark kernels. We have also developed the required libraries in Spark, by extending the MLLib library, that hides the accelerator\u2019s details to minimize the design effort to utilize the accelerators. A cluster of 4 nodes (workers) based on the all-programmable MPSoCs has been implemented and the proposed platform is evaluated in a typical machine learning application based on logistic regression. The logistic regression kernel has been developed as an accelerator and incorporated to the Spark. The developed system is compared to a high-performance Xeon cluster that is typically used in cloud computing. The performance evaluation shows that the heterogeneous accelerator-based MpSoC can achieve up to 2.3x system speedup compared with a Xeon system (with 90% accuracy) and 20x better energy-efficiency. For embedded application, the proposed system can achieve up to 40x speedup compared to the software only implementation on low-power embedded processors and 30x lower energy consumption.Session hashtag: #EUres8"},
{"speakers": [{"corp": "KTH\u2014Royal Institute of Technology", "name": "Jim Dowling"}], "base_fname": "Apache_Spark_and_Tensorflow_as_a_Service", "title": "Apache Spark-and-Tensorflow-as-a-Service", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/5bVubfUt7ec"}, "desc": "In Sweden, from the Rise ICE Data Center at www.hops.site, we are providing to reseachers both Spark-as-a-Service and, more recently, Tensorflow-as-a-Service as part of the Hops platform. In this talk, we examine the different ways in which Tensorflow can be included in Spark workflows, from batch to streaming to structured streaming applications. We will analyse the different frameworks for integrating Spark with Tensorflow, from Tensorframes to TensorflowOnSpark to Databrick\u2019s Deep Learning Pipelines. We introduce the different programming models supported and highlight the importance of cluster support for managing different versions of python libraries on behalf of users. We will also present cluster management support for sharing GPUs, including Mesos and YARN (in Hops Hadoop). Finally, we will perform a live demonstration of training and inference for a TensorflowOnSpark application written on Jupyter that can read data from either HDFS or Kafka, transform the data in Spark, and train a deep neural network on Tensorflow. We will show how to debug the application using both Spark UI and Tensorboard, and how to examine logs and monitor training.Session hashtag: #EUai8"},
{"speakers": [{"corp": "The Weather Company / IBM", "name": "Emily Curtin"}], "base_fname": "Apache_Spark_Bench_Simulate_Test_Compare_Exercise_and_Yes_Benchmark", "title": "Apache Spark-Bench: Simulate, Test, Compare, Exercise, and Yes, Benchmark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/GGO5q_TSdgI"}, "desc": "spark-bench is an open-source benchmarking tool, and it\u2019s also so much more. spark-bench is a flexible system for simulating, comparing, testing, and benchmarking Spark applications and Spark itself. spark-bench originally began as a benchmarking suite to get timing numbers on very specific algorithms mostly in the machine learning domain. Since then it has morphed into a highly configurable and flexible framework suitable for many use cases. This talk will discuss the high level design and capabilities of spark-bench before walking through some major, practical use cases. Use cases include, but are certainly not limited to: regression testing changes to Spark; comparing performance of different hardware and Spark tuning options; simulating multiple notebook users hitting a cluster at the same time; comparing parameters of a machine learning algorithm on the same set of data; providing insight into bottlenecks through use of compute-intensive and i/o-intensive workloads; and, yes, even benchmarking. In particular this talk will address the use of spark-bench in developing new features features for Spark core.Session hashtag: #EUeco8"},
{"speakers": [{"corp": "Ghent University", "name": "Miel Hostens"}], "base_fname": "Data_Mining_and_Prediction_Modelling_in_the_Dairy_Industry_Using_Time_Series_and_Sliding_Windows_with_Apache_Spark_2", "title": "Data Mining and Prediction Modelling in the Dairy Industry Using Time Series and Sliding Windows with Apache Spark 2", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/keqd1rW3x7E"}, "desc": "WHY \u2013 As a major livestock producer, the European Union is directly affected by the global need for more sustainable food production. Climate change will undoubtedly impact on farm animal production but the health and welfare of livestock is also of increasing public concern. Due to rapid development of precision livestock farming technologies and availability of high-throughput from milk sensors, large-scale massive data has become available on research farms. The preferred matrix to measure the biomarkers is milk, as it is more accessible than blood and allows low-cost, automated repeat sampling using \u2018in-line\u2019 sampling and analytical technologies.Session hashtag: #EUds14"},
{"speakers": [{"corp": "IBM", "name": "Ramya Raghavendra"}], "base_fname": "Improving_Traffic_Prediction_Using_Weather_Data", "title": "Improving Traffic Prediction Using Weather Data", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/TmhiJiNmUYE"}, "desc": "As common sense would suggest, weather has a definite impact on traffic. But how much? And under what circumstances? Can we improve traffic (congestion) prediction given weather data? Predictive traffic is envisioned to significantly impact how driver\u2019s plan their day by alerting users before they travel, find the best times to travel, and over time, learn from new IoT data such as road conditions, incidents, etc. This talk will cover the traffic prediction work conducted jointly by IBM and the traffic data provider. As a part of this work, we conducted a case study over five large metropolitans in the US, 2.58 billion traffic records and 262 million weather records, to quantify the boost in accuracy of traffic prediction using weather data. We will provide an overview of our lambda architecture with Apache Spark being used to build prediction models with weather and traffic data, and Spark Streaming used to score the model and provide real-time traffic predictions. This talk will also cover a suite of extensions to Spark to analyze geospatial and temporal patterns in traffic and weather data, as well as the suite of machine learning algorithms that were used with Spark framework. Initial results of this work were presented at the National Association of Broadcasters meeting in Las Vegas in April 2017, and there is work to scale the system to provide predictions in over a 100 cities. Audience will learn about our experience scaling using Spark in offline and streaming mode, building statistical and deep-learning pipelines with Spark, and techniques to work with geospatial and time-series data.Session hashtag: #EUent7"},
{"speakers": [{"corp": "Microsoft", "name": "Miruna Oprescu"}], "base_fname": "MMLSpark_Lessons_from_Building_a_SparkML_Compatible_Machine_Learning_Library_for_Apache_Spark", "title": "MMLSpark: Lessons from Building a SparkML-Compatible Machine Learning Library for Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/cEltmvfx51A"}, "desc": "With the rapid growth of available datasets, it is imperative to have good tools for extracting insight from big data. The Spark ML library has excellent support for performing at-scale data processing and machine learning experiments, but more often than not, Data Scientists find themselves struggling with issues such as: low level data manipulation, lack of support for image processing, text analytics and deep learning, as well as the inability to use Spark alongside other popular machine learning libraries.  To address these pain points, Microsoft recently released The Microsoft Machine Learning Library for Apache Spark (MMLSpark), an open-source machine learning library built on top of SparkML that seeks to simplify the data science process and integrate SparkML Pipelines with deep learning and computer vision libraries such as the Microsoft Cognitive Toolkit (CNTK) and OpenCV. With MMLSpark, Data Scientists can build models with 1/10th of the code through Pipeline objects that compose seamlessly with other parts of the SparkML ecosystem.  In this session, we explore some of the main lessons learned from building MMLSpark. Join us if you would like to know how to extend Pipelines to ensure seamless integration with SparkML, how to auto-generate Python and R wrappers from Scala Transformers and Estimators, how to integrate and use previously non-distributed libraries in a distributed manner and how to efficiently deploy a Spark library across multiple platforms.Session hashtag: #EUai7"},
{"speakers": [{"corp": "Hortonworks", "name": "Weiqing Yang"}], "base_fname": "Apache_Spark_Apache_HBase_Connector_Feature_Rich_and_Efficient_Access_to_HBase_through_Spark_SQL", "title": "Apache Spark\u2014Apache HBase Connector: Feature Rich and Efficient Access to HBase through Spark SQL", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/we53uiOlnLA"}, "desc": "Both Spark and HBase are widely used, but how to use them together with high performance and simplicity is a very challenging topic. Spark HBase Connector(SHC) provides feature rich and efficient access to HBase through Spark SQL. It bridges the gap between the simple HBase key value store and complex relational SQL queries and enables users to perform complex data analytics on top of HBase using Spark. SHC implements the standard Spark data source APIs, and leverages the Spark catalyst engine for query optimization. To achieve high performance, SHC constructs the RDD from scratch instead of using the standard HadoopRDD. With the customized RDD, all critical techniques can be applied and fully implemented, such as partition pruning, column pruning, predicate pushdown and data locality. The design makes the maintenance easy, while achieving a good tradeoff between performance and simplicity. In addition to fully supporting all the Avro schemas natively, SHC has also integrated natively with Phoenix data types. With SHC, Spark can execute batch jobs to read/write data from/into Phoenix tables. Phoenix can also read/write data from/into HBase tables created by SHC. For example, users can run a complex SQL query on top of an HBase table created by Phoenix inside Spark, perform a table join against an Dataframe which reads the data from a Hive table, or integrate with Spark Streaming to implement a more complicated system. In this talk, apart from explaining why SHC is of great use, we will also demo how SHC works, how to use SHC in secure/non-secure clusters, how SHC works with multiple secure HBase clusters, etc. This talk will also benefit people who use Spark and other data sources (besides HBase) as it inspires them with ideas of how to support high performance data source access at the Spark DataFrame level.Session hashtag: #EUeco7"},
{"speakers": [{"corp": "Here Technologies", "name": "Arvind Rao"}], "base_fname": "Histogram_Equalized_Heat_Maps_from_Log_Data_via_Apache_Spark", "title": "Histogram Equalized Heat Maps from Log Data via Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/n7ZpUrsUzZo"}, "desc": "Reverse geocoding is one of HERE Technologies most heavily used services. From its access logs geocodes can be extracted and then counted with respect to some cellulation of the earth\u2013-creating a sparse heat map. For our Place & Address Search products we use such a heat map to define a notion of relative place importance to rank and index addresses and places. However, the large data size, sparsity, and variations in traffic from which different global heat maps may be derived, makes faithful visualization and comparison a challenge. Additionally, common implementations of spatial image processing techniques that can help address the aforementioned challenges don\u2019t map directly onto Spark\u2019s computing engine.In this talk Arvind Rao will describe implementations of histogram equalization and kernel-based sparse image processing methods on Spark. Histogram equalization, which is best known as a method of contrast enhancement, automatically normalizes images, facilitating comparison. Along the way, Arvind will talk about how HERE uses heat maps as a feature in their autocompletion service, and say just enough about perception of contrast to put histogram equalization in context.Session hashtag: #EUds13"},
{"speakers": [{"corp": "ITAINNOVA", "name": "Francisco J. Lacueva"}, {"corp": "ITAINNOVA", "name": "Rosa Monta\u00f1\u00e9s"}], "base_fname": "Hiding_Apache_Spark_Complexity_for_Fast_Prototyping_of_Big_Data_Applications_Industry_4_0_and_Logistics_Success_Examples", "title": "Hiding Apache Spark Complexity for Fast Prototyping of Big Data Applications\u2014Industry 4.0 and Logistics Success Examples", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/QYZHnvp9kt8"}, "desc": "In many cases, Big Data becomes just another buzzword because of the lack of tools that can  support both the technological requirements for developing and deploying of the projects and/or the fluency of communication between the different profiles of people involved in the projects.In this talk, we will present Moriarty,  a set of tools for fast prototyping of Big Data applications that can be deployed in an Apache Spark environment. These tools support the creation of Big Data workflows using the already existing functional blocks or supporting the creation of new functional blocks. The created workflow can then be deployed in a Spark infrastructure and used through a REST API.For better understanding of Moriarty, the prototyping process and the way it hides the Spark environment to the Big Data users and developers, we will present it together with a couple of examples based on a Industry 4.0 success cases and other on a logistic success case.Session hashtag: #EUent6"},
{"speakers": [{"corp": "KTH", "name": "Ahsan Javed Awan"}], "base_fname": "Near_Data_Computing_Architectures_for_Apache_Spark_Challenges_and_Opportunities", "title": "Near Data Computing Architectures for Apache Spark: Challenges and Opportunities", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/7UZh1o4BbaE"}, "desc": "Scale-out big data processing frameworks like Apache Spark have been designed to use on off the shelf commodity machines where each machine has the modest amount of compute , memory and storage capacity. Recent advancement in the hardware technology motivates understanding Spark performance on novel hardware architectures. Our earlier work has shown that the performance of Spark based data analytics is bounded by the frequent accesses to the DRAM. In this talk, we argue in favor of Near Data Computing Architectures that enable processing the data where it resides (e.g Smart SSDs and Compute Memories) for Apache Spark. We envision a programmable logic based hybrid near-memory and near-storage compute architecture for Apache Spark. Furthermore we discuss the challenges involved to achieve 10x performance gain for Apache Spark on NDC architectures.Session hashtag: #EUres10"},
{"speakers": [{"corp": "IBM", "name": "Holden Karau"}, {"corp": "IBM", "name": "Nick Pentreath"}], "base_fname": "Extending_Apache_Spark_ML_Adding_Your_Own_Algorithms_and_Tools", "title": "Extending Apache Spark ML: Adding Your Own Algorithms and Tools", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/r_HIHvQzNOs"}, "desc": "Apache Spark\u2019s machine learning (ML) pipelines provide a lot of power, but sometimes the tools you need for your specific problem aren\u2019t available yet. This talk introduces Spark\u2019s ML pipelines, and then looks at how to extend them with your own custom algorithms. By integrating your own data preparation and machine learning tools into Spark\u2019s ML pipelines, you will be able to take advantage of useful meta-algorithms, like parameter searching and pipeline persistence (with a bit more work, of course). Even if you don\u2019t have your own machine learning algorithms that you want to implement, this session will give you an inside look at how the ML APIs are built. It will also help you make even more awesome ML pipelines and customize Spark models for your needs. And if you don\u2019t want to extend Spark ML pipelines with custom algorithms, you\u2019ll still benefit by developing a stronger background for future Spark ML projects. The examples in this talk will be presented in Scala, but any non-standard syntax will be explained.Session hashtag: #EUai6"},
{"speakers": [{"corp": "CSIRO Data61", "name": "Piotr Szul"}], "base_fname": "Variant_Apache_Spark_for_Bioinformatics", "title": "Variant-Apache Spark for Bioinformatics", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/GnywZLIvfA4"}, "desc": "This talk will showcase work done by the bioinformatics team at CSIRO in Sydney, Australia to make Spark more useful and usable for the bioinformatics community. They have created a custom library, variant-spark, which provides a DSL and also a custom implementation of Spark ML via random forests for genomic pipeline processing. We\u2019ve created a demo, using their \u2018Hipster-genome\u2019 and a Databricks notebook to better explain their library to the world-wide bioinformatics community. This notebooks compares results with another popular genomics library (HAIL.io) as well.Session hashtag: #EUeco6"},
{"speakers": [{"corp": "Comcast NBCUniversal", "name": "Nabeel Sarwar"}], "base_fname": "Art_of_Feature_Engineering_for_Data_Science", "title": "Art of Feature Engineering for Data Science", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/leTyvBPhYzw"}, "desc": "We will discuss what feature engineering is all about , various techniques to use and how to scale to 20000 column datasets using random forest, svd, pca. Also demonstrated is how we can build a service around these to save time and effort when building 100s of models. We will share how we did all this using spark ml to build logistic regression, neural networks, Bayesian networks, etc.Session hashtag: #EUds12"},
{"speakers": [{"corp": "Databricks", "name": "Myles Baker"}], "base_fname": "Experimental_Design_for_Distributed_Machine_Learning", "title": "Experimental Design for Distributed Machine Learning", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/databricks/experimental-design-for-distributed-machine-learning-with-myles-baker"}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/2wlG_MrHO2Y"}, "desc": "Session hashtag: #EUent5"},
{"speakers": [{"corp": "Systems Engineering Group of TU Dresden", "name": "Do Quoc Le"}], "base_fname": "Approximate_Computing_for_Stream_Analytics_in_Apache_Spark", "title": "Approximate Computing for Stream Analytics in Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/86hgL1bqW6g"}, "desc": "Approximate computing has recently emerged as a promising computing paradigm which allows making a systematic trade-off between the output accuracy and computation efficiency. Approximate computing is based on the observation that for many practical applications it is acceptable to approximate rather than produce exact output results. The idea behind approximate computing is to compute over a partial subset instead of the entire input data to achieve efficient execution. Unfortunately, state-of-the-art systems for approximate computing, such as BlinkDB and ApproxHadoop, are primarily geared towards batch analytics, where the input data remains unchanged during the course of sampling. Thus, these state-of-the-art systems cannot be deployed in the context of stream analytics where new data continuously arrives as an unbounded stream. In this talk, we will present the design of StreamApprox, a Spark-based stream analytics system for approximate computing. StreamApprox implements an online stratified reservoir sampling algorithm in Spark Streaming to produce approximate output with rigorous error bounds.Session hashtag: #EUres5"},
{"speakers": [{"corp": "Microsoft", "name": "Miruna Oprescu"}], "base_fname": "The_Power_of_Apache_Spark_on_Azure_Machine_Learning", "title": "The Power of Apache Spark on Azure Machine Learning", "slide": {"dl_link": "", "src_link": ""}, "tag": "Sponsored Sessions", "video": {"dl_link": "", "src_link": "https://youtu.be/VIVmpXuvgNs"}, "desc": "Azure Machine Learning is an integrated, end-to- data data science experience designed for professionals to prepare data and create, manage and deploy machine learning models at any scale."},
{"speakers": [{"corp": "Movile", "name": "Eiti Kimura"}, {"corp": "Movile", "name": "Flavio Cl\u00e9sio"}], "base_fname": "Preventing_Revenue_Leakage_and_Monitoring_Distributed_Systems_with_Machine_Learning", "title": "Preventing Revenue Leakage and Monitoring Distributed Systems with Machine Learning", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/8-eYUBgqOfM"}, "desc": "Have you imagined a simple machine learning solution able to prevent revenue leakage and monitor your distributed application? To answer this question, we offer a practical and a simple machine learning solution to create an intelligent monitoring application based on simple data analysis using Apache Spark MLlib. Our application uses linear regression models to make predictions and check if the platform is experiencing any operational problems that can impact in revenue losses. The application monitor distributed systems and provides notifications stating the problem detected, that way users can operate quickly to avoid serious problems which directly impact the company\u2019s revenue and reduce the time for action. We will present an architecture for not only a monitoring system, but also an active actor for our outages recoveries. At the end of the presentation you will have access to our training program source code and you will be able to adapt and implement in your company. This solution already helped to prevent about US$3mi in losses last year.Session hashtag: #EUai10"},
{"speakers": [{"corp": "IBM", "name": "Haohai Ma"}, {"corp": "IBM", "name": "Khalid Ahmed"}], "base_fname": "Running_Spark_Inside_Docker_Containers_From_Workload_to_Cluster", "title": "Running Spark Inside Docker Containers: From Workload to Cluster", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/A-5C_WsK8bM"}, "desc": "This presentation describes the journey we went through in containerizing Spark workload into multiple elastic Spark clusters in a multi-tenant kubernetes environment. Initially we deployed Spark binaries onto a host-level filesystem, and then the Spark drivers, executors and master can transparently migrate to run inside a Docker container by automatically mounting host-level volumes. In this environment, we do not need to prepare a specific Spark image in order to run Spark workload in containers. We then utilized Kubernetes helm charts to deploy a Spark cluster. The administrator could further create a Spark instance group for each tenant. A Spark instance group, which is akin to the Spark notion of a tenant, is logically an independent kingdom for a tenant\u2019s Spark applications in which they own dedicated Spark masters, history server, shuffle service and notebooks. Once a Spark instance group is created, it automatically generates its image and commits to a specified repository. Meanwhile, from Kubernetes\u2019 perspective, each Spark instance group is a first-class deployment and thus the administrator can scale up/down its size according to the tenant\u2019s SLA and demand. In a cloud-based data center, each Spark cluster can provide a Spark as a service while sharing the Kubernetes cluster. Each tenant that is registered into the service gets a fully isolated Spark instance group. In an on-prem Kubernetes cluster, each Spark cluster can map to a Business Unit, and thus each user in the BU can get a dedicated Spark instance group. The next step on this journey will address the resource sharing across Spark instance groups by leveraging new Kubernetes\u2019 features (Kubernetes31068/9), as well as the Elastic workload containers depending on job demands (Spark18278). Demo: https://www.youtube.com/watch?v=eFYu6o3-Ea4&t=5sSession hashtag: #EUeco5"},
{"speakers": [{"corp": "Red Hat", "name": "Erik Erlandson"}], "base_fname": "One_Pass_Data_Science_In_Apache_Spark_With_Generative_T_Digests", "title": "One-Pass Data Science In Apache Spark With Generative T-Digests", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/uKx5mTX4v3s"}, "desc": "The T-Digest has earned a reputation as a highly efficient and versatile sketching data structure; however, its applications as a fast generative model are less appreciated. Several common algorithms from machine learning use randomization of feature columns as a building block. Column randomization is an awkward and expensive operation when performed directly, but when implemented with generative T-Digests, it can be accomplished elegantly in a single pass that also parallelizes across Spark data partitions. In this talk Erik will review the principles of T-Digest sketching, and how T-Digests can be applied as generative models. He will explain how generative T-Digests can be used to implement fast randomization of columnar data, and conclude with demonstrations of T-Digest randomization applied to Variable Importance, Random Forest Clustering and Feature Reduction. Attendees will leave this talk with an understanding of T-Digest sketching, how T-Digests can be used as generative models, and insights into applying generative T-Digests to accelerate their own data science projects.Session hashtag: #EUds11"},
{"speakers": [{"corp": "Pure Storage", "name": "Ryan Sayre"}], "base_fname": "Pure1_Automating_SW_Dev_Triage_using_Apache_Spark", "title": "Pure1: Automating SW Dev Triage using Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Sponsored Sessions", "video": {"dl_link": "", "src_link": "https://youtu.be/c3W1RT2uc5g"}, "desc": "Explosive demands of software QA / code analysis require flexible pipelines using Spark alongside other modern stack components. We will cover the evolution of toolsets initially managing hundreds of tests to our Spark-based workflow now capable of managing hundreds of thousands.  Data lakes and decoupling compute and storage in the modern era will also be discussed."},
{"speakers": [{"corp": "Nielsen", "name": "Matt VanLandeghem"}], "base_fname": "How_Nielsen_Utilized_Databricks_for_Large_Scale_Research_and_Development", "title": "How Nielsen Utilized Databricks for Large-Scale Research and Development", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/UslCPmGeFVM"}, "desc": "Large-scale testing of new data products or enhancements to existing products in a research and development environment can be a technical challenge for data scientists. In some cases, tools available to data scientists lack production-level capacity, whereas other tools do not provide the algorithms needed to run the methodology. At Nielsen, the Databricks platform provided a solution to both of these challenges. This breakout session will cover a specific Nielsen business case where two methodology enhancements were developed and tested at large-scale using the Databricks platform. Development and large-scale testing of these enhancements would not have been possible using standard database tools.Session hashtag: #EUent4"},
{"speakers": [{"corp": "Poznan University of Technology", "name": "Jakub Guner"}], "base_fname": "Lucid_A_Genetic_Programming_Library_for_Apache_Spark", "title": "Lucid\u2014A Genetic Programming Library for Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/4Nzs5w3GUO8"}, "desc": "Many popular ML models are inherently black-box, leaving little room for interpretation and explanation of predictions or the model itself. This becomes problematic if one needs to be well-informed about the analyzed process, control it (e.g. manufacturing), or communicate the findings to third-parties (healthcare, insurance). Lucid is an ongoing research project, funded by the Polish National Centre for Research and Development, aiming at delivering tools for inducing transparent models from data. The knowledge representations used in Lucid are  symbolic expressions and rules, which can be easily interpreted by humans and, at the same time, efficiently used for automatic classification or regression.The presentation will begin with a short introduction to explanatory modelling. The following section will explain how Genetic Programming can be applied to create transparent models and how Lucid implements this paradigm. The session will conclude with the summary of recent developments in AI and legal frameworks that call for the use of explanatory modelling.Session hashtag: #EUres4"},
{"speakers": [{"corp": "MapR", "name": "Rachel Silver"}], "base_fname": "Distilling_Actionable_Insights_From_Your_Data_with_MapR", "title": "Distilling Actionable Insights From Your Data with MapR", "slide": {"dl_link": "", "src_link": ""}, "tag": "Sponsored Sessions", "video": {"dl_link": "", "src_link": "https://youtu.be/HTcQ8qnpMrM"}, "desc": "Organizations start their data science journey in different places and with different needs in mind.Some are just starting to recognize the benefits of applying new algorithms to their historical data, while others are concerned with developing production pipelines."},
{"speakers": [{"corp": "Redis Labs", "name": "Dvir Volk"}], "base_fname": "Getting_Ready_to_Use_Redis_with_Apache_Spark", "title": "Getting Ready to Use Redis with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/-ZfrNyo7q1Q"}, "desc": "Getting Ready to use Redis with Apache Spark is a technical tutorial designed to address integrating Redis with an Apache Spark deployment to increase the performance of serving complex decision models. To set the context for the session, we start with a quick introduction to Redis and the capabilities Redis provides. We cover the basic data types provided by Redis and cover the module system. Using an ad serving use-case, we look at how Redis can improve the performance and reduce the cost of using complex ML-models in production. Attendees will be guided through the key steps of setting up and integrating Redis with Spark, including how to train a model using Spark then load and serve it using Redis, as well as how to work with the Spark Redis module. The capabilities of the Redis Machine Learning Module (redis-ml) will be discussed focusing primarily on decision trees and regression (linear and logistic) with code examples to demonstrate how to use these feature. At the end of the session, developers should feel confident building a prototype/proof-of-concept application using Redis and Spark. Attendees will understand how Redis complements Spark and how to use Redis to serve complex, ML-models with high performance.Session hashtag: #EUai4"},
{"speakers": [{"corp": "IBM", "name": "Holden Karau"}], "base_fname": "Testing_Apache_Spark_Avoiding_the_Fail_Boat_Beyond_RDDs", "title": "Testing Apache Spark\u2014Avoiding the Fail Boat Beyond RDDs", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/rPXmo99sOMo"}, "desc": "As Spark continues to evolve, we need to revisit our testing techniques to support Datasets, streaming, and more. This talk expands on \u201cBeyond Parallelize and Collect\u201d (not required to have been seen) to discuss how to create large scale test jobs while supporting Spark\u2019s latest features. We will explore the difficulties with testing Streaming Programs, options for setting up integration testing, beyond just local mode, with Spark, and also examine best practices for acceptance tests.Session hashtag: #EUeco4"},
{"speakers": [{"corp": "Toon", "name": "Stephen Galsworthy"}], "base_fname": "Saving_Energy_with_Apache_Spark_and_Toon", "title": "Saving Energy with Apache Spark and Toon", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/p8H_dEU6ZRg"}, "desc": "Toon, a leading European smart thermostat and energy display, enables users to control and monitor gas and electricity consumption in their homes. Using the energy data we collect from over 400,000 homes we have developed a new Energy Waste Checker app to give actionable advice to end users to ensure that they do not needlessly waste energy. We identify inefficient electrical appliances and suboptimal uses of central heating and hot water by applying disaggregation algorithms to the raw sensor data. In this talk Stephen will describe our how our novel disaggregation algorithms are implemented in Spark and will show how Toon uses cloud-based big data processing to offer data driven services to hundreds of thousands of users.Session hashtag: #EUds10"},
{"speakers": [{"corp": "Barclays Africa Group Limited", "name": "Jan Scherbaum"}, {"corp": "Barclays Africa Group Limited", "name": "Marek Novotny"}], "base_fname": "Spline_Apache_Spark_Lineage_Not_Only_for_the_Banking_Industry", "title": "Spline: Apache Spark Lineage, Not Only for the Banking Industry", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/T2vNPBCfA64"}, "desc": "Data lineage tracking is one of the significant problems that financial institutions face when using modern big data tools. This presentation describes Spline \u2013 a data lineage tracking and visualization tool for Apache Spark. Spline captures and stores lineage information from internal Spark execution plans and visualizes it in a user-friendly manner.Session hashtag: #EUent3"},
{"speakers": [{"corp": "Mellanox Technologies", "name": "Yuval Degani"}], "base_fname": "Accelerating_Shuffle_A_Tailor_Made_RDMA_Solution_for_Apache_Spark", "title": "Accelerating Shuffle: A Tailor-Made RDMA Solution for Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/PQz_6VDAHO4"}, "desc": "The opportunity in accelerating Spark by improving its network data transfer facilities has been under much debate in the last few years. RDMA (remote direct memory access) is a network acceleration technology that is very prominent in the HPC (high-performance computing) world, but has not yet made its way to mainstream Apache Spark. Proper implementation of RDMA in network-oriented applications can improve scalability, throughput, latency and CPU utilization. In this talk we are going to present a new RDMA solution for Apache Spark that shows amazing improvements in multiple Spark use cases. The solution is under development in our labs, and is going to be released to the public as an open-source plug-in.Session hashtag: #EUres3"},
{"speakers": [{"corp": "Intel", "name": "Jessica McCarthy"}, {"corp": "Intel", "name": "\u200bMarina B. Alekseeva"}], "base_fname": "Women_in_Big_Data_Lunch", "title": "Women in Big Data Lunch", "slide": {"dl_link": "", "src_link": ""}, "tag": "Sponsored Sessions", "video": {"dl_link": "", "src_link": "https://youtu.be/JF1Ogqy3psE"}, "desc": "WiBD Overview Presentation"},
{"speakers": [{"corp": "MemSQL", "name": "Gary Orenstein"}], "base_fname": "Real_Time_Image_Recognition_with_Apache_Spark", "title": "Real-Time Image Recognition with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/wn0LNNyFemk"}, "desc": "The future of computing is visual. With everything from smartphone to Spectacles, we are about to see more digital imagery and associated processing than ever before. In conjuction, new computing models are rapidly appearing to help data engineers harness the power of this imagery. Vast resources with cloud platforms, and the sharing of processing algorithms are moving the industry forward quickly. The models are readily available as well. We\u2019ll examine the image recognition techniques available with Apache Spark, and how to put those techniques into production. We will further explore algebraic operations on tensors and examine how that can assist in large scale, high-throughput, highly parallel image recognition. In particular, we\u2019ll showcase the use of Spark in conjunction with a high performance database to operationalize these workflows. This presentation will feature a combinations of _Architectural considerations in building and image recognition pipeline _Advantages and pitfalls of specific approaches _Real-time capabilities for instant matches _Use of a fast relational datastore to persist data from Spark This talk features a live demonstration of constructing and executing a real-time image recognition pipeline.Session hashtag: #EUai0"},
{"speakers": [{"corp": "DataStax", "name": "Artem Aliev"}, {"corp": "DataStax", "name": "Russell Spitzer"}], "base_fname": "A_Tale_of_Two_Graph_Frameworks_on_Spark_GraphFrames_and_Tinkerpop_OLAP", "title": "A Tale of Two Graph Frameworks on Spark: GraphFrames and Tinkerpop OLAP", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/DW09q18OHfc"}, "desc": "Graph is on the rise and it\u2019s time to start learning about scalable graph analytics! In this session we will go over two Spark-based Graph Analytics frameworks: Tinkerpop and GraphFrames. While both frameworks can express very similar traversals, they have different performance characteristics and APIs. In this Deep-Dive by example presentation, we will demonstrate some common traversals and explain how, at a Spark level, each traversal is actually computed under the hood! Learn both the fluent Gremlin API as well as the powerful GraphFrame Motif api as we show examples of both simultaneously. No need to be familiar with Graphs or Spark for this presentation as we\u2019ll be explaining everything from the ground up!Session hashtag: #EUeco3"},
{"speakers": [{"corp": "Databricks", "name": "Jules Damji"}], "base_fname": "A_Tale_of_Three_Apache_Spark_APIs_RDDs_DataFrames_and_Datasets", "title": "A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/Ofk7G3GD9jk"}, "desc": "Of all the developers\u2019 delight, none is more attractive than a set of APIs that make developers productive, that are easy to use, and that are intuitive and expressive. Apache Spark offers these APIs across components such as Spark SQL, Streaming, Machine Learning, and Graph Processing to operate on large data sets in languages such as Scala, Java, Python, and R for doing distributed big data processing at scale. In this talk, I will explore the evolution of three sets of APIs-RDDs, DataFrames, and Datasets-available in Apache Spark 2.x. In particular, I will emphasize three takeaways: 1) why and when you should use each set as best practices 2) outline its performance and optimization benefits; and 3) underscore scenarios when to use DataFrames and Datasets instead of RDDs for your big data distributed processing. Through simple notebook demonstrations with API code examples, you\u2019ll learn how to process big data using RDDs, DataFrames, and Datasets and interoperate among them. (this will be vocalization of the blog, along with the latest developments in Apache Spark 2.x Dataframe/Datasets and Spark SQL APIs: https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)Session hashtag: #EUdev12"},
{"speakers": [{"corp": "IBM", "name": "Guy Gerson"}, {"corp": "IBM", "name": "Paula Ta-Shma"}], "base_fname": "Using_Pluggable_Apache_Spark_SQL_Filters_to_Help_GridPocket_Users_Keep_Up_with_the_Jones_and_save_the_planet", "title": "Using Pluggable Apache Spark SQL Filters to Help GridPocket Users Keep Up with the Jones' (and save the planet)", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/tifWDf84H0Y"}, "desc": "Analyzing and comparing your energy consumption with that of other consumers provides healthy peer pressure and useful insight leading to energy conservation and impacting the bottom line. We helped GridPocket (http://www.gridpocket.com/), a smart grid company developing energy management applications for electricity water and gas utilities, implement high scale anonymized energy comparison queries with an order of magnitude lower cost and higher performance than was previously possible. IoT use cases like that of GridPocket are swamping our planet with data, and drive demand for analytics on extremely scalable and low cost storage. Enter Spark SQL over Object Storage: highly scalable and low cost storage which provides RESTful APIs to store and retrieve objects and their metadata. Key performance indicators (KPIs) of query performance and cost are the number of bytes shipped from Object Storage to Spark and the number of incurred REST requests. We propose Pluggable Spark SQL Filters, which extend the existing Spark SQL partitioning mechanism with an ability to dynamically filter irrelevant objects during query execution. Our approach handles any data format supported by Spark SQL (Parquet, JSON, csv etc.), and unlike pushdown compatible formats such as Parquet which require touching each object to determine its relevance, it avoids accessing irrelevant objects altogether. We developed a pluggable interface for developing and deploying Filters, and implemented GridPocket\u2019s filter which screens objects according to their metadata, for example geo-spatial bounding boxes which describe the area covered by an object\u2019s data points. This leads to drastically lower KPIs since there is no need to ship the entire dataset from Object Storage to Spark if you are only comparing yourself with your neighborhood. We demonstrate GridPocket analytics notebooks, report on our implementation and resulting 10-20x speedups, explain how to implement a Pluggable File Filter, and how we applied this to other use cases.Session hashtag: #EUres2"},
{"speakers": [{"corp": "IBM", "name": "Luis Arellano"}], "base_fname": "Powering_a_Data_Science_Platform_Using_Apache_Spark", "title": "Powering a Data Science Platform Using Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Sponsored Sessions", "video": {"dl_link": "", "src_link": "https://youtu.be/CX7iUdrA1yw"}, "desc": "Business transformation requires an architecture that can democratize data and make data science simpler across the organization. Attend this session and learn about IBM\u2019s approach to providing a seamless user experience for data science built on Apache Spark."},
{"speakers": [{"corp": "Elsevier", "name": "Reza Karimi"}], "base_fname": "Deduplication_and_Author_Disambiguation_of_Streaming_Records_via_Supervised_Models_Based_on_Content_Encoders", "title": "Deduplication and Author-Disambiguation of Streaming Records via Supervised Models Based on Content Encoders", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/so4rkI3aXUY"}, "desc": "Here we present a general supervised framework for record deduplication and author-disambiguation via Spark. This work differentiates itself by \u2013 Application of Databricks and AWS makes this a scalable implementation. Compute resources are comparably lower than traditional legacy technology using big boxes 24/7. Scalability is crucial as Elsevier\u2019s Scopus data, the biggest scientific abstract repository, covers roughly 250 million authorships from 70 million abstracts covering a few hundred years. \u2013 We create a fingerprint for each content by deep learning and/or word2vec algorithms to expedite pairwise similarity calculation. These encoders substantially reduce compute time while maintaining semantic similarity (unlike traditional TFIDF or predefined taxonomies). We will briefly discuss how to optimize word2vec training with high parallelization. Moreover, we show how these encoders can be used to derive a standard representation for all our entities namely such as documents, authors, users, journals, etc. This standard representation can simplify the recommendation problem into a pairwise similarity search and hence it can offer a basic recommender for cross-product applications where we may not have a dedicate recommender engine designed. \u2013 Traditional author-disambiguation or record deduplication algorithms are batch-processing with small to no training data. However, we have roughly 25 million authorships that are manually curated or corrected upon user feedback. Hence, it is crucial to maintain historical profiles and hence we have developed a machine learning implementation to deal with data streams and process them in mini batches or one document at a time. We will discuss how to measure the accuracy of such a system, how to tune it and how to process the raw data of pairwise similarity function into final clusters. Lessons learned from this talk can help all sort of companies where they want to integrate their data or deduplicate their user/customer/product databases.Session hashtag: #EUai2"},
{"speakers": [{"corp": "Alluxio, Inc.", "name": "Gene Pang"}], "base_fname": "Best_Practices_for_Using_Alluxio_with_Apache_Spark", "title": "Best Practices for Using Alluxio with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/vdTJPMGPi9Q"}, "desc": "Alluxio, formerly Tachyon, is a memory speed virtual distributed storage system and leverages memory for storing data and accelerating access to data in different storage systems. Many organizations and deployments use Alluxio with Apache Spark, and some of them scale out to over PB\u2019s of data. Alluxio can enable Spark to be even more effective, in both on-premise deployments and public cloud deployments. Alluxio bridges Spark applications with various storage systems and further accelerates data intensive applications. In this talk, we briefly introduce Alluxio, and present different ways how Alluxio can help Spark jobs. We discuss best practices of using Alluxio with Spark, including RDDs and DataFrames, as well as on-premise deployments and public cloud deployments.Session hashtag: #EUeco2"},
{"speakers": [{"corp": "Red Hat", "name": "Michael McCune"}], "base_fname": "Fire_in_the_Sky_An_Introduction_to_Monitoring_Apache_Spark_in_the_Cloud", "title": "Fire in the Sky: An Introduction to Monitoring Apache Spark in the Cloud", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/S-1qbcGYPL0"}, "desc": "Writing intelligent cloud native applications is hard enough when things go well, but what happens when there are performance and debugging issues that arise during production? Inspecting the logs is a good start, but what if the logs don\u2019t show the whole picture? Now you have to go deeper, examining the live performance metrics that are generated by Spark, or even deploying specialized microservices to monitor and act upon that data. Spark provides several built-in sinks for exposing metrics data about the internal state of its executors and drivers, but getting at that information when your cluster is in the cloud can be a time consuming and arduous process. In this presentation, Michael McCune will walk through the options available for gaining access to the metrics data even when a Spark cluster lives in a cloud native containerized environment. Attendees will see demonstrations of techniques that will help them to integrate a full-fledged metrics story into their deployments. Michael will also discuss the pain points and challenges around publishing this data outside of the cloud and explain how to overcome them. In this talk you will learn about: Deploying metrics sinks as microservices, Common configuration options, and Accessing metrics data through a variety of mechanisms.Session hashtag: #EUdev11"},
{"speakers": [{"corp": "Swoop", "name": "Sim Simeonov"}], "base_fname": "Goal_Based_Data_Production_The_Spark_of_a_Revolution", "title": "Goal-Based Data Production: The Spark of a Revolution", "slide": {"dl_link": "", "src_link": ""}, "tag": "Enterprise", "video": {"dl_link": "", "src_link": "https://youtu.be/VR2lAMVD4_4"}, "desc": "Since the invention of SQL and relational databases, data production has been about specifying how data is transformed through queries. While Apache Spark can certainly be used as a general distributed query engine, the power and granularity of Spark\u2019s APIs enables a revolutionary increase in data engineering productivity: goal-based data production. Goal-based data production concerns itself with specifying WHAT the desired result is, leaving the details of HOW the result is achieved to a smart data warehouse running on top of Spark. That not only substantially increases productivity, but also significantly expands the audience that can work directly with Spark: from developers and data scientists to technical business users. With specific data and architecture patterns spanning the range from ETL to machine learning data prep and with live demos, this session will demonstrate how Spark users can gain the benefits of goal-based data production.Session hashtag: #EUent1"},
{"speakers": [{"corp": "Intel", "name": "Qi Xie"}, {"corp": "Intel", "name": "Quanfu Wang"}], "base_fname": "FPGA_Based_Acceleration_Architecture_for_Spark_SQL", "title": "FPGA-Based Acceleration Architecture for Spark SQL", "slide": {"dl_link": "", "src_link": ""}, "tag": "Research", "video": {"dl_link": "", "src_link": "https://youtu.be/fTLsNc6fA3o"}, "desc": "In this session we will present a Configurable FPGA-Based Spark SQL Acceleration Architecture. It is target to leverage FPGA highly parallel computing capability to accelerate Spark SQL Query and for FPGA\u2019s higher power efficiency than CPU we can lower the power consumption at the same time. The Architecture consists of SQL query decomposition algorithms, fine-grained FPGA based Engine Units which perform basic computation of sub string, arithmetic and logic operations. Using SQL query decomposition algorithm, we are able to decompose a complex SQL query into basic operations and according to their patterns each is fed into an Engine Unit. SQL Engine Units are highly configurable and can be chained together to perform complex Spark SQL queries, finally one SQL query is transformed into a Hardware Pipeline. We will present the performance benchmark results comparing the queries with FGPA-Based Spark SQL Acceleration Architecture on XEON E5 and FPGA to the ones with Spark SQL Query on XEON E5 with 10X ~ 100X improvement and we will demonstrate one SQL query workload from a real customer.Session hashtag: #EUres0"},
{"speakers": [{"corp": "Cloudera", "name": "Marton Balassi"}], "base_fname": "Improving_Computer_Vision_Models_at_Scale", "title": "Improving Computer Vision Models at Scale", "slide": {"dl_link": "", "src_link": ""}, "tag": "Sponsored Sessions", "video": {"dl_link": "", "src_link": "https://youtu.be/qKJRggNK0zY"}, "desc": "Rigorous improvement of an image recognition model often requires multiple iterations of eyeballing outliers, inspecting statistics of the output labels then modifying and retraining the model. When testing data is present at the PetaByte scale the ability to seamlessly access all the images that have been assigned specific labels poses a technical challenge by itself."},
{"speakers": [{"corp": "Hortonworks", "name": "Mingjie Tang"}, {"corp": "Hortonworks", "name": "Yanbo Liang"}], "base_fname": "MatFast_In_Memory_Distributed_Matrix_Computation_Processing_and_Optimization_Based_on_Spark_SQL", "title": "MatFast: In-Memory Distributed Matrix Computation Processing and Optimization Based on Spark SQL", "slide": {"dl_link": "", "src_link": ""}, "tag": "AI", "video": {"dl_link": "", "src_link": "https://youtu.be/Ce83qamOq1E"}, "desc": "The use of large-scale machine learning and data mining methods is becoming ubiquitous in many application domains ranging from business intelligence and bioinformatics to self-driving cars. These methods heavily rely on matrix computations, and it is hence critical to make these computations scalable and efficient. These matrix computations are often complex and involve multiple steps that need to be optimized and sequenced properly for efficient execution. This work presents new efficient and scalable matrix processing and optimization techniques based on Spark. The proposed techniques estimate the sparsity of intermediate matrix-computation results and optimize communication costs. An evaluation plan generator for complex matrix computations is introduced as well as a distributed plan optimizer that exploits dynamic cost-based analysis and rule-based heuristics The result of a matrix operation will often serve as an input to another matrix operation, thus defining the matrix data dependencies within a matrix program. The matrix query plan generator produces query execution plans that minimize memory usage and communication overhead by partitioning the matrix based on the data dependencies in the execution plan. We implemented the proposed matrix techniques inside the Spark SQL, and optimize the matrix execution plan based on Spark SQL Catalyst. We conduct case studies on a series of ML models and matrix computations with special features on different datasets. These are PageRank, GNMF, BFGS, sparse matrix chain multiplications, and a biological data analysis. The open-source library ScaLAPACK and the array-based database SciDB are used for performance evaluation. Our experiments are performed on six real-world datasets are: social network data ( e.g., soc-pokec, cit-Patents, LiveJournal), Twitter2010, Netflix recommendation data, and 1000 Genomes Project sample. Experiments demonstrate that our proposed techniques achieve up to an order-of-magnitude performance.Session hashtag: #EUai1"},
{"speakers": [{"corp": "Mesosphere", "name": "Jorg Schad"}], "base_fname": "Smack_Stack_and_Beyond_Building_Fast_Data_Pipelines", "title": "Smack Stack and Beyond\u2014Building Fast Data Pipelines", "slide": {"dl_link": "", "src_link": ""}, "tag": "Spark Ecosystem", "video": {"dl_link": "", "src_link": "https://youtu.be/P8ziya5v0us"}, "desc": "There are an ever increasing number of use cases, like online fraud detection, for which the response times of traditional batch processing are too slow. In order to be able to react to such events in close to real-time, you need to go beyond classical batch processing and utilize stream processing systems such as Apache Spark Streaming, Apache Flink, or Apache Storm. These systems, however, are not sufficient on their own. For an efficient and fault-tolerant setup, you also need a message queue and storage system. One common example for setting up a fast data pipeline is the SMACK stack. SMACK stands for Spark (Streaming) \u2013 the stream processing system Mesos \u2013 the cluster orchestrator Akka \u2013 the system for providing custom actors for reacting upon the analyses Cassandra \u2013 the storage system Kafka \u2013 the message queue Setting up this kind of pipeline in a scalable, efficient and fault-tolerant manner is not trivial. First, this workshop will discuss the different components in the SMACK stack. Then, participants will get hands-on experience in setting up and maintaining data pipelines.Session hashtag: #EUeco1"},
{"speakers": [{"corp": "Cloudera", "name": "Mladen Kovacevic"}], "base_fname": "Storage_Engine_Considerations_for_Your_Apache_Spark_Applications", "title": "Storage Engine Considerations for Your Apache Spark Applications", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/yUtZomrwduI"}, "desc": "You have the perfect use case for your Spark applications \u2013 whether it be batch processing or super fast near-real time streaming \u2014 Now, where to store your valuable data!? In this talk we take a look at four storage options; HDFS, HBase, Solr and Kudu. With so many to choose from, which will fit your use case? What considerations should be taken into account? What are the pros and cons, what are the similarities and differences and how do they fit in with your Spark application? Learn the answers to these questions and more with a look at design patterns and techniques, and sample code to integrate into your application immediately. Walk away with the confidence to propose the right architecture for your use cases and the development know-how to implement and deliver with success.Session hashtag: #EUdev10"},
{"speakers": [{"corp": "", "name": "Jer Thorp"}], "base_fname": "Living_in_Data", "title": "Living in Data", "slide": {"dl_link": "", "src_link": ""}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/2CbHchR8yxQ"}, "desc": "What is it like to live in data? To be a human in a world where every action is instrumented, where our jobs and our families and our love lives are increasingly quantified? In this session, Jer Thorp, National Geographic Explorer and former data artist in residence at The New York Times, will share a series of projects by The Office for Creative Research which examine the fast-changing relationship between data and culture. From visualizations to performances and public art, Jer will explore ways in which we can bring data closer to humans, and propose ways that we can all work to make a more livable data world."},
{"speakers": [{"corp": "Salesforce.com", "name": "Leah McGuire"}], "base_fname": "Low_Touch_Machine_Learning", "title": "Low Touch Machine Learning", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/SparkSummit/low-touch-machine-learning-with-leah-mcguire-salesforce"}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/PKTvo9X9Sjg"}, "desc": "Leveraging your data to make better decisions is something every business wants to do, but doing it correctly involves weeks or months of work from highly skilled, hard to find individuals. However, many of the laborious steps a machine learning specialist follows in creating a custom application can be automated with enough compute power and flexibility. By leveraging Spark to do machine learning at scale, Salesforce Einstein has created a system that lets individuals with domain knowledge, but no machine learning expertise, create high quality, high impact machine learning applications."},
{"speakers": [{"corp": "Nielsen", "name": "Jarrett Garcia"}], "base_fname": "Scaling_Your_Skillset_with_Your_Data", "title": "Scaling Your Skillset with Your Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/SparkSummit/scaling-your-skillset-with-your-data-with-jarrett-garcia-nielsen"}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/a9F6fi35aag"}, "desc": "More data is being created than ever before and the rate at which it is being created is not slowing. This means that new techniques are being developed and used to tackle big data challenges. At Nielsen, big data means opportunities for new products like digital measurement, but also exposed a skills gap for dealing with big data. This session will cover Nielsen\u2019s successful implementation of Spark and Databricks which has allowed Nielsen to scale its products and its Data Scientists\u2019 skillsets."},
{"speakers": [{"corp": "CERN", "name": "Jakub Wozniak"}], "base_fname": "The_Next_CERN_Accelerator_Logging_Service_A_Road_to_Big_Data", "title": "The Next CERN Accelerator Logging Service\u2014A Road to Big Data", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/SparkSummit/the-next-cern-accelerator-logging-servicea-road-to-big-data-with-jakub-wozniak-cern"}, "tag": "keynote", "video": {"dl_link": "", "src_link": "https://youtu.be/c5Xz5TCGAAE"}, "desc": "The LHC performance has reached new heights, in-turn the amount of data produced by the underlying infrastructure and observation systems has increased by an order of magnitude putting unprecedented load on the current data Logging Service. In this talk Jakub will introduce you to the core business of beam production at CERN and the motivations behind the evolution of the current CERN Accelerator Logging Service towards Hadoop and Apache Spark."},
{"speakers": [{"corp": "Zalando SE", "name": "Lorand Dali"}], "base_fname": "Lessons_Learned_while_Implementing_a_Sparse_Logistic_Regression_Algorithm_in_Apache_Spark", "title": "Lessons Learned while Implementing a Sparse Logistic Regression Algorithm in Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/a_LN_lasFm0"}, "desc": "This talk tells the story of implementation and optimization of a sparse logistic regression algorithm in spark. I would like to share the lessons I learned and the steps I had to take to improve the speed of execution and convergence of my initial naive implementation. The message isn\u2019t to convince the audience that logistic regression is great and my implementation is awesome, rather it will give details about how it works under the hood, and general tips for implementing an iterative parallel machine learning algorithm in spark. The talk is structured as a sequence of \u201clessons learned\u201d that are shown in form of code examples building on the initial naive implementation. The performance impact of each \u201clesson\u201d on execution time and speed of convergence is measured on benchmark datasets. You will see how to formulate logistic regression in a parallel setting, how to avoid data shuffles, when to use a custom partitioner, how to use the \u2018aggregate\u2019 and \u2018treeAggregate\u2019 functions, how momentum can accelerate the convergence of gradient descent, and much more. I will assume basic understanding of machine learning and some prior knowledge of spark. The code examples are written in scala, and the code will be made available for each step in the walkthrough.Session hashtag: #EUds9"},
{"speakers": [{"corp": "LinkedIn", "name": "Akshay Rai"}], "base_fname": "Dr_Elephant_Achieving_Quicker_Easier_and_Cost_Effective_Big_Data_Analytics", "title": "Dr. Elephant: Achieving Quicker, Easier, and Cost-Effective Big Data Analytics", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/Aj_BtBBGL30"}, "desc": "Is your job running slower than usual? Do you want to make sense from the thousands of Hadoop & Spark metrics? Do you want to monitor the performance of your flow, get alerts and auto tune them? These are the common questions every Hadoop user asks but there is not a single solution that addresses it. We at Linkedin faced lots of such issues and have built a simple self serve tool for the hadoop users called Dr. Elephant. Dr. Elephant, which is already open sourced, is a performance monitoring and tuning tool for Hadoop and Spark. It tries to improve the developer productivity and cluster efficiency by making it easier to tune jobs. Since its open source, it has been adopted by multiple organizations and followed with a lot of interest in the Hadoop and Spark community. In this talk, we will discuss about Dr. Elephant and outline our efforts to expand the scope of Dr. Elephant to be a comprehensive monitoring, debugging and tuning tool for Hadoop and Spark applications. We will talk about how Dr. Elephant performs exception analysis, give clear and specific suggestions on tuning, tracking metrics and monitoring their historical trends. Open Source: https://github.com/linkedin/dr-elephantSession hashtag: #EUdev9"},
{"speakers": [{"corp": "CERN", "name": "Daniel Lanza"}, {"corp": "CERN", "name": "Prasanth Kothuri"}], "base_fname": "Real_Time_Detection_of_Anomalies_in_the_Database_Infrastructure_using_Apache_Spark", "title": "Real-Time Detection of Anomalies in the Database Infrastructure using Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/1IsMMmug5q0"}, "desc": "At CERN, the biggest physics laboratory in the world, large volumes of data are generated every hour, it implies serious challenges to store and process all this data. An important part of this responsibility comes to the database group which not only provides services for RDBMS but also scalable systems as Hadoop, Spark and HBase. Since databases are critical, they need to be monitored, for that we have built a highly scalable, secure and central repository that stores consolidated audit data and listener, alert and OS log events generated by the databases. This central platform is used for reporting, alerting and security policy management. The database group want to further exploit the information available in this central repository to build intrusion detection system to enhance the security of the database infrastructure. In addition, build pattern detection models to flush out anomalies using the monitoring and performance metrics available in the central repository. Finally, this platform also helps us for capacity planning of the database deployment. The audience would get first-hand experience of how to build real time Apache Spark application that is deployed in production. They would hear the challenges faced and decisions taken while developing the application and troubleshooting Apache Spark and Spark streaming application in production.Session hashtag: #EUde13"},
{"speakers": [{"corp": "", "name": "Jacek Laskowski"}], "base_fname": "From_Basic_to_Advanced_Aggregate_Operators_in_Apache_Spark_SQL_2_2_by_Examples_and_their_Catalyst_Optimizations_continues", "title": "From Basic to Advanced Aggregate Operators in Apache Spark SQL 2.2 by Examples and their Catalyst Optimizations - continues", "slide": {"dl_link": "", "src_link": ""}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/tUkynBZewHQ"}, "desc": "There are many different aggregate operators in Spark SQL. They range from the very basic groupBy and not so basic groupByKey that shines bright in Apache Spark Structured Streaming\u2019s stateful aggregations, including the more advanced cube, rollup and pivot to my beloved windowed aggregations. It\u2019s unbelievable how different the performance characteristic they have, even for the same use cases.What is particularly interesting is the comparison of the simplicity and performance of windowed aggregations vs groupBy. And that\u2019s just Spark SQL alone. Then there is Spark Structured Streaming that has put groupByKey operator at the forefront of stateful stream processing (and to my surprise as the performance might not be that satisfactory).This deep-dive talk is going to show all the different use cases for the aggregate operators and functions as well as their performance differences in Spark SQL 2.2 and beyond. Code and fun included!Session hashtag: #EUdd5"},
{"speakers": [{"corp": "Shell Research Ltd.", "name": "Daniel Jeavons"}, {"corp": "Shell", "name": "Wayne Jones"}], "base_fname": "Parallelizing_Large_Simulations_with_Apache_SparkR", "title": "Parallelizing Large Simulations with Apache SparkR", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/YWybY1wBtDo"}, "desc": "Across all assets globally, Shell carries a huge stock of spare part inventory which ties up large quantities of working capital. Over the past 2 years an interdisciplinary project team has produced a tool, Inventory Optimization Analytics solution (IOTA), based on advanced analytical methods, that helps assets optimise stock levels and purchase strategies. To calculate the recommended stocking inventory level requirement for a material the Data Science team have written a Markov Chain Monte Carlo (MCMC) bootstrapping statistical model in R. Cumulatively, the computational task is large but, fortunately, is one of an embarrassingly parallel nature because the model can be applied independently to each material. The original solution which utilised the R \u201cparallel\u201d package was deployed on a single 48 core PC and took 48 hours to run. In this presentation, we describe how we moved the original solution to a distributed cloud-based Apache Spark framework. Using the new R User Defined Functions API in Apache Spark and with only a minimal amount of code changes the computational run time was reduced to 4 hours. A restructuring of the architecture to \u201cpipeline\u201d the problem resulted in a run time of less than 1 hour. This use case is important because it verifies the scalability and performance of SparkR.Session hashtag: #EUds8"},
{"speakers": [{"corp": "IBM", "name": "Brad Kaiser"}, {"corp": "IBM", "name": "Craig Ingram"}], "base_fname": "Supporting_Highly_Multitenant_Spark_Notebook_Workloads_Best_Practices_and_Useful_Patches", "title": "Supporting Highly Multitenant Spark Notebook Workloads: Best Practices and Useful Patches", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/D9m_hMUvoVc"}, "desc": "Notebooks: they enable our users, but they can cripple our clusters. Let\u2019s fix that. Notebooks have soared in popularity at companies world-wide because they provide an easy, user-friendly way of accessing the cluster-computing power of Spark. But the more users you have hitting a cluster, the harder it is to manage the cluster resources as big, long-running jobs start to starve out small, short-running jobs. While you could have users spin up EMR-style clusters, this reduces the ability to take advantage of the collaborative nature of notebooks. It also quickly becomes expensive as clusters sit idle for long periods of time waiting on single users. What we want is fair, efficient resource utilization on a large single cluster for a large number of users. In this talk we\u2019ll discuss dynamic allocation and the best practices for configuring the current version of Spark as-is to help solve this problem. We\u2019ll also present new improvements we\u2019ve made to address this use case. These include: decommissioning executors without losing cached data, proactively shutting down executors to prevent starvation, and improving the start times of new executors.Session hashtag: #EUdev8"},
{"speakers": [{"corp": "LinkedIn", "name": "Anant Nag"}, {"corp": "LinkedIn", "name": "Shankar Manian"}], "base_fname": "Beyond_Unit_Tests_End_to_End_Testing_for_Spark_Workflows", "title": "Beyond Unit Tests: End-to-End Testing for Spark Workflows", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/L8lANUOa2g0"}, "desc": "As a Spark developer, do you want to quickly develop your Spark workflows? Do you want to test your workflows in a sandboxed environment similar to production? Do you want to write end-to-end tests for your workflows and add assertions on top of it? In just a few years, the number of users writing Spark jobs at LinkedIn have grown from tens to hundreds, and the number of jobs running every day has grown from hundreds to thousands. With the ever increasing number of users and jobs, it becomes crucial to reduce the development time for these jobs. It is also important to test these jobs thoroughly before they go to production. Currently, there is no way users can test their spark jobs end-to-end. The only way is to divide the spark jobs into functions and unit test the functions. We\u2019ve tried to address these issues by creating a testing framework for Spark workflows. The testing framework enables the users to run their jobs in an environment similar to the production environment and on the data which is sampled from the original data. The testing framework consists of a test deployment system, a data generation pipeline to generate the sampled data, a data management system to help users manage and search the sampled data and an assertion engine to validate the test output. In this talk, we will discuss the motivation behind the testing framework before deep diving into its design. We will further discuss how the testing framework is helping the Spark users at LinkedIn to be more productive.Session hashtag: #EUde12"},
{"speakers": [{"corp": "", "name": "Jacek Laskowski"}], "base_fname": "From_Basic_to_Advanced_Aggregate_Operators_in_Apache_Spark_SQL_2_2_by_Examples_and_their_Catalyst_Optimizations", "title": "From Basic to Advanced Aggregate Operators in Apache Spark SQL 2.2 by Examples and their Catalyst Optimizations", "slide": {"dl_link": "", "src_link": ""}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/fk-5LeH_tjw"}, "desc": "There are many different aggregate operators in Spark SQL. They range from the very basic groupBy and not so basic groupByKey that shines bright in Apache Spark Structured Streaming\u2019s stateful aggregations, including the more advanced cube, rollup and pivot to my beloved windowed aggregations. It\u2019s unbelievable how different the performance characteristic they have, even for the same use cases.What is particularly interesting is the comparison of the simplicity and performance of windowed aggregations vs groupBy. And that\u2019s just Spark SQL alone. Then there is Spark Structured Streaming that has put groupByKey operator at the forefront of stateful stream processing (and to my surprise as the performance might not be that satisfactory).This deep-dive talk is going to show all the different use cases for the aggregate operators and functions as well as their performance differences in Spark SQL 2.2 and beyond. Code and fun included!Session hashtag: #EUdd5"},
{"speakers": [{"corp": "BBVA Data & Analytics", "name": "Jose A. Rodriguez-Serrano"}], "base_fname": "Classifying_Text_in_Money_Transfers_A_Use_Case_of_Apache_Spark_in_Production_for_Banking", "title": "Classifying Text in Money Transfers: A Use Case of Apache Spark in Production for Banking", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/YWybY1wBtDo"}, "desc": "At BBVA (second biggest bank in Spain), every money transfer a customer makes goes through an engine that infers a category from its textual description. This engine has been developed in Spark, mixes MLLib and own implementations, and is currently into production serving more than 5M customers daily. In our proposed presentation, we plan to describe the process undergone by the Data Science team. This includes the problem (classify 700K daily transfers by its text), the data science challenges, the algorithmic and engineering solution, and the achievements and learnings. To make the talk practical and focused, we will walk through our best performing pipeline, which uses (i) word2vec embeddings to represent words, (ii) a pooling algorithm to aggregate them into sentence embeddings, (iii) a supervised classifier. We believe this is relevant because it mixes (i) off-the-shelf MLLib funcions, such as the word2vec embeddings, (ii) components that build on Mllib and were adapted (e.g. a calibrated multic-class logistic regression which outputs probabilities) and (iii) an own implementations of the pooling algorithm, which is known as the Vector of Locally Aggregated Descriptors (VLAD). We highlight that we are not aware of the previous application of word2vec with VLAD in NLP, so this would introduce a novelty. The relevant audience are data scientists, engineers, team leaders and executives interested in Spark and seeking examples of machine learning deployments in a real-world productive environment. The main takeaways will be: (i) it is relatively simple to build a text classification system in Spark, (ii) with extra effort one it is feasible to build state-of-the-art, productive solutions. We will mention the problems we found in practice, such as how to design a training corpus to maximize precision, not recall, or how we designed the system against \u201ccatastrophic\u201d classification mistakes.Session hashtag: #EUds7"},
{"speakers": [{"corp": "IBM", "name": "Ioana Delaney"}, {"corp": "IBM", "name": "Jia Li"}], "base_fname": "Extending_Apache_Spark_SQL_Data_Source_APIs_with_Join_Push_Down", "title": "Extending Apache Spark SQL Data Source APIs with Join Push Down", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/MDWgPK6XfEo"}, "desc": "When Spark applications operate on distributed data coming from disparate data sources, they often have to directly query data sources external to Spark such as backing relational databases, or data warehouses. For that, Spark provides Data Source APIs, which are a pluggable mechanism for accessing structured data through Spark SQL. Data Source APIs are tightly integrated with the Spark Optimizer. They provide optimizations such as filter push down to the external data source and column pruning. While these optimizations significantly speed up Spark query execution, depending on the data source, they only provide a subset of the functionality that can be pushed down and executed at the data source. As part of our ongoing project to provide a generic data source push down API, this presentation will show our work related to join push down. An example is star-schema join, which can be simply viewed as filters applied to the fact table. Today, Spark Optimizer recognizes star-schema joins based on heuristics and executes star-joins using efficient left-deep trees. An alternative execution proposed by this work is to push down the star-join to the external data source in order to take advantage of multi-column indexes defined on the fact tables, and other star-join optimization techniques implemented by the relational data source.Session hashtag: #EUdev7"},
{"speakers": [{"corp": "Barcelona Super Computing", "name": "Nicolas Poggi"}], "base_fname": "The_State_of_Apache_Spark_in_the_Cloud", "title": "The State of Apache Spark in the Cloud", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/3uq8IiaV7fM"}, "desc": "Cloud providers currently offer convenient on-demand managed big data clusters (PaaS) with a pay-as-you-go model. In PaaS, analytical engines such as Spark and Hive come ready to use, with a general-purpose configuration and upgrade management. Over the last year, the Spark framework and APIs have been evolving very rapidly, with major improvements on performance and the release of v2, making it challenging to keep up-to-date production services both on-premises and in the cloud for compatibility and stability. Nicolas Poggi evaluates the out-of-the-box support for Spark and compares the offerings, reliability, scalability, and price-performance from major PaaS providers, including Azure HDinsight, Amazon Web Services EMR, Google Dataproc with an on-premises commodity cluster as baseline. Nicolas uses BigBench, the brand new standard (TPCx-BB) for big data systems, with both Spark and Hive implementations for benchmarking the systems. BigBench combines SQL queries, MapReduce, user code (UDF), and machine learning, which makes it ideal to stress Spark libraries (SparkSQL, DataFrames, MLlib, etc.). The work is framed within the ALOJA research project, which features an open source benchmarking and analysis platform that has been recently extended to support SQL-on-Hadoop engines and BigBench. The ALOJA project aims to lower the total cost of ownership (TCO) of big data deployments and study their performance characteristics for optimization. Nicolas highlights how to easily repeat the benchmarks through ALOJA and benefit from BigBench to optimize your Spark cluster for advanced users. The work is a continuation of a paper to be published at the IEEE Big Data 16 conference.Session hashtag: #EUde6"},
{"speakers": [{"corp": "Indeed", "name": "Alexander Thomas"}], "base_fname": "Natural_Language_Understanding_at_Scale_with_Spark_Native_NLP_Spark_ML_and_TensorFlow", "title": "Natural Language Understanding at Scale with Spark-Native NLP, Spark ML, and TensorFlow", "slide": {"dl_link": "", "src_link": ""}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/k5X12mdEvb8"}, "desc": "Natural language processing is a key component in many data science systems that must understand or reason about text. Common use cases include question answering, paraphrasing or summarization, sentiment analysis, natural language BI, language modeling, and disambiguation. Building such systems usually requires combining three types of software libraries: NLP annotation frameworks, machine learning frameworks, and deep learning frameworks. Ideally, all three of these pieces should be able to be integrated into a single workflow. This makes development, experimentation, and deploying results much easier. Spark\u2019s MLlib provides a number of machine learning algorithms, and now there are also projects making deep learning achievable in MLlib pipelines. All we need is the NLP annotation frameworks. SparkNLP adds NLP annotations into the MLlib ecosystem. This talk will introduce SparkNLP: how to use it, its current functionality, and where it is going in the future.Session hashtag: #EUdd4"},
{"speakers": [{"corp": "Databricks", "name": "Tim Hunter"}], "base_fname": "GraphFrames_Scaling_Web_Scale_Graph_Analytics_with_Apache_Spark", "title": "GraphFrames: Scaling Web-Scale Graph Analytics with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/NmbKst7ny5Q"}, "desc": "Graph analytics has a wide range of applications, from information propagation and network flow optimization to fraud and anomaly detection. The rise of social networks and the Internet of Things has given us complex web-scale graphs with billions of vertices and edges. However, in order to extract the hidden gems of understanding and information within those graphs, you need tools to analyze the graphs easily and efficiently.At Spark Summit 2016, Databricks introduced GraphFrames, which implements graph queries and pattern matching on top of Spark SQL to simplify graph analytics. In this talk, we\u2019ll discuss the work that has made graph algorithms in GraphFrames faster and more scalable. For example, new implementations of connected components have received algorithm improvements based on recent research, as well as performance improvements from Spark DataFrames. Discover lessons learned from scaling the implementation from millions to billions of nodes; see its performance in the context of other popular graph libraries; and hear about real-world applications.Session hashtag: #EUds6"},
{"speakers": [{"corp": "Oplo", "name": "Jean Georges Perrin"}], "base_fname": "Extending_Apache_Spark_s_Ingestion_Building_Your_Own_Java_Data_Source", "title": "Extending Apache Spark's Ingestion: Building Your Own Java Data Source", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/M6NdFsKJ7os"}, "desc": "Apache Spark is a wonderful platform for running your analytics jobs. It has great ingestion features from CSV, Hive, JDBC, etc. however, you may have your own data sources or formats you want to use. Your solution could be to convert your data in a CSV or JSON file and then ask Spark to do ingest it through its built-in tools. However, for enhanced performance, we will explore the way to build a data source, in Java, to extend Spark\u2019s ingestion capabilities. We will first understand how Spark works for ingestion, then walk through the development of this data source plug-in. Targeted audience Software and data engineers who need to expand Spark\u2019s ingestion capability. Key takeaways Requirements, needs & architecture \u2013 15%. Build the required tool set in Java \u2013 85%.Session hashtag: #EUdev6"},
{"speakers": [{"corp": "Alluxio, Inc.", "name": "Gene Pang"}], "base_fname": "Apache_Spark_Pipelines_in_the_Cloud_with_Alluxio", "title": "Apache Spark Pipelines in the Cloud with Alluxio", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/_gweJWJv-rs"}, "desc": "Organizations commonly use Apache Spark to gain actionable insight from their large amounts of data. Often, these analytics are in the form of data processing pipelines, where there are a series of processing stages, and each stage performs a particular function, and the output of one stage is the input of the next stage. There are several examples of pipelines, such as log processing, IoT pipelines, and machine learning. The common attribute among different pipelines is the sharing of data between stages. It is also common for Spark pipelines to process data stored in the public cloud, such as Amazon S3, Microsoft Azure Blob Storage, or Google Cloud Storage. The global availability and cost effectiveness of these public cloud storage services make them the preferred storage for data. However, running pipeline jobs while sharing data via cloud storage can be expensive in terms of increased network traffic, and slower data sharing and job completion times. Using Alluxio, a memory speed virtual distributed storage system, enables sharing data between different stages or jobs at memory speed. By reading and writing data in Alluxio, the data can stay in memory for the next stage of the pipeline, and this result in great performance gains. In this talk, we discuss how Alluxio can be deployed and used with a Spark data processing pipeline in the cloud. We show how pipeline stages can share data with Alluxio memory for improved performance benefits, and how Alluxio can improves completion times and reduces performance variability for Spark pipelines in the cloud.Session hashtag: #EUde5"},
{"speakers": [{"corp": "Red Hat", "name": "William Benton"}], "base_fname": "Building_Machine_Learning_Algorithms_on_Apache_Spark", "title": "Building Machine Learning Algorithms on Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/DtKBGwedap0"}, "desc": "There are lots of reasons why you might want to implement your own machine learning algorithms on Spark: you might want to experiment with a new idea, try and reproduce results from a recent research paper, or simply to use an existing technique that isn\u2019t implemented in MLlib. In this talk, we\u2019ll walk through the process of developing a new machine learning model for Spark. We\u2019ll start with the basics, by considering how we\u2019d design a parallel implementation of a particular unsupervised learning technique. The bulk of the talk will focus on the details you need to know to turn an algorithm design into an efficient parallel implementation on Spark: we\u2019ll start by reviewing a simple RDD-based implementation, show some improvements, point out some pitfalls to avoid, and iteratively extend our implementation to support contemporary Spark features like ML Pipelines and structured query processing. You\u2019ll leave this talk with everything you need to build a new machine learning technique that runs on Spark.Session hashtag: #EUds5"},
{"speakers": [{"corp": "Databricks", "name": "Silvio Fiorito"}], "base_fname": "Lessons_From_the_Field_Applying_Best_Practices_to_Your_Apache_Spark_Applications", "title": "Lessons From the Field: Applying Best Practices to Your Apache Spark Applications", "slide": {"dl_link": "", "src_link": "https://www.slideshare.net/databricks/lessons-from-the-field-applying-best-practices-to-your-apache-spark-applications-with-silvio-fiorito"}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/iwQel6JHMpA"}, "desc": "Apache Spark is an excellent tool to accelerate your analytics, whether you\u2019re doing ETL, Machine Learning, or Data Warehousing. However, to really make the most of Spark it pays to understand best practices for data storage, file formats, and query optimization. This talk will cover best practices I\u2019ve applied over years in the field helping customers write Spark applications as well as identifying what patterns make sense for your use case.Session hashtag: #EUdev5"},
{"speakers": [{"corp": "GoDataDriven", "name": "Fokko Driesprong"}, {"corp": "KnowIT", "name": "Rob Keevil"}], "base_fname": "Working_with_Skewed_Data_The_Iterative_Broadcast", "title": "Working with Skewed Data: The Iterative Broadcast", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/6zg7NTw-kTQ"}, "desc": "Skewed data is the enemy when joining tables using Spark. It shuffles a large proportion of the data onto a few overloaded nodes, bottlenecking Spark\u2019s parallelism and resulting in out of memory errors. The go-to answer is to use broadcast joins; leaving the large, skewed dataset in place and transmitting a smaller table to every machine in the cluster for joining. But what happens when your second table is too large to broadcast, and does not fit into memory? Or even worse, when a single key is bigger than the total size of your executor? Firstly, we will give an introduction into the problem. Secondly, the current ways of fighting the problem will be explained, including why these solutions are limited. Finally, we will demonstrate a new technique \u2013 the iterative broadcast join \u2013 developed while processing ING Bank\u2019s global transaction data. This technique, implemented on top of the Spark SQL API, allows multiple large and highly skewed datasets to be joined successfully, while retaining a high level of parallelism. This is something that is not possible with existing Spark join types.Session hashtag: #EUde11"},
{"speakers": [{"corp": "Databricks", "name": "Sue Ann Hong"}, {"corp": "Databricks", "name": "Tim Hunter"}], "base_fname": "Deep_Dive_into_Deep_Learning_Pipelines", "title": "Deep Dive into Deep Learning Pipelines", "slide": {"dl_link": "", "src_link": ""}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/bs64EDO873w"}, "desc": "Deep learning has shown tremendous successes, yet it often requires a lot of effort to leverage its power. Existing deep learning frameworks require writing a lot of code to run a model, let alone in a distributed manner. Deep Learning Pipelines is a Spark Package library that makes practical deep learning simple based on the Spark MLlib Pipelines API. Leveraging Spark, Deep Learning Pipelines scales out many compute-intensive deep learning tasks. In this talk we dive into \u2013 the various use cases of Deep Learning Pipelines such as prediction at massive scale, transfer learning, and hyperparameter tuning, many of which can be done in just a few lines of code. \u2013 how to work with complex data such as images in Spark and Deep Learning Pipelines. \u2013 how to deploy deep learning models through familiar Spark APIs such as MLlib and Spark SQL to empower everyone from machine learning practitioners to business analysts. Finally, we discuss integration with popular deep learning frameworks.Session hashtag: #EUdd3"},
{"speakers": [{"corp": "ZS Associates", "name": "Sandeep Varma"}, {"corp": "ZS Associates", "name": "Vickye Jain"}], "base_fname": "High_Performance_Enterprise_Data_Processing_with_Apache_Spark", "title": "High Performance Enterprise Data Processing with Apache Spark", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/_VNWyJUStN0"}, "desc": "Data engineering to support reporting and analytics for commercial Lifesciences groups consists of very complex interdependent processing with highly complex business rules (thousands of transformations on hundreds of data sources). We will talk about our experiences in building a very high performance data processing platform powered by Spark that balances the considerations of extreme performance, speed of development, and cost of maintenance. We will touch upon optimizing enterprise grade Spark architecture for data warehousing and data mart type applications, optimizing end to end pipelines for extreme performance, running hundreds of jobs in parallel in Spark, orchestrating across multiple Spark clusters, and some guidelines for high speed platform and application development within enterprises. Key takeaways: \u2013 example architecture for complex data warehousing and data mart applications on Spark \u2013 architecture to build high performance Spark platforms for enterprises that balance functionality with total cost of ownership \u2013 orchestrating multiple elastic Spark clusters while running hundreds of jobs in parallel \u2013 business benefits of high performance data engineering, especially for Lifesciences.Session hashtag: #EUde3"},
{"speakers": [{"corp": "Databricks", "name": "Wenchen Fan"}], "base_fname": "A_Developer_s_View_Into_Spark_s_Memory_Model", "title": "A Developer's View Into Spark's Memory Model", "slide": {"dl_link": "", "src_link": ""}, "tag": "Technical Deep Dives", "video": {"dl_link": "", "src_link": "https://youtu.be/P1AuP7q731U"}, "desc": "As part of Project Tungsten, we started an ongoing effort to substantially improve the memory and CPU efficiency of Apache Spark\u2019s backend execution and push performance closer to the limits of modern hardware. In this talk, we\u2019ll take a deep dive into Apache Spark\u2019s unified memory model and discuss how Spark exploits memory hierarchy and leverages application semantics to manage memory explicitly (both on and off-heap) to eliminate the overheads of JVM object model and garbage collection.Session hashtag: #EUdd2"},
{"speakers": [{"corp": "BMW AG", "name": "Marc Kaminski"}], "base_fname": "Building_Custom_ML_PipelineStages_for_Feature_Selection", "title": "Building Custom ML PipelineStages for Feature Selection", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/iUNk-i5aFPY"}, "desc": "For predicting vehicle defects at BMW, a machine learning pipeline evaluating several thousand features was implemented. As important features can be useful for evaluating specific defects, a feature selection approach has been used. For further evaluating the importance of features, several feature selection techniques (filters and wrappers) have been implemented as ml PipelineStages for usage on dataframes for incorporation in a complete Spark ml Pipeline, including preprocessing and classification. The general steps for building custom Spark ml Estimators are presented. The API of the newly implemented feature selection techniques is demonstrated and results of a performance analysis are shown. Besides that, experiences gained and pitfalls that should be avoided are shared.Session hashtag: #EUds16"},
{"speakers": [{"corp": "", "name": "Jacek Laskowski"}], "base_fname": "Monitoring_Structured_Streaming_Applications_Using_Web_UI", "title": "Monitoring Structured Streaming Applications Using Web UI", "slide": {"dl_link": "", "src_link": ""}, "tag": "Streaming", "video": {"dl_link": "", "src_link": "https://youtu.be/38NUPSWvmxw"}, "desc": "Spark Structured Streaming in Apache Spark 2.2 comes with quite a few unique Catalyst operators, most notably stateful streaming operators and three different output modes. Understanding how Spark Structured Streaming manages intermediate state between triggers and how it affects performance is paramount. After all you use Apache Spark for processing huge amount of data that alone can be tricky to get right, and Spark Structured Streaming adds the additional streaming factor that given a structured query can make the data even bigger due to state management.This deep-dive talk is going to show you what is included in execution diagrams, logical and physical plans, and metrics in SQL tab\u2019s Details for Query page.The talk is going to answer the following questions:* What do blue boxes represent in Details for Query page in SQL tab?"},
{"speakers": [{"corp": "Neustar", "name": "Emma Tang"}], "base_fname": "Optimal_Strategies_for_Large_Scale_Batch_ETL_Jobs", "title": "Optimal Strategies for Large-Scale Batch ETL Jobs", "slide": {"dl_link": "", "src_link": ""}, "tag": "Developer", "video": {"dl_link": "", "src_link": "https://youtu.be/UmBL5RApe-I"}, "desc": "The ad tech industry processes large volumes of pixel and server-to-server data for each online user\u2019s click, impression, and conversion data. At Neustar, we process 10+ billion events per day, and all of our events are fed through a number of Spark ETL batch jobs. Many of our Spark jobs process over 100 terabytes of data per run, each job runs to completion in around 3.5 hours. This means we needed to optimize our jobs in specific ways to achieve massive parallelization while keeping the memory usage (and cost) as low as possible. Our talk is focused on strategies dealing with extremely large data. We will talk about the things we learned and the mistakes we made. This includes: \u2013 Optimizing memory usage using Ganglia \u2013 Optimizing partition counts for different types of stages and effective joins \u2013 Counterintuitive strategies for materializing data to maximize efficiency \u2013 Spark default settings specific to large scale jobs, and how they matter \u2013 Running Spark using Amazon EMR with more than 3200 cores \u2013 Review different types of errors and stack traces that occur with large-scale jobs and how to read and handle them \u2013 How to deal with large number of map output status when there are 100k partitions joining with 100k partitions \u2013 How to prevent serialization buffer overflow as well as map out status buffer overflow. This can easily happen when data is extremely large \u2013 How to effectively use partitioners to combine stages and minimize shuffle.Session hashtag: #EUdev3"},
{"speakers": [{"corp": "Toon", "name": "Telmo Oliveira"}], "base_fname": "Using_Apache_Spark_in_the_Cloud_A_Devops_Perspective", "title": "Using Apache Spark in the Cloud\u2014A Devops Perspective", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Engineering", "video": {"dl_link": "", "src_link": "https://youtu.be/pxBh9elR-sM"}, "desc": "Toon is a leading brand in the European smart energy market, currently expanding internationally, providing energy usage insights, eco-friendly energy management and smart thermostat use for the connected home. As value added services become ever more relevant in this market, we have the need to ensure that we can easily and safely on-board new tenants into our data platform. In this talk we\u2019re going to guide you across a less discussed side of using Spark in production \u2013 devops. We will speak about our journey from an on-premise cluster to a managed solution in the cloud. A lot of moving parts were involved: ETL flows, data sharing with 3rd parties and data migration to the new environment. Add to this the need to have a multi-tenant environment, revamp our toolset and deploy a live public facing service. It\u2019s possible to find a lot of great examples of how Spark is used for data-science purposes. On the data engineering side, we need to deploy production services, ensure data is cleaned, secured and available, and keep the data-science teams happy. We\u2019d like to share some of the options we took and some of the lessons learned from this (ongoing) transition.Session hashtag: #EUde10"},
{"speakers": [{"corp": "IBM", "name": "Nick Pentreath"}], "base_fname": "Feature_Hashing_for_Scalable_Machine_Learning", "title": "Feature Hashing for Scalable Machine Learning", "slide": {"dl_link": "", "src_link": ""}, "tag": "Data Science", "video": {"dl_link": "", "src_link": "https://youtu.be/SLqKepl9rEo"}, "desc": "Feature hashing is a powerful technique for handling high-dimensional features in machine learning. It is fast, simple, memory-efficient, and well suited to online learning scenarios. While an approximation, it has surprisingly low accuracy tradeoffs in many machine learning problems. Feature hashing has been made somewhat popular by libraries such as Vowpal Wabbit and scikit-learn. In Spark MLlib, it is mostly used for text features; however, its use cases extend more broadly. Many Spark users are not familiar with the ways in which feature hashing might be applied to their problems. In this talk, I will cover the basics of feature hashing, and how to use it for all feature types in machine learning. I will also introduce a more flexible and powerful feature hashing transformer for use within Spark ML pipelines. Finally, I will explore the performance and scalability tradeoffs of feature hashing on various datasets.Session hashtag: #EUds15"}
]
